{"hbase-server-1.1.1.jar": [["org.apache.hadoop.hbase.CoordinatedStateManagerFactory", "org.apache.hadoop.hbase.CoordinatedStateManagerFactory()", "public org.apache.hadoop.hbase.CoordinatedStateManagerFactory()"], ["org.apache.hadoop.hbase.CoordinatedStateManager", "org.apache.hadoop.hbase.CoordinatedStateManagerFactory.getCoordinatedStateManager(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.DaemonThreadFactory", "org.apache.hadoop.hbase.DaemonThreadFactory(java.lang.String)", "public org.apache.hadoop.hbase.DaemonThreadFactory(java.lang.String)"], ["java.lang.Thread", "org.apache.hadoop.hbase.DaemonThreadFactory.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight$WeightComparator", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight$WeightComparator()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight$WeightComparator()"], ["int", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight$WeightComparator.compare(org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight, org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight)", "public int compare(org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight, org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight)"], ["int", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight$WeightComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight(java.lang.String, long)", "public org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight(java.lang.String, long)"], ["void", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight.addWeight(long)", "public void addWeight(long)"], ["java.lang.String", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight.getHost()", "public java.lang.String getHost()"], ["long", "org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight.getWeight()", "public long getWeight()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.HDFSBlocksDistribution()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution()"], ["synchronized", "org.apache.hadoop.hbase.HDFSBlocksDistribution.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.HDFSBlocksDistribution.addHostsAndBlockWeight(java.lang.String[], long)", "public void addHostsAndBlockWeight(java.lang.String[], long)"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight>", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getHostAndWeights()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight> getHostAndWeights()"], ["long", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getWeight(java.lang.String)", "public long getWeight(java.lang.String)"], ["long", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getUniqueBlocksTotalWeight()", "public long getUniqueBlocksTotalWeight()"], ["float", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getBlockLocalityIndex(java.lang.String)", "public float getBlockLocalityIndex(java.lang.String)"], ["void", "org.apache.hadoop.hbase.HDFSBlocksDistribution.add(org.apache.hadoop.hbase.HDFSBlocksDistribution)", "public void add(org.apache.hadoop.hbase.HDFSBlocksDistribution)"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getTopHosts()", "public java.util.List<java.lang.String> getTopHosts()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight[]", "org.apache.hadoop.hbase.HDFSBlocksDistribution.getTopHostsWithWeights()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution$HostAndWeight[] getTopHostsWithWeights()"], ["org.apache.hadoop.hbase.HealthCheckChore", "org.apache.hadoop.hbase.HealthCheckChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.HealthCheckChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus[]", "org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus.values()", "public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus[] values()"], ["org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus", "org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.HealthChecker.init(java.lang.String, long)", "public void init(java.lang.String, long)"], ["org.apache.hadoop.hbase.HealthReport", "org.apache.hadoop.hbase.HealthChecker.checkHealth()", "public org.apache.hadoop.hbase.HealthReport checkHealth()"], ["java.lang.String", "org.apache.hadoop.hbase.HealthReport.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.HealthReport.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.HealthReport.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["org.apache.hadoop.hbase.JMXListener", "org.apache.hadoop.hbase.JMXListener()", "public org.apache.hadoop.hbase.JMXListener()"], ["javax.management.remote.JMXServiceURL", "org.apache.hadoop.hbase.JMXListener.buildJMXServiceURL(int, int)", "public static javax.management.remote.JMXServiceURL buildJMXServiceURL(int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.JMXListener.startConnectorServer(int, int)", "public void startConnectorServer(int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.JMXListener.stopConnectorServer()", "public void stopConnectorServer() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.JMXListener.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.JMXListener.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.LocalHBaseCluster$1.run()", "public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.LocalHBaseCluster$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.LocalHBaseCluster$2.run()", "public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.LocalHBaseCluster$2.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.LocalHBaseCluster", "org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.LocalHBaseCluster", "org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int)", "public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.LocalHBaseCluster", "org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int)", "public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.LocalHBaseCluster", "org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>)", "public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addRegionServer()", "public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer() throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addRegionServer(org.apache.hadoop.conf.Configuration, int)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addRegionServer(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addMaster()", "public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster() throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addMaster(org.apache.hadoop.conf.Configuration, int)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.LocalHBaseCluster.addMaster(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.LocalHBaseCluster.getRegionServer(int)", "public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer(int)"], ["java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>", "org.apache.hadoop.hbase.LocalHBaseCluster.getRegionServers()", "public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getRegionServers()"], ["java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>", "org.apache.hadoop.hbase.LocalHBaseCluster.getLiveRegionServers()", "public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getLiveRegionServers()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.LocalHBaseCluster.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["java.lang.String", "org.apache.hadoop.hbase.LocalHBaseCluster.waitOnRegionServer(int)", "public java.lang.String waitOnRegionServer(int)"], ["java.lang.String", "org.apache.hadoop.hbase.LocalHBaseCluster.waitOnRegionServer(org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread)", "public java.lang.String waitOnRegionServer(org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.LocalHBaseCluster.getMaster(int)", "public org.apache.hadoop.hbase.master.HMaster getMaster(int)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.LocalHBaseCluster.getActiveMaster()", "public org.apache.hadoop.hbase.master.HMaster getActiveMaster()"], ["java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>", "org.apache.hadoop.hbase.LocalHBaseCluster.getMasters()", "public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getMasters()"], ["java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>", "org.apache.hadoop.hbase.LocalHBaseCluster.getLiveMasters()", "public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getLiveMasters()"], ["java.lang.String", "org.apache.hadoop.hbase.LocalHBaseCluster.waitOnMaster(int)", "public java.lang.String waitOnMaster(int)"], ["java.lang.String", "org.apache.hadoop.hbase.LocalHBaseCluster.waitOnMaster(org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread)", "public java.lang.String waitOnMaster(org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread)"], ["void", "org.apache.hadoop.hbase.LocalHBaseCluster.join()", "public void join()"], ["void", "org.apache.hadoop.hbase.LocalHBaseCluster.startup()", "public void startup() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.LocalHBaseCluster.shutdown()", "public void shutdown()"], ["boolean", "org.apache.hadoop.hbase.LocalHBaseCluster.isLocal(org.apache.hadoop.conf.Configuration)", "public static boolean isLocal(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.LocalHBaseCluster.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.MetaMigrationConvertingToPB$ConvertToPBMetaVisitor", "org.apache.hadoop.hbase.MetaMigrationConvertingToPB$ConvertToPBMetaVisitor(org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.MetaMigrationConvertingToPB$ConvertToPBMetaVisitor(org.apache.hadoop.hbase.master.MasterServices)"], ["boolean", "org.apache.hadoop.hbase.MetaMigrationConvertingToPB$ConvertToPBMetaVisitor.visit(org.apache.hadoop.hbase.client.Result)", "public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.MetaMigrationConvertingToPB", "org.apache.hadoop.hbase.MetaMigrationConvertingToPB()", "public org.apache.hadoop.hbase.MetaMigrationConvertingToPB()"], ["long", "org.apache.hadoop.hbase.MetaMigrationConvertingToPB.updateMetaIfNecessary(org.apache.hadoop.hbase.master.MasterServices)", "public static long updateMetaIfNecessary(org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.SplitLogCounters", "org.apache.hadoop.hbase.SplitLogCounters()", "public org.apache.hadoop.hbase.SplitLogCounters()"], ["void", "org.apache.hadoop.hbase.SplitLogCounters.resetCounters()", "public static void resetCounters() throws java.lang.Exception"], ["org.apache.hadoop.hbase.SplitLogTask$Done", "org.apache.hadoop.hbase.SplitLogTask$Done(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.SplitLogTask$Done(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["org.apache.hadoop.hbase.SplitLogTask$Err", "org.apache.hadoop.hbase.SplitLogTask$Err(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.SplitLogTask$Err(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["org.apache.hadoop.hbase.SplitLogTask$Owned", "org.apache.hadoop.hbase.SplitLogTask$Owned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.SplitLogTask$Owned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["org.apache.hadoop.hbase.SplitLogTask$Resigned", "org.apache.hadoop.hbase.SplitLogTask$Resigned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.SplitLogTask$Resigned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["org.apache.hadoop.hbase.SplitLogTask$Unassigned", "org.apache.hadoop.hbase.SplitLogTask$Unassigned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.SplitLogTask$Unassigned(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.SplitLogTask.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode", "org.apache.hadoop.hbase.SplitLogTask.getMode()", "public org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode getMode()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isUnassigned(org.apache.hadoop.hbase.ServerName)", "public boolean isUnassigned(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isUnassigned()", "public boolean isUnassigned()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isOwned(org.apache.hadoop.hbase.ServerName)", "public boolean isOwned(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isOwned()", "public boolean isOwned()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isResigned(org.apache.hadoop.hbase.ServerName)", "public boolean isResigned(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isResigned()", "public boolean isResigned()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isDone(org.apache.hadoop.hbase.ServerName)", "public boolean isDone(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isDone()", "public boolean isDone()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isErr(org.apache.hadoop.hbase.ServerName)", "public boolean isErr(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.isErr()", "public boolean isErr()"], ["java.lang.String", "org.apache.hadoop.hbase.SplitLogTask.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.SplitLogTask.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.SplitLogTask.hashCode()", "public int hashCode()"], ["org.apache.hadoop.hbase.SplitLogTask", "org.apache.hadoop.hbase.SplitLogTask.parseFrom(byte[])", "public static org.apache.hadoop.hbase.SplitLogTask parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException"], ["byte[]", "org.apache.hadoop.hbase.SplitLogTask.toByteArray()", "public byte[] toByteArray()"], ["org.apache.hadoop.hbase.TagRewriteCell", "org.apache.hadoop.hbase.TagRewriteCell(org.apache.hadoop.hbase.Cell, byte[])", "public org.apache.hadoop.hbase.TagRewriteCell(org.apache.hadoop.hbase.Cell, byte[])"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getRowArray()", "public byte[] getRowArray()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getRowOffset()", "public int getRowOffset()"], ["short", "org.apache.hadoop.hbase.TagRewriteCell.getRowLength()", "public short getRowLength()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getFamilyArray()", "public byte[] getFamilyArray()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getFamilyOffset()", "public int getFamilyOffset()"], ["byte", "org.apache.hadoop.hbase.TagRewriteCell.getFamilyLength()", "public byte getFamilyLength()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getQualifierArray()", "public byte[] getQualifierArray()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getQualifierOffset()", "public int getQualifierOffset()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getQualifierLength()", "public int getQualifierLength()"], ["long", "org.apache.hadoop.hbase.TagRewriteCell.getTimestamp()", "public long getTimestamp()"], ["byte", "org.apache.hadoop.hbase.TagRewriteCell.getTypeByte()", "public byte getTypeByte()"], ["long", "org.apache.hadoop.hbase.TagRewriteCell.getMvccVersion()", "public long getMvccVersion()"], ["long", "org.apache.hadoop.hbase.TagRewriteCell.getSequenceId()", "public long getSequenceId()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getValueArray()", "public byte[] getValueArray()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getValueOffset()", "public int getValueOffset()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getValueLength()", "public int getValueLength()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getTagsArray()", "public byte[] getTagsArray()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getTagsOffset()", "public int getTagsOffset()"], ["int", "org.apache.hadoop.hbase.TagRewriteCell.getTagsLength()", "public int getTagsLength()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getValue()", "public byte[] getValue()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getFamily()", "public byte[] getFamily()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getQualifier()", "public byte[] getQualifier()"], ["byte[]", "org.apache.hadoop.hbase.TagRewriteCell.getRow()", "public byte[] getRow()"], ["long", "org.apache.hadoop.hbase.TagRewriteCell.heapSize()", "public long heapSize()"], ["void", "org.apache.hadoop.hbase.TagRewriteCell.setTimestamp(long)", "public void setTimestamp(long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.TagRewriteCell.setTimestamp(byte[], int)", "public void setTimestamp(byte[], int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.TagRewriteCell.setSequenceId(long)", "public void setSequenceId(long) throws java.io.IOException"], ["org.apache.hadoop.hbase.ZKNamespaceManager", "org.apache.hadoop.hbase.ZKNamespaceManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.ZKNamespaceManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.start()", "public void start() throws java.io.IOException"], ["org.apache.hadoop.hbase.NamespaceDescriptor", "org.apache.hadoop.hbase.ZKNamespaceManager.get(java.lang.String)", "public org.apache.hadoop.hbase.NamespaceDescriptor get(java.lang.String)"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.update(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void update(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.remove(java.lang.String)", "public void remove(java.lang.String) throws java.io.IOException"], ["java.util.NavigableSet<org.apache.hadoop.hbase.NamespaceDescriptor>", "org.apache.hadoop.hbase.ZKNamespaceManager.list()", "public java.util.NavigableSet<org.apache.hadoop.hbase.NamespaceDescriptor> list() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.ZKNamespaceManager.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.ZNodeClearer$1.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.ZNodeClearer$1.isAborted()", "public boolean isAborted()"], ["void", "org.apache.hadoop.hbase.ZNodeClearer.writeMyEphemeralNodeOnDisk(java.lang.String)", "public static void writeMyEphemeralNodeOnDisk(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.ZNodeClearer.readMyEphemeralNodeOnDisk()", "public static java.lang.String readMyEphemeralNodeOnDisk() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.ZNodeClearer.getMyEphemeralNodeFileName()", "public static java.lang.String getMyEphemeralNodeFileName()"], ["void", "org.apache.hadoop.hbase.ZNodeClearer.deleteMyEphemeralNodeOnDisk()", "public static void deleteMyEphemeralNodeOnDisk()"], ["boolean", "org.apache.hadoop.hbase.ZNodeClearer.clear(org.apache.hadoop.conf.Configuration)", "public static boolean clear(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.backup.HFileArchiver$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.backup.HFileArchiver$File", "org.apache.hadoop.hbase.backup.HFileArchiver$File(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.backup.HFileArchiver$File(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.backup.HFileArchiver$File.moveAndClose(org.apache.hadoop.fs.Path)", "public boolean moveAndClose(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.backup.HFileArchiver$File.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["java.lang.String", "org.apache.hadoop.hbase.backup.HFileArchiver$File.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter", "org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter", "org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.hbase.backup.HFileArchiver$File", "org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter.apply(org.apache.hadoop.fs.FileStatus)", "public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.fs.FileStatus)"], ["java.lang.Object", "org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath.delete()", "public void delete() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath.getName()", "public java.lang.String getName()"], ["java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File>", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath.getChildren()", "public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath.isFile()", "public boolean isFile() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.StoreFile)", "public org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.StoreFile)"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile.delete()", "public void delete() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile.getName()", "public java.lang.String getName()"], ["boolean", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile.isFile()", "public boolean isFile()"], ["java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File>", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile.getChildren()", "public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile", "org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.hbase.backup.HFileArchiver$File", "org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver.archiveRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo)", "public static void archiveRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.backup.HFileArchiver.archiveRegion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static boolean archiveRegion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver.archiveFamily(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[])", "public static void archiveFamily(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver.archiveStoreFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static void archiveStoreFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.HFileArchiver.archiveStoreFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[], org.apache.hadoop.fs.Path)", "public static void archiveStoreFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[], org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveManager", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.backup.example.HFileArchiveManager(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveManager", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager.enableHFileBackup(byte[])", "public org.apache.hadoop.hbase.backup.example.HFileArchiveManager enableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveManager", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager.disableHFileBackup(byte[])", "public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveManager", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager.disableHFileBackup()", "public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager.stop()", "public void stop()"], ["boolean", "org.apache.hadoop.hbase.backup.example.HFileArchiveManager.isArchivingEnabled(byte[])", "public boolean isArchivingEnabled(byte[]) throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor()", "public org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor()"], ["synchronized", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.void setArchiveTables(java.util.List<java.lang.String>)", "public synchronized void setArchiveTables(java.util.List<java.lang.String>)"], ["synchronized", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.void addTable(java.lang.String)", "public synchronized void addTable(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.void removeTable(java.lang.String)", "public synchronized void removeTable(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.void clearArchive()", "public synchronized void clearArchive()"], ["synchronized", "org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.boolean shouldArchiveTable(java.lang.String)", "public synchronized boolean shouldArchiveTable(java.lang.String)"], ["org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner", "org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner()", "public org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner()"], ["boolean", "org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner.isFileDeletable(org.apache.hadoop.fs.FileStatus)", "public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus)"], ["void", "org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["void", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.start()", "public void start() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.keepHFiles(java.lang.String)", "public boolean keepHFiles(java.lang.String)"], ["org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.getMonitor()", "public final org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor getMonitor()"], ["org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.create(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker create(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.getZooKeeperWatcher()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher getZooKeeperWatcher()"], ["void", "org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.stop()", "public void stop()"], ["org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ClusterConnection)", "public org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ClusterConnection)"], ["void", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.enableHFileBackupAsync(byte[])", "public void enableHFileBackupAsync(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.disableHFileBackup(java.lang.String)", "public void disableHFileBackup(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.disableHFileBackup(byte[])", "public void disableHFileBackup(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.disableHFileBackup()", "public void disableHFileBackup() throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["boolean", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.getArchivingEnabled(byte[])", "public boolean getArchivingEnabled(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["boolean", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.getArchivingEnabled(java.lang.String)", "public boolean getArchivingEnabled(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["java.lang.String", "org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.getArchiveZNode(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public static java.lang.String getArchiveZNode(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["org.apache.hadoop.hbase.client.ClientSideRegionScanner", "org.apache.hadoop.hbase.client.ClientSideRegionScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.client.metrics.ScanMetrics)", "public org.apache.hadoop.hbase.client.ClientSideRegionScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.client.metrics.ScanMetrics) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.ClientSideRegionScanner.next()", "public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.ClientSideRegionScanner.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.client.ClientSideRegionScanner.renewLease()", "public boolean renewLease()"], ["org.apache.hadoop.hbase.client.ClusterConnection", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getConnectionForEnvironment(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public static org.apache.hadoop.hbase.client.ClusterConnection getConnectionForEnvironment(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.CoprocessorHConnection", "org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.CoprocessorHConnection", "org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.CoprocessorHConnection", "org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.client.CoprocessorHConnection(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getClient(org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingInterface getClient(org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.NonceGenerator", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getNonceGenerator()", "public org.apache.hadoop.hbase.client.NonceGenerator getNonceGenerator()"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isManaged()", "public boolean isManaged()"], ["org.apache.hadoop.hbase.client.RpcRetryingCallerFactory", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getNewRpcRetryingCallerFactory(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.client.RpcRetryingCallerFactory getNewRpcRetryingCallerFactory(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getHTableDescriptor(byte[])", "public org.apache.hadoop.hbase.HTableDescriptor getHTableDescriptor(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getHTableDescriptor(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.HTableDescriptor getHTableDescriptor(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor[]", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getHTableDescriptors(java.util.List)", "public org.apache.hadoop.hbase.HTableDescriptor[] getHTableDescriptors(java.util.List) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor[]", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getHTableDescriptorsByTableName(java.util.List)", "public org.apache.hadoop.hbase.HTableDescriptor[] getHTableDescriptorsByTableName(java.util.List) throws java.io.IOException"], ["org.apache.hadoop.hbase.TableName[]", "org.apache.hadoop.hbase.client.CoprocessorHConnection.listTableNames()", "public org.apache.hadoop.hbase.TableName[] listTableNames() throws java.io.IOException"], ["java.lang.String[]", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTableNames()", "public java.lang.String[] getTableNames() throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor[]", "org.apache.hadoop.hbase.client.CoprocessorHConnection.listTables()", "public org.apache.hadoop.hbase.HTableDescriptor[] listTables() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.close()", "public void close()"], ["int", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getCurrentNrHRS()", "public int getCurrentNrHRS() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isAborted()", "public boolean isAborted()"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isClosed()", "public boolean isClosed()"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getRegionCachePrefetch(byte[])", "public boolean getRegionCachePrefetch(byte[])"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getRegionCachePrefetch(org.apache.hadoop.hbase.TableName)", "public boolean getRegionCachePrefetch(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.setRegionCachePrefetch(byte[], boolean)", "public void setRegionCachePrefetch(byte[], boolean)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.setRegionCachePrefetch(org.apache.hadoop.hbase.TableName, boolean)", "public void setRegionCachePrefetch(org.apache.hadoop.hbase.TableName, boolean)"], ["org.apache.hadoop.hbase.client.backoff.ClientBackoffPolicy", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getBackoffPolicy()", "public org.apache.hadoop.hbase.client.backoff.ClientBackoffPolicy getBackoffPolicy()"], ["org.apache.hadoop.hbase.client.ServerStatisticTracker", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getStatisticsTracker()", "public org.apache.hadoop.hbase.client.ServerStatisticTracker getStatisticsTracker()"], ["org.apache.hadoop.hbase.client.AsyncProcess", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getAsyncProcess()", "public org.apache.hadoop.hbase.client.AsyncProcess getAsyncProcess()"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.processBatchCallback(java.util.List, byte[], java.util.concurrent.ExecutorService, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback)", "public void processBatchCallback(java.util.List, byte[], java.util.concurrent.ExecutorService, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.processBatchCallback(java.util.List, org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback)", "public void processBatchCallback(java.util.List, org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.processBatch(java.util.List, byte[], java.util.concurrent.ExecutorService, java.lang.Object[])", "public void processBatch(java.util.List, byte[], java.util.concurrent.ExecutorService, java.lang.Object[]) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.processBatch(java.util.List, org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService, java.lang.Object[])", "public void processBatch(java.util.List, org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService, java.lang.Object[]) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.updateCachedLocations(byte[], byte[], java.lang.Object, org.apache.hadoop.hbase.HRegionLocation)", "public void updateCachedLocations(byte[], byte[], java.lang.Object, org.apache.hadoop.hbase.HRegionLocation)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.updateCachedLocations(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.Object, org.apache.hadoop.hbase.ServerName)", "public void updateCachedLocations(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.Object, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.updateCachedLocations(org.apache.hadoop.hbase.TableName, byte[], java.lang.Object, org.apache.hadoop.hbase.HRegionLocation)", "public void updateCachedLocations(org.apache.hadoop.hbase.TableName, byte[], java.lang.Object, org.apache.hadoop.hbase.HRegionLocation)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.deleteCachedRegionLocation(org.apache.hadoop.hbase.HRegionLocation)", "public void deleteCachedRegionLocation(org.apache.hadoop.hbase.HRegionLocation)"], ["org.apache.hadoop.hbase.client.MasterKeepAliveConnection", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getKeepAliveMasterService()", "public org.apache.hadoop.hbase.client.MasterKeepAliveConnection getKeepAliveMasterService() throws org.apache.hadoop.hbase.MasterNotRunningException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getMaster()", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingInterface getMaster() throws org.apache.hadoop.hbase.MasterNotRunningException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getAdmin(org.apache.hadoop.hbase.ServerName, boolean)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface getAdmin(org.apache.hadoop.hbase.ServerName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getAdmin(org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface getAdmin(org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.clearRegionCache(byte[])", "public void clearRegionCache(byte[])"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.clearRegionCache(org.apache.hadoop.hbase.TableName)", "public void clearRegionCache(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.clearRegionCache()", "public void clearRegionCache()"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.clearCaches(org.apache.hadoop.hbase.ServerName)", "public void clearCaches(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.client.CoprocessorHConnection.clearRegionCache(org.apache.hadoop.hbase.TableName, byte[])", "public void clearRegionCache(org.apache.hadoop.hbase.TableName, byte[])"], ["org.apache.hadoop.hbase.RegionLocations", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegion(org.apache.hadoop.hbase.TableName, byte[], boolean, boolean, int)", "public org.apache.hadoop.hbase.RegionLocations locateRegion(org.apache.hadoop.hbase.TableName, byte[], boolean, boolean, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.RegionLocations", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegion(org.apache.hadoop.hbase.TableName, byte[], boolean, boolean)", "public org.apache.hadoop.hbase.RegionLocations locateRegion(org.apache.hadoop.hbase.TableName, byte[], boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.relocateRegion(byte[], byte[])", "public org.apache.hadoop.hbase.HRegionLocation relocateRegion(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.RegionLocations", "org.apache.hadoop.hbase.client.CoprocessorHConnection.relocateRegion(org.apache.hadoop.hbase.TableName, byte[], int)", "public org.apache.hadoop.hbase.RegionLocations relocateRegion(org.apache.hadoop.hbase.TableName, byte[], int) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.relocateRegion(org.apache.hadoop.hbase.TableName, byte[])", "public org.apache.hadoop.hbase.HRegionLocation relocateRegion(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegion(byte[], byte[])", "public org.apache.hadoop.hbase.HRegionLocation locateRegion(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegion(org.apache.hadoop.hbase.TableName, byte[])", "public org.apache.hadoop.hbase.HRegionLocation locateRegion(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["java.util.List", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegions(byte[], boolean, boolean)", "public java.util.List locateRegions(byte[], boolean, boolean) throws java.io.IOException"], ["java.util.List", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegions(org.apache.hadoop.hbase.TableName, boolean, boolean)", "public java.util.List locateRegions(org.apache.hadoop.hbase.TableName, boolean, boolean) throws java.io.IOException"], ["java.util.List", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegions(byte[])", "public java.util.List locateRegions(byte[]) throws java.io.IOException"], ["java.util.List", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegions(org.apache.hadoop.hbase.TableName)", "public java.util.List locateRegions(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isDeadServer(org.apache.hadoop.hbase.ServerName)", "public boolean isDeadServer(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.locateRegion(byte[])", "public org.apache.hadoop.hbase.HRegionLocation locateRegion(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableAvailable(byte[], byte[][])", "public boolean isTableAvailable(byte[], byte[][]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableAvailable(org.apache.hadoop.hbase.TableName, byte[][])", "public boolean isTableAvailable(org.apache.hadoop.hbase.TableName, byte[][]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableAvailable(byte[])", "public boolean isTableAvailable(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableAvailable(org.apache.hadoop.hbase.TableName)", "public boolean isTableAvailable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableDisabled(byte[])", "public boolean isTableDisabled(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableDisabled(org.apache.hadoop.hbase.TableName)", "public boolean isTableDisabled(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableEnabled(byte[])", "public boolean isTableEnabled(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isTableEnabled(org.apache.hadoop.hbase.TableName)", "public boolean isTableEnabled(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getRegionLocation(byte[], byte[], boolean)", "public org.apache.hadoop.hbase.HRegionLocation getRegionLocation(byte[], byte[], boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionLocation", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getRegionLocation(org.apache.hadoop.hbase.TableName, byte[], boolean)", "public org.apache.hadoop.hbase.HRegionLocation getRegionLocation(org.apache.hadoop.hbase.TableName, byte[], boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.CoprocessorHConnection.isMasterRunning()", "public boolean isMasterRunning() throws org.apache.hadoop.hbase.MasterNotRunningException, org.apache.hadoop.hbase.ZooKeeperConnectionException"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["java.lang.String", "org.apache.hadoop.hbase.client.CoprocessorHConnection.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.client.Admin", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getAdmin()", "public org.apache.hadoop.hbase.client.Admin getAdmin() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.RegionLocator", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getRegionLocator(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.client.RegionLocator getRegionLocator(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.BufferedMutator", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getBufferedMutator(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.client.BufferedMutator", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getBufferedMutator(org.apache.hadoop.hbase.client.BufferedMutatorParams)", "public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.client.BufferedMutatorParams)"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(byte[], java.util.concurrent.ExecutorService)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(byte[], java.util.concurrent.ExecutorService) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(java.lang.String, java.util.concurrent.ExecutorService)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(java.lang.String, java.util.concurrent.ExecutorService) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(byte[])", "public org.apache.hadoop.hbase.client.HTableInterface getTable(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.CoprocessorHConnection.getTable(java.lang.String)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.client.HTableWrapper.createWrapper(java.util.List<org.apache.hadoop.hbase.client.HTableInterface>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment, java.util.concurrent.ExecutorService)", "public static org.apache.hadoop.hbase.client.HTableInterface createWrapper(java.util.List<org.apache.hadoop.hbase.client.HTableInterface>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment, java.util.concurrent.ExecutorService) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.internalClose()", "public void internalClose() throws java.io.IOException"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.client.HTableWrapper.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.HTableWrapper.getRowOrBefore(byte[], byte[])", "public org.apache.hadoop.hbase.client.Result getRowOrBefore(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.HTableWrapper.get(org.apache.hadoop.hbase.client.Get)", "public org.apache.hadoop.hbase.client.Result get(org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.exists(org.apache.hadoop.hbase.client.Get)", "public boolean exists(org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["boolean[]", "org.apache.hadoop.hbase.client.HTableWrapper.existsAll(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public boolean[] existsAll(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["java.lang.Boolean[]", "org.apache.hadoop.hbase.client.HTableWrapper.exists(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public java.lang.Boolean[] exists(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.put(org.apache.hadoop.hbase.client.Put)", "public void put(org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.put(java.util.List<org.apache.hadoop.hbase.client.Put>)", "public void put(java.util.List<org.apache.hadoop.hbase.client.Put>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.delete(org.apache.hadoop.hbase.client.Delete)", "public void delete(org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.delete(java.util.List<org.apache.hadoop.hbase.client.Delete>)", "public void delete(java.util.List<org.apache.hadoop.hbase.client.Delete>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.checkAndPut(byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.client.Put)", "public boolean checkAndPut(byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.checkAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.Put)", "public boolean checkAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.checkAndDelete(byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.client.Delete)", "public boolean checkAndDelete(byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.checkAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.Delete)", "public boolean checkAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.client.HTableWrapper.incrementColumnValue(byte[], byte[], byte[], long)", "public long incrementColumnValue(byte[], byte[], byte[], long) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.client.HTableWrapper.incrementColumnValue(byte[], byte[], byte[], long, org.apache.hadoop.hbase.client.Durability)", "public long incrementColumnValue(byte[], byte[], byte[], long, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.HTableWrapper.append(org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.HTableWrapper.increment(org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.flushCommits()", "public void flushCommits() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.isAutoFlush()", "public boolean isAutoFlush()"], ["org.apache.hadoop.hbase.client.ResultScanner", "org.apache.hadoop.hbase.client.HTableWrapper.getScanner(org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.client.ResultScanner getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.ResultScanner", "org.apache.hadoop.hbase.client.HTableWrapper.getScanner(byte[])", "public org.apache.hadoop.hbase.client.ResultScanner getScanner(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.ResultScanner", "org.apache.hadoop.hbase.client.HTableWrapper.getScanner(byte[], byte[])", "public org.apache.hadoop.hbase.client.ResultScanner getScanner(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.client.HTableWrapper.getTableDescriptor()", "public org.apache.hadoop.hbase.HTableDescriptor getTableDescriptor() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.client.HTableWrapper.getTableName()", "public byte[] getTableName()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.client.HTableWrapper.getName()", "public org.apache.hadoop.hbase.TableName getName()"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.batch(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, java.lang.Object[])", "public void batch(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, java.lang.Object[]) throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object[]", "org.apache.hadoop.hbase.client.HTableWrapper.batch(java.util.List<? extends org.apache.hadoop.hbase.client.Row>)", "public java.lang.Object[] batch(java.util.List<? extends org.apache.hadoop.hbase.client.Row>) throws java.io.IOException, java.lang.InterruptedException"], ["<R> void", "org.apache.hadoop.hbase.client.HTableWrapper.batchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>)", "public <R> void batchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>) throws java.io.IOException, java.lang.InterruptedException"], ["<R> java.lang.Object[]", "org.apache.hadoop.hbase.client.HTableWrapper.batchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>)", "public <R> java.lang.Object[] batchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.client.Result[]", "org.apache.hadoop.hbase.client.HTableWrapper.get(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public org.apache.hadoop.hbase.client.Result[] get(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel", "org.apache.hadoop.hbase.client.HTableWrapper.coprocessorService(byte[])", "public org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel coprocessorService(byte[])"], ["<T extends com.google.protobuf.Service, R> java.util.Map<byte[], R>", "org.apache.hadoop.hbase.client.HTableWrapper.coprocessorService(java.lang.Class<T>, byte[], byte[], org.apache.hadoop.hbase.client.coprocessor.Batch$Call<T, R>)", "public <T extends com.google.protobuf.Service, R> java.util.Map<byte[], R> coprocessorService(java.lang.Class<T>, byte[], byte[], org.apache.hadoop.hbase.client.coprocessor.Batch$Call<T, R>) throws com.google.protobuf.ServiceException, java.lang.Throwable"], ["<T extends com.google.protobuf.Service, R> void", "org.apache.hadoop.hbase.client.HTableWrapper.coprocessorService(java.lang.Class<T>, byte[], byte[], org.apache.hadoop.hbase.client.coprocessor.Batch$Call<T, R>, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>)", "public <T extends com.google.protobuf.Service, R> void coprocessorService(java.lang.Class<T>, byte[], byte[], org.apache.hadoop.hbase.client.coprocessor.Batch$Call<T, R>, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>) throws com.google.protobuf.ServiceException, java.lang.Throwable"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.mutateRow(org.apache.hadoop.hbase.client.RowMutations)", "public void mutateRow(org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.setAutoFlush(boolean)", "public void setAutoFlush(boolean)"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.setAutoFlush(boolean, boolean)", "public void setAutoFlush(boolean, boolean)"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.setAutoFlushTo(boolean)", "public void setAutoFlushTo(boolean)"], ["long", "org.apache.hadoop.hbase.client.HTableWrapper.getWriteBufferSize()", "public long getWriteBufferSize()"], ["void", "org.apache.hadoop.hbase.client.HTableWrapper.setWriteBufferSize(long)", "public void setWriteBufferSize(long) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.client.HTableWrapper.incrementColumnValue(byte[], byte[], byte[], long, boolean)", "public long incrementColumnValue(byte[], byte[], byte[], long, boolean) throws java.io.IOException"], ["<R extends com.google.protobuf.Message> java.util.Map<byte[], R>", "org.apache.hadoop.hbase.client.HTableWrapper.batchCoprocessorService(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, byte[], byte[], R)", "public <R extends com.google.protobuf.Message> java.util.Map<byte[], R> batchCoprocessorService(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, byte[], byte[], R) throws com.google.protobuf.ServiceException, java.lang.Throwable"], ["<R extends com.google.protobuf.Message> void", "org.apache.hadoop.hbase.client.HTableWrapper.batchCoprocessorService(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, byte[], byte[], R, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>)", "public <R extends com.google.protobuf.Message> void batchCoprocessorService(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, byte[], byte[], R, org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>) throws com.google.protobuf.ServiceException, java.lang.Throwable"], ["boolean", "org.apache.hadoop.hbase.client.HTableWrapper.checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.RowMutations)", "public boolean checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, byte[], org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.TableSnapshotScanner", "org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.TableSnapshotScanner", "org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.client.TableSnapshotScanner.next()", "public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.client.TableSnapshotScanner.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.client.TableSnapshotScanner.renewLease()", "public boolean renewLease()"], ["org.apache.hadoop.hbase.client.coprocessor.RowProcessorClient", "org.apache.hadoop.hbase.client.coprocessor.RowProcessorClient()", "public org.apache.hadoop.hbase.client.coprocessor.RowProcessorClient()"], ["<S extends com.google.protobuf.Message, T extends com.google.protobuf.Message> org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessRequest", "org.apache.hadoop.hbase.client.coprocessor.RowProcessorClient.getRowProcessorPB(org.apache.hadoop.hbase.regionserver.RowProcessor<S, T>)", "public static <S extends com.google.protobuf.Message, T extends com.google.protobuf.Message> org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessRequest getRowProcessorPB(org.apache.hadoop.hbase.regionserver.RowProcessor<S, T>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.codec.MessageCodec$MessageEncoder.write(org.apache.hadoop.hbase.Cell)", "public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.codec.MessageCodec", "org.apache.hadoop.hbase.codec.MessageCodec()", "public org.apache.hadoop.hbase.codec.MessageCodec()"], ["org.apache.hadoop.hbase.codec.Codec$Decoder", "org.apache.hadoop.hbase.codec.MessageCodec.getDecoder(java.io.InputStream)", "public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream)"], ["org.apache.hadoop.hbase.codec.Codec$Encoder", "org.apache.hadoop.hbase.codec.MessageCodec.getEncoder(java.io.OutputStream)", "public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream)"], ["org.apache.hadoop.hbase.conf.ConfigurationManager", "org.apache.hadoop.hbase.conf.ConfigurationManager()", "public org.apache.hadoop.hbase.conf.ConfigurationManager()"], ["void", "org.apache.hadoop.hbase.conf.ConfigurationManager.registerObserver(org.apache.hadoop.hbase.conf.ConfigurationObserver)", "public void registerObserver(org.apache.hadoop.hbase.conf.ConfigurationObserver)"], ["void", "org.apache.hadoop.hbase.conf.ConfigurationManager.deregisterObserver(org.apache.hadoop.hbase.conf.ConfigurationObserver)", "public void deregisterObserver(org.apache.hadoop.hbase.conf.ConfigurationObserver)"], ["void", "org.apache.hadoop.hbase.conf.ConfigurationManager.notifyAllObservers(org.apache.hadoop.conf.Configuration)", "public void notifyAllObservers(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.conf.ConfigurationManager.getNumObservers()", "public int getNumObservers()"], ["org.apache.hadoop.hbase.constraint.BaseConstraint", "org.apache.hadoop.hbase.constraint.BaseConstraint()", "public org.apache.hadoop.hbase.constraint.BaseConstraint()"], ["org.apache.hadoop.hbase.constraint.ConstraintException", "org.apache.hadoop.hbase.constraint.ConstraintException()", "public org.apache.hadoop.hbase.constraint.ConstraintException()"], ["org.apache.hadoop.hbase.constraint.ConstraintException", "org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String)", "public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String)"], ["org.apache.hadoop.hbase.constraint.ConstraintException", "org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String, java.lang.Throwable)", "public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.constraint.ConstraintProcessor", "org.apache.hadoop.hbase.constraint.ConstraintProcessor()", "public org.apache.hadoop.hbase.constraint.ConstraintProcessor()"], ["void", "org.apache.hadoop.hbase.constraint.ConstraintProcessor.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["void", "org.apache.hadoop.hbase.constraint.ConstraintProcessor.prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.constraint.Constraints$1.compare(org.apache.hadoop.hbase.constraint.Constraint, org.apache.hadoop.hbase.constraint.Constraint)", "public int compare(org.apache.hadoop.hbase.constraint.Constraint, org.apache.hadoop.hbase.constraint.Constraint)"], ["int", "org.apache.hadoop.hbase.constraint.Constraints$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.enable(org.apache.hadoop.hbase.HTableDescriptor)", "public static void enable(org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.disable(org.apache.hadoop.hbase.HTableDescriptor)", "public static void disable(org.apache.hadoop.hbase.HTableDescriptor)"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.remove(org.apache.hadoop.hbase.HTableDescriptor)", "public static void remove(org.apache.hadoop.hbase.HTableDescriptor)"], ["boolean", "org.apache.hadoop.hbase.constraint.Constraints.has(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)", "public static boolean has(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.add(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>...)", "public static void add(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>...) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.add(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.util.Pair<java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration>...)", "public static void add(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.util.Pair<java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration>...) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.add(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration)", "public static void add(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.setConfiguration(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration)", "public static void setConfiguration(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.IllegalArgumentException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.remove(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)", "public static void remove(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.enableConstraint(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)", "public static void enableConstraint(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.constraint.Constraints.disableConstraint(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)", "public static void disableConstraint(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.constraint.Constraints.enabled(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>)", "public static boolean enabled(org.apache.hadoop.hbase.HTableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException"], ["org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager", "org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager()", "public org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager()"], ["void", "org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager.initialize(org.apache.hadoop.hbase.Server)", "public void initialize(org.apache.hadoop.hbase.Server)"], ["void", "org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager.stop()", "public void stop()"], ["org.apache.hadoop.hbase.Server", "org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager.getServer()", "public org.apache.hadoop.hbase.Server getServer()"], ["org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails", "org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails(java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task>, org.apache.hadoop.hbase.master.MasterServices, java.util.Set<java.lang.String>, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails(java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task>, org.apache.hadoop.hbase.master.MasterServices, java.util.Set<java.lang.String>, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.master.MasterServices", "org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails.getMaster()", "public org.apache.hadoop.hbase.master.MasterServices getMaster()"], ["java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task>", "org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails.getTasks()", "public java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task> getTasks()"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails.getFailedDeletions()", "public java.util.Set<java.lang.String> getFailedDeletions()"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$1.finish(org.apache.hadoop.hbase.ServerName, java.lang.String)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status finish(org.apache.hadoop.hbase.ServerName, java.lang.String)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback.processResult(int, java.lang.String, java.lang.Object, java.lang.String)", "public void processResult(int, java.lang.String, java.lang.Object, java.lang.String)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback.processResult(int, java.lang.String, java.lang.Object, java.lang.String)", "public void processResult(int, java.lang.String, java.lang.Object, java.lang.String)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback.processResult(int, java.lang.String, java.lang.Object)", "public void processResult(int, java.lang.String, java.lang.Object)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback.processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat)", "public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status[]", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status.values()", "public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status[] values()"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.init()", "public void init() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.prepareTask(java.lang.String)", "public java.lang.String prepareTask(java.lang.String)"], ["int", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.remainingTasksInCoordination()", "public int remainingTasksInCoordination()"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.deleteTask(java.lang.String)", "public void deleteTask(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective)", "public boolean resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.checkTasks()", "public void checkTasks()"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.submitTask(java.lang.String)", "public void submitTask(java.lang.String)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.checkTaskStillAvailable(java.lang.String)", "public void checkTaskStillAvailable(java.lang.String)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.removeRecoveringRegions(java.util.Set<java.lang.String>, java.lang.Boolean)", "public void removeRecoveringRegions(java.util.Set<java.lang.String>, java.lang.Boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.markRegionsRecovering(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>)", "public void markRegionsRecovering(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException, java.io.InterruptedIOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.removeStaleRecoveringRegions(java.util.Set<java.lang.String>)", "public void removeStaleRecoveringRegions(java.util.Set<java.lang.String>) throws java.io.IOException, java.io.InterruptedIOException"], ["synchronized", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.boolean isReplaying()", "public synchronized boolean isReplaying()"], ["synchronized", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.boolean isSplitting()", "public synchronized boolean isSplitting()"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.setRecoveryMode(boolean)", "public void setRecoveryMode(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails)", "public void setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails)"], ["org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.getDetails()", "public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails getDetails()"], ["synchronized", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode getRecoveryMode()", "public synchronized org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode getRecoveryMode()"], ["long", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.getLastRecoveryTime()", "public long getLastRecoveryTime()"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.setIgnoreDeleteForTesting(boolean)", "public void setIgnoreDeleteForTesting(boolean)"], ["org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination$ZkSplitTransactionDetails", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination$ZkSplitTransactionDetails()", "public org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination$ZkSplitTransactionDetails()"], ["int", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination$ZkSplitTransactionDetails.getZnodeVersion()", "public int getZnodeVersion()"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination$ZkSplitTransactionDetails.setZnodeVersion(int)", "public void setZnodeVersion(int)"], ["org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.startSplitTransaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public void startSplitTransaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.waitForSplitTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails)", "public void waitForSplitTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.completeSplitTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails, org.apache.hadoop.hbase.regionserver.Region)", "public void completeSplitTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.clean(org.apache.hadoop.hbase.HRegionInfo)", "public void clean(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.getDefaultDetails()", "public org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails getDefaultDetails()"], ["int", "org.apache.hadoop.hbase.coordination.ZKSplitTransactionCoordination.processTransition(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails)", "public int processTransition(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.SplitTransactionCoordination$SplitTransactionDetails) throws java.io.IOException"], ["org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails()", "public org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails()"], ["org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails(boolean, int)", "public org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails(boolean, int)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails.isPublishStatusInZk()", "public boolean isPublishStatusInZk()"], ["void", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails.setPublishStatusInZk(boolean)", "public void setPublishStatusInZk(boolean)"], ["int", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails.getExpectedVersion()", "public int getExpectedVersion()"], ["void", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination$ZkCloseRegionDetails.setExpectedVersion(int)", "public void setExpectedVersion(int)"], ["org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination.checkClosingState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)", "public boolean checkClosingState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)"], ["void", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination.setClosedState(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)", "public void setClosedState(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)"], ["org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination.parseFromProtoRequest(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionRequest)", "public org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails parseFromProtoRequest(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionRequest)"], ["org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails", "org.apache.hadoop.hbase.coordination.ZkCloseRegionCoordination.getDetaultDetails()", "public org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails getDetaultDetails()"], ["org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager()", "public org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager()"], ["void", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.initialize(org.apache.hadoop.hbase.Server)", "public void initialize(org.apache.hadoop.hbase.Server)"], ["org.apache.hadoop.hbase.Server", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getServer()", "public org.apache.hadoop.hbase.Server getServer()"], ["org.apache.hadoop.hbase.TableStateManager", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getTableStateManager()", "public org.apache.hadoop.hbase.TableStateManager getTableStateManager() throws java.lang.InterruptedException, org.apache.hadoop.hbase.CoordinatedStateException"], ["org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getSplitLogWorkerCoordination()", "public org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination getSplitLogWorkerCoordination()"], ["org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getSplitLogManagerCoordination()", "public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination getSplitLogManagerCoordination()"], ["org.apache.hadoop.hbase.coordination.SplitTransactionCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getSplitTransactionCoordination()", "public org.apache.hadoop.hbase.coordination.SplitTransactionCoordination getSplitTransactionCoordination()"], ["org.apache.hadoop.hbase.coordination.CloseRegionCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getCloseRegionCoordination()", "public org.apache.hadoop.hbase.coordination.CloseRegionCoordination getCloseRegionCoordination()"], ["org.apache.hadoop.hbase.coordination.OpenRegionCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getOpenRegionCoordination()", "public org.apache.hadoop.hbase.coordination.OpenRegionCoordination getOpenRegionCoordination()"], ["org.apache.hadoop.hbase.coordination.RegionMergeCoordination", "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager.getRegionMergeCoordination()", "public org.apache.hadoop.hbase.coordination.RegionMergeCoordination getRegionMergeCoordination()"], ["org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails()", "public org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails()"], ["org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails(int)", "public org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails(int)"], ["int", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.getVersionOfOfflineNode()", "public int getVersionOfOfflineNode()"], ["void", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.setVersionOfOfflineNode(int)", "public void setVersionOfOfflineNode(int)"], ["int", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.getVersion()", "public int getVersion()"], ["void", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.setVersion(int)", "public void setVersion(int)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["void", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination$ZkOpenRegionDetails.setServerName(org.apache.hadoop.hbase.ServerName)", "public void setServerName(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.transitionToOpened(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public boolean transitionToOpened(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.transitionFromOfflineToOpening(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public boolean transitionFromOfflineToOpening(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.tickleOpening(org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.lang.String)", "public boolean tickleOpening(org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.tryTransitionFromOfflineToFailedOpen(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public boolean tryTransitionFromOfflineToFailedOpen(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.tryTransitionFromOpeningToFailedOpen(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public boolean tryTransitionFromOpeningToFailedOpen(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.parseFromProtoRequest(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionRequest$RegionOpenInfo)", "public org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails parseFromProtoRequest(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionRequest$RegionOpenInfo)"], ["org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.getDetailsForNonCoordinatedOpening()", "public org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails getDetailsForNonCoordinatedOpening()"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkOpenRegionCoordination.commitOpenOnMasterSide(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public boolean commitOpenOnMasterSide(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination$ZkRegionMergeDetails", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination$ZkRegionMergeDetails()", "public org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination$ZkRegionMergeDetails()"], ["int", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination$ZkRegionMergeDetails.getZnodeVersion()", "public int getZnodeVersion()"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination$ZkRegionMergeDetails.setZnodeVersion(int)", "public void setZnodeVersion(int)"], ["org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination(org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.getDefaultDetails()", "public org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails getDefaultDetails()"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.waitForRegionMergeTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails)", "public void waitForRegionMergeTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.startRegionMergeTransaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public void startRegionMergeTransaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.clean(org.apache.hadoop.hbase.HRegionInfo)", "public void clean(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.completeRegionMergeTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails, org.apache.hadoop.hbase.regionserver.HRegion)", "public void completeRegionMergeTransaction(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.confirmRegionMergeTransaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails)", "public void confirmRegionMergeTransaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZkRegionMergeCoordination.processRegionMergeRequest(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails)", "public void processRegionMergeRequest(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.coordination.RegionMergeCoordination$RegionMergeDetails) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$1.progress()", "public boolean progress()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$GetDataAsyncCallback.processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat)", "public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat)"], ["org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails()", "public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails()"], ["org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails(java.lang.String, org.apache.commons.lang.mutable.MutableInt)", "public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails(java.lang.String, org.apache.commons.lang.mutable.MutableInt)"], ["java.lang.String", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.getTaskNode()", "public java.lang.String getTaskNode()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.setTaskNode(java.lang.String)", "public void setTaskNode(java.lang.String)"], ["org.apache.commons.lang.mutable.MutableInt", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.getCurTaskZKVersion()", "public org.apache.commons.lang.mutable.MutableInt getCurTaskZKVersion()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.setCurTaskZKVersion(org.apache.commons.lang.mutable.MutableInt)", "public void setCurTaskZKVersion(org.apache.commons.lang.mutable.MutableInt)"], ["java.lang.String", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.getWALFile()", "public java.lang.String getWALFile()"], ["org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination(org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination(org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker)", "public void init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker)"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.getDataSetWatchAsync()", "public void getDataSetWatchAsync()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.taskLoop()", "public void taskLoop() throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem)", "public void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.isReady()", "public boolean isReady() throws java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.getTaskReadySeq()", "public int getTaskReadySeq()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.registerListener()", "public void registerListener()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.removeListener()", "public void removeListener()"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.stopProcessingTasks()", "public void stopProcessingTasks()"], ["boolean", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.isStop()", "public boolean isStop()"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.getRegionFlushedSequenceId(java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getRegionFlushedSequenceId(java.lang.String, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.AtomicLong, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails)", "public void endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.AtomicLong, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails)"], ["org.apache.hadoop.hbase.coprocessor.AggregateImplementation", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation()", "public org.apache.hadoop.hbase.coprocessor.AggregateImplementation()"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getMax(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getMax(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getMin(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getMin(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getSum(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getSum(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getRowNum(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getRowNum(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getAvg(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getAvg(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getStd(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getStd(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getMedian(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)", "public void getMedian(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AggregateProtos$AggregateResponse>)"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.getService()", "public com.google.protobuf.Service getService()"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.AggregateImplementation.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver()", "public org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void preListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void postAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void postUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void postRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>)", "public void postBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean)", "public boolean preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean)", "public void postBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void postMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void preGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.postSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseMasterObserver", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver()", "public org.apache.hadoop.hbase.coprocessor.BaseMasterObserver()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDeleteTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTruncateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void preListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumnHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postEnableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDisableTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void postAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void postUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void postRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>)", "public void postBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean)", "public boolean preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean)", "public void postBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void postMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void preGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.postSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseRegionObserver", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver()", "public org.apache.hadoop.hbase.coprocessor.BaseRegionObserver()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean)", "public void preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean)", "public void postClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean)"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preFlushScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preFlushScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile)", "public void postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[])", "public void preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preSplitBeforePONR(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>)", "public void preSplitBeforePONR(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preSplitAfterPONR(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preSplitAfterPONR(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preRollBackSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preRollBackSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postRollBackSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postRollBackSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCompleteSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postCompleteSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void preCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void preCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void postCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void postCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile)", "public void postCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void postCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result)", "public void preGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result)", "public void postGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public void postGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean)", "public boolean preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean)", "public boolean postExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get)", "public void prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void postBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postBatchMutateIndispensably(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean)", "public void postBatchMutateIndispensably(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean postCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean postCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preAppendAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppendAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result)", "public org.apache.hadoop.hbase.client.Result postAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean)", "public long preIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean, long)", "public long postIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preIncrementAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrementAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result)", "public org.apache.hadoop.hbase.client.Result postIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.KeyValueScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preStoreScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.regionserver.KeyValueScanner)", "public org.apache.hadoop.hbase.regionserver.KeyValueScanner preStoreScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.regionserver.KeyValueScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean)", "public boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean)", "public boolean postScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postScannerFilterRow(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, byte[], int, short, boolean)", "public boolean postScannerFilterRow(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, byte[], int, short, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>)", "public void preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean)", "public boolean postBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.preStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader preStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader postStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postStartRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void postStartRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postCloseRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void postCloseRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker", "org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.DeleteTracker)", "public org.apache.hadoop.hbase.regionserver.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.DeleteTracker) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver()", "public org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void preMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.client.Mutation>)", "public void preMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void preRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.postReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void postReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint", "org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint()", "public org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint.process(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessResponse>)", "public void process(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos$ProcessResponse>)"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint.getService()", "public com.google.protobuf.Service getService()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.BaseWALObserver", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver()", "public org.apache.hadoop.hbase.coprocessor.BaseWALObserver()"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.preWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public boolean preWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.preWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public boolean preWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.postWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.BaseWALObserver.postWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$1.compare(java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>, java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>)", "public int compare(java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>, java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>)"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.startup()", "public void startup() throws java.io.IOException"], ["org.apache.hadoop.hbase.Coprocessor", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getInstance()", "public org.apache.hadoop.hbase.Coprocessor getInstance()"], ["java.lang.ClassLoader", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getClassLoader()", "public java.lang.ClassLoader getClassLoader()"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getPriority()", "public int getPriority()"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getLoadSequence()", "public int getLoadSequence()"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getVersion()", "public int getVersion()"], ["java.lang.String", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getHBaseVersion()", "public java.lang.String getHBaseVersion()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getTable(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.HTableInterface", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.getTable(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService)", "public org.apache.hadoop.hbase.client.HTableInterface getTable(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$EnvironmentPriorityComparator.compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment)", "public int compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["int", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost$EnvironmentPriorityComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.coprocessor.CoprocessorHost", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost(org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.coprocessor.CoprocessorHost(org.apache.hadoop.hbase.Abortable)"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.getLoadedCoprocessors()", "public static java.util.Set<java.lang.String> getLoadedCoprocessors()"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.getCoprocessors()", "public java.util.Set<java.lang.String> getCoprocessors()"], ["E", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration)", "public E load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.load(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration)", "public void load(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["E", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.loadInstance(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration)", "public E loadInstance(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.shutdown(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void shutdown(org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["org.apache.hadoop.hbase.Coprocessor", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.findCoprocessor(java.lang.String)", "public org.apache.hadoop.hbase.Coprocessor findCoprocessor(java.lang.String)"], ["<T extends org.apache.hadoop.hbase.Coprocessor> java.util.List<T>", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.findCoprocessors(java.lang.Class<T>)", "public <T extends org.apache.hadoop.hbase.Coprocessor> java.util.List<T> findCoprocessors(java.lang.Class<T>)"], ["org.apache.hadoop.hbase.CoprocessorEnvironment", "org.apache.hadoop.hbase.coprocessor.CoprocessorHost.findCoprocessorEnvironment(java.lang.String)", "public org.apache.hadoop.hbase.CoprocessorEnvironment findCoprocessorEnvironment(java.lang.String)"], ["org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint", "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint()", "public org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint()"], ["void", "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.mutateRows(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MutateRowsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MutateRowsResponse>)", "public void mutateRows(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MutateRowsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MutateRowsResponse>)"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.getService()", "public com.google.protobuf.Service getService()"], ["void", "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.coprocessor.ObserverContext", "org.apache.hadoop.hbase.coprocessor.ObserverContext()", "public org.apache.hadoop.hbase.coprocessor.ObserverContext()"], ["E", "org.apache.hadoop.hbase.coprocessor.ObserverContext.getEnvironment()", "public E getEnvironment()"], ["void", "org.apache.hadoop.hbase.coprocessor.ObserverContext.prepare(E)", "public void prepare(E)"], ["void", "org.apache.hadoop.hbase.coprocessor.ObserverContext.bypass()", "public void bypass()"], ["void", "org.apache.hadoop.hbase.coprocessor.ObserverContext.complete()", "public void complete()"], ["boolean", "org.apache.hadoop.hbase.coprocessor.ObserverContext.shouldBypass()", "public boolean shouldBypass()"], ["boolean", "org.apache.hadoop.hbase.coprocessor.ObserverContext.shouldComplete()", "public boolean shouldComplete()"], ["<T extends org.apache.hadoop.hbase.CoprocessorEnvironment> org.apache.hadoop.hbase.coprocessor.ObserverContext<T>", "org.apache.hadoop.hbase.coprocessor.ObserverContext.createAndPrepare(T, org.apache.hadoop.hbase.coprocessor.ObserverContext<T>)", "public static <T extends org.apache.hadoop.hbase.CoprocessorEnvironment> org.apache.hadoop.hbase.coprocessor.ObserverContext<T> createAndPrepare(T, org.apache.hadoop.hbase.coprocessor.ObserverContext<T>)"], ["org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType[]", "org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType.values()", "public static org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType[] values()"], ["org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType", "org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.Throwable)", "public org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.errorhandling.ForeignException.getSource()", "public java.lang.String getSource()"], ["boolean", "org.apache.hadoop.hbase.errorhandling.ForeignException.isRemote()", "public boolean isRemote()"], ["java.lang.String", "org.apache.hadoop.hbase.errorhandling.ForeignException.toString()", "public java.lang.String toString()"], ["byte[]", "org.apache.hadoop.hbase.errorhandling.ForeignException.serialize(java.lang.String, java.lang.Throwable)", "public static byte[] serialize(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.errorhandling.ForeignException.deserialize(byte[])", "public static org.apache.hadoop.hbase.errorhandling.ForeignException deserialize(byte[]) throws com.google.protobuf.InvalidProtocolBufferException"], ["org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher(java.lang.String)", "public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher(java.lang.String)"], ["org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher()", "public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher()"], ["java.lang.String", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.getName()", "public java.lang.String getName()"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.void receive(org.apache.hadoop.hbase.errorhandling.ForeignException)", "public synchronized void receive(org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.void rethrowException()", "public synchronized void rethrowException() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.boolean hasException()", "public synchronized boolean hasException()"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.org.apache.hadoop.hbase.errorhandling.ForeignException getException()", "public synchronized org.apache.hadoop.hbase.errorhandling.ForeignException getException()"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.void addListener(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener)", "public synchronized void addListener(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener)"], ["org.apache.hadoop.hbase.errorhandling.TimeoutException", "org.apache.hadoop.hbase.errorhandling.TimeoutException(java.lang.String, long, long, long)", "public org.apache.hadoop.hbase.errorhandling.TimeoutException(java.lang.String, long, long, long)"], ["long", "org.apache.hadoop.hbase.errorhandling.TimeoutException.getStart()", "public long getStart()"], ["long", "org.apache.hadoop.hbase.errorhandling.TimeoutException.getEnd()", "public long getEnd()"], ["long", "org.apache.hadoop.hbase.errorhandling.TimeoutException.getMaxAllowedOperationTime()", "public long getMaxAllowedOperationTime()"], ["java.lang.String", "org.apache.hadoop.hbase.errorhandling.TimeoutException.getSourceName()", "public java.lang.String getSourceName()"], ["void", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector$1.run()", "public void run()"], ["org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener, long)", "public org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener, long)"], ["long", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.getMaxTime()", "public long getMaxTime()"], ["void", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.complete()", "public void complete()"], ["synchronized", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.void start()", "public synchronized void start() throws java.lang.IllegalStateException"], ["void", "org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.trigger()", "public void trigger()"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.executor.EventHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.executor.EventType)", "public org.apache.hadoop.hbase.executor.EventHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.executor.EventType)"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.executor.EventHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.executor.EventHandler.run()", "public void run()"], ["org.apache.hadoop.hbase.executor.EventType", "org.apache.hadoop.hbase.executor.EventHandler.getEventType()", "public org.apache.hadoop.hbase.executor.EventType getEventType()"], ["int", "org.apache.hadoop.hbase.executor.EventHandler.getPriority()", "public int getPriority()"], ["long", "org.apache.hadoop.hbase.executor.EventHandler.getSeqid()", "public long getSeqid()"], ["int", "org.apache.hadoop.hbase.executor.EventHandler.compareTo(java.lang.Runnable)", "public int compareTo(java.lang.Runnable)"], ["synchronized", "org.apache.hadoop.hbase.executor.EventHandler.org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener getListener()", "public synchronized org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener getListener()"], ["synchronized", "org.apache.hadoop.hbase.executor.EventHandler.void setListener(org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener)", "public synchronized void setListener(org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener)"], ["java.lang.String", "org.apache.hadoop.hbase.executor.EventHandler.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.executor.EventHandler.getInformativeName()", "public java.lang.String getInformativeName()"], ["int", "org.apache.hadoop.hbase.executor.EventHandler.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.executor.ExecutorService$Executor.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus", "org.apache.hadoop.hbase.executor.ExecutorService$Executor.getStatus()", "public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus getStatus()"], ["void", "org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus.dumpTo(java.io.Writer, java.lang.String)", "public void dumpTo(java.io.Writer, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus", "org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus(java.lang.Thread, org.apache.hadoop.hbase.executor.EventHandler)", "public org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus(java.lang.Thread, org.apache.hadoop.hbase.executor.EventHandler)"], ["org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor", "org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor(int, int, long, java.util.concurrent.TimeUnit, java.util.concurrent.BlockingQueue<java.lang.Runnable>)", "public org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor(int, int, long, java.util.concurrent.TimeUnit, java.util.concurrent.BlockingQueue<java.lang.Runnable>)"], ["java.util.concurrent.ConcurrentMap<java.lang.Thread, java.lang.Runnable>", "org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor.getRunningTasks()", "public java.util.concurrent.ConcurrentMap<java.lang.Thread, java.lang.Runnable> getRunningTasks()"], ["org.apache.hadoop.hbase.executor.ExecutorService", "org.apache.hadoop.hbase.executor.ExecutorService(java.lang.String)", "public org.apache.hadoop.hbase.executor.ExecutorService(java.lang.String)"], ["void", "org.apache.hadoop.hbase.executor.ExecutorService.shutdown()", "public void shutdown()"], ["void", "org.apache.hadoop.hbase.executor.ExecutorService.startExecutorService(org.apache.hadoop.hbase.executor.ExecutorType, int)", "public void startExecutorService(org.apache.hadoop.hbase.executor.ExecutorType, int)"], ["void", "org.apache.hadoop.hbase.executor.ExecutorService.submit(org.apache.hadoop.hbase.executor.EventHandler)", "public void submit(org.apache.hadoop.hbase.executor.EventHandler)"], ["void", "org.apache.hadoop.hbase.executor.ExecutorService.registerListener(org.apache.hadoop.hbase.executor.EventType, org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener)", "public void registerListener(org.apache.hadoop.hbase.executor.EventType, org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener)"], ["org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener", "org.apache.hadoop.hbase.executor.ExecutorService.unregisterListener(org.apache.hadoop.hbase.executor.EventType)", "public org.apache.hadoop.hbase.executor.EventHandler$EventHandlerListener unregisterListener(org.apache.hadoop.hbase.executor.EventType)"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus>", "org.apache.hadoop.hbase.executor.ExecutorService.getAllExecutorStatuses()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus> getAllExecutorStatuses()"], ["java.lang.Object", "org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[])", "public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable"], ["void", "org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks.reorderBlocks(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hdfs.protocol.LocatedBlocks, java.lang.String)", "public void reorderBlocks(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hdfs.protocol.LocatedBlocks, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.fs.HFileSystem", "org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.conf.Configuration, boolean)", "public org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.fs.HFileSystem", "org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.fs.HFileSystem.getNoChecksumFs()", "public org.apache.hadoop.fs.FileSystem getNoChecksumFs()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.fs.HFileSystem.getBackingFs()", "public org.apache.hadoop.fs.FileSystem getBackingFs() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.fs.HFileSystem.useHBaseChecksum()", "public boolean useHBaseChecksum()"], ["void", "org.apache.hadoop.hbase.fs.HFileSystem.close()", "public void close() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.fs.HFileSystem.addLocationsOrderInterceptor(org.apache.hadoop.conf.Configuration)", "public static boolean addLocationsOrderInterceptor(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.fs.HFileSystem.get(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.fs.HFileSystem.getLocalFs(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.fs.FileSystem getLocalFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.fs.FSDataOutputStream", "org.apache.hadoop.hbase.fs.HFileSystem.createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable)", "public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.generated.master.master_jsp", "org.apache.hadoop.hbase.generated.master.master_jsp()", "public org.apache.hadoop.hbase.generated.master.master_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.master.master_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.master.master_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.generated.master.snapshot_jsp", "org.apache.hadoop.hbase.generated.master.snapshot_jsp()", "public org.apache.hadoop.hbase.generated.master.snapshot_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.master.snapshot_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.master.snapshot_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.generated.master.table_jsp", "org.apache.hadoop.hbase.generated.master.table_jsp()", "public org.apache.hadoop.hbase.generated.master.table_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.master.table_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.master.table_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp", "org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp()", "public org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.generated.master.zk_jsp", "org.apache.hadoop.hbase.generated.master.zk_jsp()", "public org.apache.hadoop.hbase.generated.master.zk_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.master.zk_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.master.zk_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp", "org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp()", "public org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp()"], ["java.lang.Object", "org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp.getDependants()", "public java.lang.Object getDependants()"], ["void", "org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp._jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.http.AdminAuthorizedServlet", "org.apache.hadoop.hbase.http.AdminAuthorizedServlet()", "public org.apache.hadoop.hbase.http.AdminAuthorizedServlet()"], ["org.apache.hadoop.hbase.http.FilterInitializer", "org.apache.hadoop.hbase.http.FilterInitializer()", "public org.apache.hadoop.hbase.http.FilterInitializer()"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting$1.write(byte[], int, int)", "public void write(byte[], int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting$1.write(int)", "public void write(int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting$1.flush()", "public void flush() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting$1.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HtmlQuoting", "org.apache.hadoop.hbase.http.HtmlQuoting()", "public org.apache.hadoop.hbase.http.HtmlQuoting()"], ["boolean", "org.apache.hadoop.hbase.http.HtmlQuoting.needsQuoting(byte[], int, int)", "public static boolean needsQuoting(byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.http.HtmlQuoting.needsQuoting(java.lang.String)", "public static boolean needsQuoting(java.lang.String)"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting.quoteHtmlChars(java.io.OutputStream, byte[], int, int)", "public static void quoteHtmlChars(java.io.OutputStream, byte[], int, int) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.http.HtmlQuoting.quoteHtmlChars(java.lang.String)", "public static java.lang.String quoteHtmlChars(java.lang.String)"], ["java.io.OutputStream", "org.apache.hadoop.hbase.http.HtmlQuoting.quoteOutputStream(java.io.OutputStream)", "public static java.io.OutputStream quoteOutputStream(java.io.OutputStream) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.http.HtmlQuoting.unquoteHtmlChars(java.lang.String)", "public static java.lang.String unquoteHtmlChars(java.lang.String)"], ["void", "org.apache.hadoop.hbase.http.HtmlQuoting.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.http.HttpConfig$Policy[]", "org.apache.hadoop.hbase.http.HttpConfig$Policy.values()", "public static org.apache.hadoop.hbase.http.HttpConfig$Policy[] values()"], ["org.apache.hadoop.hbase.http.HttpConfig$Policy", "org.apache.hadoop.hbase.http.HttpConfig$Policy.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.http.HttpConfig$Policy valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpConfig$Policy", "org.apache.hadoop.hbase.http.HttpConfig$Policy.fromString(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpConfig$Policy fromString(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.http.HttpConfig$Policy.isHttpEnabled()", "public boolean isHttpEnabled()"], ["boolean", "org.apache.hadoop.hbase.http.HttpConfig$Policy.isHttpsEnabled()", "public boolean isHttpsEnabled()"], ["org.apache.hadoop.hbase.http.HttpConfig", "org.apache.hadoop.hbase.http.HttpConfig(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.http.HttpConfig(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.http.HttpConfig.setPolicy(org.apache.hadoop.hbase.http.HttpConfig$Policy)", "public void setPolicy(org.apache.hadoop.hbase.http.HttpConfig$Policy)"], ["boolean", "org.apache.hadoop.hbase.http.HttpConfig.isSecure()", "public boolean isSecure()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpConfig.getSchemePrefix()", "public java.lang.String getSchemePrefix()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpConfig.getScheme(org.apache.hadoop.hbase.http.HttpConfig$Policy)", "public java.lang.String getScheme(org.apache.hadoop.hbase.http.HttpConfig$Policy)"], ["org.apache.hadoop.hbase.http.HttpRequestLog", "org.apache.hadoop.hbase.http.HttpRequestLog()", "public org.apache.hadoop.hbase.http.HttpRequestLog()"], ["org.mortbay.jetty.RequestLog", "org.apache.hadoop.hbase.http.HttpRequestLog.getRequestLog(java.lang.String)", "public static org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpRequestLogAppender", "org.apache.hadoop.hbase.http.HttpRequestLogAppender()", "public org.apache.hadoop.hbase.http.HttpRequestLogAppender()"], ["void", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.setRetainDays(int)", "public void setRetainDays(int)"], ["int", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.getRetainDays()", "public int getRetainDays()"], ["void", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.setFilename(java.lang.String)", "public void setFilename(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.getFilename()", "public java.lang.String getFilename()"], ["void", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.append(org.apache.log4j.spi.LoggingEvent)", "public void append(org.apache.log4j.spi.LoggingEvent)"], ["void", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.http.HttpRequestLogAppender.requiresLayout()", "public boolean requiresLayout()"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder()", "public org.apache.hadoop.hbase.http.HttpServer$Builder()"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.addEndpoint(java.net.URI)", "public org.apache.hadoop.hbase.http.HttpServer$Builder addEndpoint(java.net.URI)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.hostName(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder hostName(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.trustStore(java.lang.String, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder trustStore(java.lang.String, java.lang.String, java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.keyStore(java.lang.String, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder keyStore(java.lang.String, java.lang.String, java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.keyPassword(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder keyPassword(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.needsClientAuth(boolean)", "public org.apache.hadoop.hbase.http.HttpServer$Builder needsClientAuth(boolean)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setName(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setName(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setBindAddress(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setBindAddress(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setPort(int)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setPort(int)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setFindPort(boolean)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setFindPort(boolean)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setConf(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setConnector(org.mortbay.jetty.Connector)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setConnector(org.mortbay.jetty.Connector)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setPathSpec(java.lang.String[])", "public org.apache.hadoop.hbase.http.HttpServer$Builder setPathSpec(java.lang.String[])"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setACL(org.apache.hadoop.security.authorize.AccessControlList)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setACL(org.apache.hadoop.security.authorize.AccessControlList)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setSecurityEnabled(boolean)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setSecurityEnabled(boolean)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setUsernameConfKey(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setUsernameConfKey(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setKeytabConfKey(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setKeytabConfKey(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setAppDir(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setAppDir(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer$Builder", "org.apache.hadoop.hbase.http.HttpServer$Builder.setLogDir(java.lang.String)", "public org.apache.hadoop.hbase.http.HttpServer$Builder setLogDir(java.lang.String)"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer$Builder.build()", "public org.apache.hadoop.hbase.http.HttpServer build() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter$1.hasMoreElements()", "public boolean hasMoreElements()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter$1.nextElement()", "public java.lang.String nextElement()"], ["java.lang.Object", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter$1.nextElement()", "public java.lang.Object nextElement()"], ["org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter(javax.servlet.http.HttpServletRequest)", "public org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter(javax.servlet.http.HttpServletRequest)"], ["java.util.Enumeration<java.lang.String>", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameterNames()", "public java.util.Enumeration<java.lang.String> getParameterNames()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameter(java.lang.String)", "public java.lang.String getParameter(java.lang.String)"], ["java.lang.String[]", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameterValues(java.lang.String)", "public java.lang.String[] getParameterValues(java.lang.String)"], ["java.util.Map<java.lang.String, java.lang.String[]>", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameterMap()", "public java.util.Map<java.lang.String, java.lang.String[]> getParameterMap()"], ["java.lang.StringBuffer", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getRequestURL()", "public java.lang.StringBuffer getRequestURL()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter$RequestQuoter.getServerName()", "public java.lang.String getServerName()"], ["org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter()", "public org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter()"], ["void", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.init(javax.servlet.FilterConfig)", "public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException"], ["void", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.destroy()", "public void destroy()"], ["void", "org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain)", "public void doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain) throws java.io.IOException, javax.servlet.ServletException"], ["org.apache.hadoop.hbase.http.HttpServer$StackServlet", "org.apache.hadoop.hbase.http.HttpServer$StackServlet()", "public org.apache.hadoop.hbase.http.HttpServer$StackServlet()"], ["void", "org.apache.hadoop.hbase.http.HttpServer$StackServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean)", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector)", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[])", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList)", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector)", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException"], ["org.apache.hadoop.hbase.http.HttpServer", "org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[])", "public org.apache.hadoop.hbase.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException"], ["org.mortbay.jetty.Connector", "org.apache.hadoop.hbase.http.HttpServer.createBaseListener(org.apache.hadoop.conf.Configuration)", "public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.mortbay.jetty.Connector", "org.apache.hadoop.hbase.http.HttpServer.createDefaultChannelConnector()", "public static org.mortbay.jetty.Connector createDefaultChannelConnector()"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addContext(org.mortbay.jetty.servlet.Context, boolean)", "public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.HttpServer.setAttribute(java.lang.String, java.lang.Object)", "public void setAttribute(java.lang.String, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addJerseyResourcePackage(java.lang.String, java.lang.String)", "public void addJerseyResourcePackage(java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)", "public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)", "public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean)", "public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>)", "public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>)", "public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[])", "public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[])"], ["java.lang.Object", "org.apache.hadoop.hbase.http.HttpServer.getAttribute(java.lang.String)", "public java.lang.Object getAttribute(java.lang.String)"], ["org.mortbay.jetty.webapp.WebAppContext", "org.apache.hadoop.hbase.http.HttpServer.getWebAppContext()", "public org.mortbay.jetty.webapp.WebAppContext getWebAppContext()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpServer.getWebAppsPath(java.lang.String)", "public java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException"], ["int", "org.apache.hadoop.hbase.http.HttpServer.getPort()", "public int getPort()"], ["java.net.InetSocketAddress", "org.apache.hadoop.hbase.http.HttpServer.getConnectorAddress(int)", "public java.net.InetSocketAddress getConnectorAddress(int)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.setThreads(int, int)", "public void setThreads(int, int)"], ["void", "org.apache.hadoop.hbase.http.HttpServer.start()", "public void start() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.HttpServer.stop()", "public void stop() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.http.HttpServer.join()", "public void join() throws java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.http.HttpServer.isAlive()", "public boolean isAlive()"], ["java.lang.String", "org.apache.hadoop.hbase.http.HttpServer.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.http.HttpServer.isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.http.HttpServer.hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.http.HttpServer.userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String)", "public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String)"], ["org.apache.hadoop.hbase.http.InfoServer", "org.apache.hadoop.hbase.http.InfoServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.http.InfoServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.http.InfoServer.addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)", "public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>)"], ["void", "org.apache.hadoop.hbase.http.InfoServer.setAttribute(java.lang.String, java.lang.Object)", "public void setAttribute(java.lang.String, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.http.InfoServer.start()", "public void start() throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.http.InfoServer.getPort()", "public int getPort()"], ["void", "org.apache.hadoop.hbase.http.InfoServer.stop()", "public void stop() throws java.lang.Exception"], ["org.apache.hadoop.hbase.http.NoCacheFilter", "org.apache.hadoop.hbase.http.NoCacheFilter()", "public org.apache.hadoop.hbase.http.NoCacheFilter()"], ["void", "org.apache.hadoop.hbase.http.NoCacheFilter.init(javax.servlet.FilterConfig)", "public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException"], ["void", "org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain)", "public void doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain) throws java.io.IOException, javax.servlet.ServletException"], ["void", "org.apache.hadoop.hbase.http.NoCacheFilter.destroy()", "public void destroy()"], ["org.apache.hadoop.hbase.http.SslSocketConnectorSecure", "org.apache.hadoop.hbase.http.SslSocketConnectorSecure()", "public org.apache.hadoop.hbase.http.SslSocketConnectorSecure()"], ["org.apache.hadoop.hbase.http.conf.ConfServlet$BadFormatException", "org.apache.hadoop.hbase.http.conf.ConfServlet$BadFormatException(java.lang.String)", "public org.apache.hadoop.hbase.http.conf.ConfServlet$BadFormatException(java.lang.String)"], ["org.apache.hadoop.hbase.http.conf.ConfServlet", "org.apache.hadoop.hbase.http.conf.ConfServlet()", "public org.apache.hadoop.hbase.http.conf.ConfServlet()"], ["void", "org.apache.hadoop.hbase.http.conf.ConfServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException"], ["org.apache.hadoop.hbase.http.jmx.JMXJsonServlet", "org.apache.hadoop.hbase.http.jmx.JMXJsonServlet()", "public org.apache.hadoop.hbase.http.jmx.JMXJsonServlet()"], ["void", "org.apache.hadoop.hbase.http.jmx.JMXJsonServlet.init()", "public void init() throws javax.servlet.ServletException"], ["void", "org.apache.hadoop.hbase.http.jmx.JMXJsonServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)"], ["java.security.Principal", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter$1.getUserPrincipal()", "public java.security.Principal getUserPrincipal()"], ["java.lang.String", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter$1.getRemoteUser()", "public java.lang.String getRemoteUser()"], ["org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter()", "public org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter()"], ["void", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.destroy()", "public void destroy()"], ["void", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain)", "public void doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain) throws java.io.IOException, javax.servlet.ServletException"], ["void", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.init(javax.servlet.FilterConfig)", "public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException"], ["org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User(java.lang.String)", "public org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User.getName()", "public java.lang.String getName()"], ["int", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$User.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.http.lib.StaticUserWebFilter", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter()", "public org.apache.hadoop.hbase.http.lib.StaticUserWebFilter()"], ["void", "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter.initFilter(org.apache.hadoop.hbase.http.FilterContainer, org.apache.hadoop.conf.Configuration)", "public void initFilter(org.apache.hadoop.hbase.http.FilterContainer, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.http.log.LogLevel$Servlet", "org.apache.hadoop.hbase.http.log.LogLevel$Servlet()", "public org.apache.hadoop.hbase.http.log.LogLevel$Servlet()"], ["void", "org.apache.hadoop.hbase.http.log.LogLevel$Servlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException"], ["org.apache.hadoop.hbase.http.log.LogLevel", "org.apache.hadoop.hbase.http.log.LogLevel()", "public org.apache.hadoop.hbase.http.log.LogLevel()"], ["void", "org.apache.hadoop.hbase.http.log.LogLevel.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["java.io.OutputStream", "org.apache.hadoop.hbase.io.DataOutputOutputStream.constructOutputStream(java.io.DataOutput)", "public static java.io.OutputStream constructOutputStream(java.io.DataOutput)"], ["void", "org.apache.hadoop.hbase.io.DataOutputOutputStream.write(int)", "public void write(int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.DataOutputOutputStream.write(byte[], int, int)", "public void write(byte[], int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.DataOutputOutputStream.write(byte[])", "public void write(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.FSDataInputStreamWrapper", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.FSDataInputStreamWrapper", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink)", "public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.prepareForBlockReader(boolean)", "public void prepareForBlockReader(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.FSDataInputStreamWrapper", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream)", "public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream)"], ["org.apache.hadoop.hbase.io.FSDataInputStreamWrapper", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.fs.FSDataInputStream)", "public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.fs.FSDataInputStream)"], ["boolean", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.shouldUseHBaseChecksum()", "public boolean shouldUseHBaseChecksum()"], ["org.apache.hadoop.fs.FSDataInputStream", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.getStream(boolean)", "public org.apache.hadoop.fs.FSDataInputStream getStream(boolean)"], ["org.apache.hadoop.fs.FSDataInputStream", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.fallbackToFsChecksum(int)", "public org.apache.hadoop.fs.FSDataInputStream fallbackToFsChecksum(int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.checksumOk()", "public void checksumOk()"], ["void", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.fs.HFileSystem", "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.getHfs()", "public org.apache.hadoop.hbase.fs.HFileSystem getHfs()"], ["org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink)", "public org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink, int)", "public org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read()", "public int read() throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read(byte[])", "public int read(byte[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read(byte[], int, int)", "public int read(byte[], int, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read(long, byte[], int, int)", "public int read(long, byte[], int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.readFully(long, byte[])", "public void readFully(long, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.readFully(long, byte[], int, int)", "public void readFully(long, byte[], int, int) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.skip(long)", "public long skip(long) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.available()", "public int available() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.seek(long)", "public void seek(long) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.getPos()", "public long getPos() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.seekToNewSource(long)", "public boolean seekToNewSource(long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.close()", "public void close() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.void mark(int)", "public synchronized void mark(int)"], ["synchronized", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.void reset()", "public synchronized void reset() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.markSupported()", "public boolean markSupported()"], ["org.apache.hadoop.hbase.io.FileLink", "org.apache.hadoop.hbase.io.FileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path...)", "public org.apache.hadoop.hbase.io.FileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path...)"], ["org.apache.hadoop.hbase.io.FileLink", "org.apache.hadoop.hbase.io.FileLink(java.util.Collection<org.apache.hadoop.fs.Path>)", "public org.apache.hadoop.hbase.io.FileLink(java.util.Collection<org.apache.hadoop.fs.Path>)"], ["org.apache.hadoop.fs.Path[]", "org.apache.hadoop.hbase.io.FileLink.getLocations()", "public org.apache.hadoop.fs.Path[] getLocations()"], ["java.lang.String", "org.apache.hadoop.hbase.io.FileLink.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.io.FileLink.exists(org.apache.hadoop.fs.FileSystem)", "public boolean exists(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.FileLink.getAvailablePath(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.fs.Path getAvailablePath(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus", "org.apache.hadoop.hbase.io.FileLink.getFileStatus(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.FSDataInputStream", "org.apache.hadoop.hbase.io.FileLink.open(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.FSDataInputStream", "org.apache.hadoop.hbase.io.FileLink.open(org.apache.hadoop.fs.FileSystem, int)", "public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.FileSystem, int) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.FileLink.getBackReferencesDir(org.apache.hadoop.fs.Path, java.lang.String)", "public static org.apache.hadoop.fs.Path getBackReferencesDir(org.apache.hadoop.fs.Path, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.FileLink.getBackReferenceFileName(org.apache.hadoop.fs.Path)", "public static java.lang.String getBackReferenceFileName(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.io.FileLink.isBackReferencesDir(org.apache.hadoop.fs.Path)", "public static boolean isBackReferencesDir(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.io.FileLink.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.FileLink.hashCode()", "public int hashCode()"], ["org.apache.hadoop.hbase.io.HFileLink", "org.apache.hadoop.hbase.io.HFileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.io.HFileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.io.HFileLink", "org.apache.hadoop.hbase.io.HFileLink.buildFromHFileLinkPattern(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public static final org.apache.hadoop.hbase.io.HFileLink buildFromHFileLinkPattern(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.HFileLink", "org.apache.hadoop.hbase.io.HFileLink.buildFromHFileLinkPattern(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static final org.apache.hadoop.hbase.io.HFileLink buildFromHFileLinkPattern(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.HFileLink.createPath(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String)", "public static org.apache.hadoop.fs.Path createPath(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String)"], ["org.apache.hadoop.hbase.io.HFileLink", "org.apache.hadoop.hbase.io.HFileLink.build(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String)", "public static org.apache.hadoop.hbase.io.HFileLink build(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.HFileLink.getOriginPath()", "public org.apache.hadoop.fs.Path getOriginPath()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.HFileLink.getArchivePath()", "public org.apache.hadoop.fs.Path getArchivePath()"], ["boolean", "org.apache.hadoop.hbase.io.HFileLink.isHFileLink(org.apache.hadoop.fs.Path)", "public static boolean isHFileLink(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.io.HFileLink.isHFileLink(java.lang.String)", "public static boolean isHFileLink(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.HFileLink.getReferencedHFileName(java.lang.String)", "public static java.lang.String getReferencedHFileName(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.HFileLink.getReferencedRegionName(java.lang.String)", "public static java.lang.String getReferencedRegionName(java.lang.String)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.io.HFileLink.getReferencedTableName(java.lang.String)", "public static org.apache.hadoop.hbase.TableName getReferencedTableName(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.HFileLink.createHFileLinkName(org.apache.hadoop.hbase.HRegionInfo, java.lang.String)", "public static java.lang.String createHFileLinkName(org.apache.hadoop.hbase.HRegionInfo, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.HFileLink.createHFileLinkName(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String)", "public static java.lang.String createHFileLinkName(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.io.HFileLink.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, java.lang.String)", "public static boolean create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HFileLink.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String)", "public static boolean create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HFileLink.createFromHFileLink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String)", "public static boolean createFromHFileLink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.HFileLink.getHFileFromBackReference(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getHFileFromBackReference(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.HFileLink.getHFileFromBackReference(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getHFileFromBackReference(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getKey()", "public java.nio.ByteBuffer getKey()"], ["java.lang.String", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getKeyString()", "public java.lang.String getKeyString()"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getValue()", "public java.nio.ByteBuffer getValue()"], ["java.lang.String", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getValueString()", "public java.lang.String getValueString()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getKeyValue()", "public org.apache.hadoop.hbase.Cell getKeyValue()"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.next()", "public boolean next() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(byte[])", "public boolean seekBefore(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(byte[], int, int)", "public boolean seekBefore(byte[], int, int) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo()", "public boolean seekTo() throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo(byte[])", "public int seekTo(byte[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo(byte[], int, int)", "public int seekTo(byte[], int, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.reseekTo(byte[])", "public int reseekTo(byte[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.reseekTo(byte[], int, int)", "public int reseekTo(byte[], int, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFile$Reader", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getReader()", "public org.apache.hadoop.hbase.io.hfile.HFile$Reader getReader()"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.isSeeked()", "public boolean isSeeked()"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo(org.apache.hadoop.hbase.Cell)", "public int seekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.reseekTo(org.apache.hadoop.hbase.Cell)", "public int reseekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(org.apache.hadoop.hbase.Cell)", "public boolean seekBefore(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.HalfStoreFileReader$1.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.io.HalfStoreFileReader", "org.apache.hadoop.hbase.io.HalfStoreFileReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.HalfStoreFileReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.HalfStoreFileReader", "org.apache.hadoop.hbase.io.HalfStoreFileReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.HalfStoreFileReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.io.HalfStoreFileReader.getScanner(boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.HalfStoreFileReader.passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan)", "public boolean passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan)"], ["byte[]", "org.apache.hadoop.hbase.io.HalfStoreFileReader.getLastKey()", "public byte[] getLastKey()"], ["byte[]", "org.apache.hadoop.hbase.io.HalfStoreFileReader.midkey()", "public byte[] midkey() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.io.HalfStoreFileReader.getFirstKey()", "public byte[] getFirstKey()"], ["long", "org.apache.hadoop.hbase.io.HalfStoreFileReader.getEntries()", "public long getEntries()"], ["long", "org.apache.hadoop.hbase.io.HalfStoreFileReader.getFilterEntries()", "public long getFilterEntries()"], ["org.apache.hadoop.hbase.io.Reference$Range[]", "org.apache.hadoop.hbase.io.Reference$Range.values()", "public static org.apache.hadoop.hbase.io.Reference$Range[] values()"], ["org.apache.hadoop.hbase.io.Reference$Range", "org.apache.hadoop.hbase.io.Reference$Range.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.io.Reference$Range valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.io.Reference.createTopReference(byte[])", "public static org.apache.hadoop.hbase.io.Reference createTopReference(byte[])"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.io.Reference.createBottomReference(byte[])", "public static org.apache.hadoop.hbase.io.Reference createBottomReference(byte[])"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.io.Reference()", "public org.apache.hadoop.hbase.io.Reference()"], ["org.apache.hadoop.hbase.io.Reference$Range", "org.apache.hadoop.hbase.io.Reference.getFileRegion()", "public org.apache.hadoop.hbase.io.Reference$Range getFileRegion()"], ["byte[]", "org.apache.hadoop.hbase.io.Reference.getSplitKey()", "public byte[] getSplitKey()"], ["java.lang.String", "org.apache.hadoop.hbase.io.Reference.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.io.Reference.isTopFileRegion(org.apache.hadoop.hbase.io.Reference$Range)", "public static boolean isTopFileRegion(org.apache.hadoop.hbase.io.Reference$Range)"], ["void", "org.apache.hadoop.hbase.io.Reference.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.Reference.write(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.fs.Path write(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.io.Reference.read(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.io.Reference read(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.FSProtos$Reference", "org.apache.hadoop.hbase.io.Reference.convert()", "public org.apache.hadoop.hbase.protobuf.generated.FSProtos$Reference convert()"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.io.Reference.convert(org.apache.hadoop.hbase.protobuf.generated.FSProtos$Reference)", "public static org.apache.hadoop.hbase.io.Reference convert(org.apache.hadoop.hbase.protobuf.generated.FSProtos$Reference)"], ["int", "org.apache.hadoop.hbase.io.Reference.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.io.Reference.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["org.apache.hadoop.hbase.io.WALLink", "org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.WALLink", "org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)"], ["org.apache.hadoop.hbase.io.WALLink", "org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$BlockIndexNotLoadedException", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$BlockIndexNotLoadedException()", "public org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$BlockIndexNotLoadedException()"], ["org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$NotSeekedException", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$NotSeekedException()", "public org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$NotSeekedException()"], ["org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner(org.apache.hadoop.hbase.io.hfile.HFile$Reader, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner(org.apache.hadoop.hbase.io.hfile.HFile$Reader, boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.isSeeked()", "public boolean isSeeked()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.seekTo(byte[])", "public int seekTo(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.seekBefore(byte[])", "public boolean seekBefore(byte[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.reseekTo(byte[])", "public int reseekTo(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFile$Reader", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.getReader()", "public org.apache.hadoop.hbase.io.hfile.HFile$Reader getReader()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.length()", "public long length()"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getScanner(boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getFirstKey()", "public byte[] getFirstKey()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getFirstRowKey()", "public byte[] getFirstRowKey()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getLastRowKey()", "public byte[] getLastRowKey()"], ["long", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getEntries()", "public long getEntries()"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["org.apache.hadoop.hbase.io.compress.Compression$Algorithm", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getCompressionAlgorithm()", "public org.apache.hadoop.hbase.io.compress.Compression$Algorithm getCompressionAlgorithm()"], ["long", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.indexSize()", "public long indexSize()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getName()", "public java.lang.String getName()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getDataBlockIndexReader()", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader getDataBlockIndexReader()"], ["org.apache.hadoop.hbase.io.hfile.FixedFileTrailer", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getTrailer()", "public org.apache.hadoop.hbase.io.hfile.FixedFileTrailer getTrailer()"], ["org.apache.hadoop.hbase.io.hfile.HFile$FileInfo", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.loadFileInfo()", "public org.apache.hadoop.hbase.io.hfile.HFile$FileInfo loadFileInfo() throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getDataBlockEncoding()", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["java.util.Map", "org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.loadFileInfo()", "public java.util.Map loadFileInfo() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter", "org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["void", "org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.appendFileInfo(byte[], byte[])", "public void appendFileInfo(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.compress.Compression$Algorithm", "org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.compressionByName(java.lang.String)", "public static org.apache.hadoop.hbase.io.compress.Compression$Algorithm compressionByName(java.lang.String)"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.get75thPercentile()", "public double get75thPercentile()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.get95thPercentile()", "public double get95thPercentile()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.get98thPercentile()", "public double get98thPercentile()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.get999thPercentile()", "public double get999thPercentile()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.get99thPercentile()", "public double get99thPercentile()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.getMean()", "public double getMean()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.getMax()", "public double getMax()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.getMin()", "public double getMin()"], ["double", "org.apache.hadoop.hbase.io.hfile.AgeSnapshot.getStdDev()", "public double getStdDev()"], ["org.apache.hadoop.hbase.io.hfile.BlockCacheKey", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey(java.lang.String, long)", "public org.apache.hadoop.hbase.io.hfile.BlockCacheKey(java.lang.String, long)"], ["int", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.heapSize()", "public long heapSize()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.getHfileName()", "public java.lang.String getHfileName()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheKey.getOffset()", "public long getOffset()"], ["int", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile.getCount()", "public int getCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile.getSize()", "public long getSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile.getCountData()", "public int getCountData()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile.getSizeData()", "public long getSizeData()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile.getFilename()", "public java.lang.String getFilename()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.update(org.apache.hadoop.hbase.io.hfile.CachedBlock)", "public boolean update(org.apache.hadoop.hbase.io.hfile.CachedBlock)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.isFull()", "public boolean isFull()"], ["java.util.NavigableMap<java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>>", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getCachedBlockStatsByFile()", "public java.util.NavigableMap<java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>> getCachedBlockStatsByFile()"], ["int", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getCount()", "public int getCount()"], ["int", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getDataCount()", "public int getDataCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getSize()", "public long getSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getDataSize()", "public long getDataSize()"], ["org.apache.hadoop.hbase.io.hfile.AgeSnapshot", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.getAgeInCacheSnapshot()", "public org.apache.hadoop.hbase.io.hfile.AgeSnapshot getAgeInCacheSnapshot()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.BlockCacheUtil", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil()", "public org.apache.hadoop.hbase.io.hfile.BlockCacheUtil()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.toString(org.apache.hadoop.hbase.io.hfile.CachedBlock, long)", "public static java.lang.String toString(org.apache.hadoop.hbase.io.hfile.CachedBlock, long)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.toJSON(java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>)", "public static java.lang.String toJSON(java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>) throws org.codehaus.jackson.JsonGenerationException, org.codehaus.jackson.map.JsonMappingException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.toJSON(org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile)", "public static java.lang.String toJSON(org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile) throws org.codehaus.jackson.JsonGenerationException, org.codehaus.jackson.map.JsonMappingException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.toJSON(org.apache.hadoop.hbase.io.hfile.BlockCache)", "public static java.lang.String toJSON(org.apache.hadoop.hbase.io.hfile.BlockCache) throws org.codehaus.jackson.JsonGenerationException, org.codehaus.jackson.map.JsonMappingException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.toStringMinusFileName(org.apache.hadoop.hbase.io.hfile.CachedBlock, long)", "public static java.lang.String toStringMinusFileName(org.apache.hadoop.hbase.io.hfile.CachedBlock, long)"], ["org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile", "org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.getLoadedCachedBlocksByFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache)", "public static org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile getLoadedCachedBlocksByFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.BlockCachesIterator.hasNext()", "public boolean hasNext()"], ["org.apache.hadoop.hbase.io.hfile.CachedBlock", "org.apache.hadoop.hbase.io.hfile.BlockCachesIterator.next()", "public org.apache.hadoop.hbase.io.hfile.CachedBlock next()"], ["void", "org.apache.hadoop.hbase.io.hfile.BlockCachesIterator.remove()", "public void remove()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.BlockCachesIterator.next()", "public java.lang.Object next()"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority[]", "org.apache.hadoop.hbase.io.hfile.BlockPriority.values()", "public static org.apache.hadoop.hbase.io.hfile.BlockPriority[] values()"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority", "org.apache.hadoop.hbase.io.hfile.BlockPriority.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.io.hfile.BlockPriority valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo", "org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo(org.apache.hadoop.hbase.io.hfile.HFileBlock, org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo(org.apache.hadoop.hbase.io.hfile.HFileBlock, org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo.getHFileBlock()", "public org.apache.hadoop.hbase.io.hfile.HFileBlock getHFileBlock()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches[]", "org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches.values()", "public static org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches[] values()"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches", "org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.io.hfile.CacheConfig$ExternalBlockCaches valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HColumnDescriptor)", "public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HColumnDescriptor)"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.isBlockCacheEnabled()", "public boolean isBlockCacheEnabled()"], ["org.apache.hadoop.hbase.io.hfile.BlockCache", "org.apache.hadoop.hbase.io.hfile.CacheConfig.getBlockCache()", "public org.apache.hadoop.hbase.io.hfile.BlockCache getBlockCache()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheDataOnRead()", "public boolean shouldCacheDataOnRead()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheBlockOnRead(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory)", "public boolean shouldCacheBlockOnRead(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.isInMemory()", "public boolean isInMemory()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.isCacheDataInL1()", "public boolean isCacheDataInL1()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheDataOnWrite()", "public boolean shouldCacheDataOnWrite()"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheConfig.setCacheDataOnWrite(boolean)", "public void setCacheDataOnWrite(boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheConfig.setCacheDataInL1(boolean)", "public void setCacheDataInL1(boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheIndexesOnWrite()", "public boolean shouldCacheIndexesOnWrite()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheBloomsOnWrite()", "public boolean shouldCacheBloomsOnWrite()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldEvictOnClose()", "public boolean shouldEvictOnClose()"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheConfig.setEvictOnClose(boolean)", "public void setEvictOnClose(boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheDataCompressed()", "public boolean shouldCacheDataCompressed()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldCacheCompressed(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory)", "public boolean shouldCacheCompressed(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CacheConfig.shouldPrefetchOnOpen()", "public boolean shouldPrefetchOnOpen()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.CacheConfig.toString()", "public java.lang.String toString()"], ["synchronized", "org.apache.hadoop.hbase.io.hfile.CacheConfig.org.apache.hadoop.hbase.io.hfile.BlockCache instantiateBlockCache(org.apache.hadoop.conf.Configuration)", "public static synchronized org.apache.hadoop.hbase.io.hfile.BlockCache instantiateBlockCache(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String)", "public org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String, int)", "public org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String, int)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.CacheStats.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheStats.miss(boolean)", "public void miss(boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheStats.hit(boolean)", "public void hit(boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheStats.evict()", "public void evict()"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheStats.evicted(long)", "public void evicted(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getRequestCount()", "public long getRequestCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getRequestCachingCount()", "public long getRequestCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getMissCount()", "public long getMissCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getMissCachingCount()", "public long getMissCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitCount()", "public long getHitCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitCachingCount()", "public long getHitCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getEvictionCount()", "public long getEvictionCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getEvictedCount()", "public long getEvictedCount()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitRatio()", "public double getHitRatio()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitCachingRatio()", "public double getHitCachingRatio()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getMissRatio()", "public double getMissRatio()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getMissCachingRatio()", "public double getMissCachingRatio()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.evictedPerEviction()", "public double evictedPerEviction()"], ["void", "org.apache.hadoop.hbase.io.hfile.CacheStats.rollMetricsPeriod()", "public void rollMetricsPeriod()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getSumHitCountsPastNPeriods()", "public long getSumHitCountsPastNPeriods()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getSumRequestCountsPastNPeriods()", "public long getSumRequestCountsPastNPeriods()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getSumHitCachingCountsPastNPeriods()", "public long getSumHitCachingCountsPastNPeriods()"], ["long", "org.apache.hadoop.hbase.io.hfile.CacheStats.getSumRequestCachingCountsPastNPeriods()", "public long getSumRequestCachingCountsPastNPeriods()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitRatioPastNPeriods()", "public double getHitRatioPastNPeriods()"], ["double", "org.apache.hadoop.hbase.io.hfile.CacheStats.getHitCachingRatioPastNPeriods()", "public double getHitCachingRatioPastNPeriods()"], ["org.apache.hadoop.hbase.io.hfile.AgeSnapshot", "org.apache.hadoop.hbase.io.hfile.CacheStats.getAgeAtEvictionSnapshot()", "public org.apache.hadoop.hbase.io.hfile.AgeSnapshot getAgeAtEvictionSnapshot()"], ["org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager", "org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager()", "public org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager()"], ["int", "org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager.registerDeserializer(org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable>)", "public static int registerDeserializer(org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable>)"], ["org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable>", "org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager.getDeserializer(int)", "public static org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> getDeserializer(int)"], ["org.apache.hadoop.hbase.io.hfile.ChecksumUtil", "org.apache.hadoop.hbase.io.hfile.ChecksumUtil()", "public org.apache.hadoop.hbase.io.hfile.ChecksumUtil()"], ["void", "org.apache.hadoop.hbase.io.hfile.ChecksumUtil.generateExceptionForChecksumFailureForTest(boolean)", "public static void generateExceptionForChecksumFailureForTest(boolean)"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getRequestCount()", "public long getRequestCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getRequestCachingCount()", "public long getRequestCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getMissCount()", "public long getMissCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getMissCachingCount()", "public long getMissCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getHitCount()", "public long getHitCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getHitCachingCount()", "public long getHitCachingCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getEvictionCount()", "public long getEvictionCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getEvictedCount()", "public long getEvictedCount()"], ["double", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getHitRatioPastNPeriods()", "public double getHitRatioPastNPeriods()"], ["double", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats.getHitCachingRatioPastNPeriods()", "public double getHitCachingRatioPastNPeriods()"], ["org.apache.hadoop.hbase.io.hfile.CombinedBlockCache", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache(org.apache.hadoop.hbase.io.hfile.LruBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache)", "public org.apache.hadoop.hbase.io.hfile.CombinedBlockCache(org.apache.hadoop.hbase.io.hfile.LruBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache)"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.heapSize()", "public long heapSize()"], ["void", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["int", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.evictBlocksByHfileName(java.lang.String)", "public int evictBlocksByHfileName(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getStats()", "public org.apache.hadoop.hbase.io.hfile.CacheStats getStats()"], ["void", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.shutdown()", "public void shutdown()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.size()", "public long size()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getFreeSize()", "public long getFreeSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getCurrentSize()", "public long getCurrentSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getBlockCount()", "public long getBlockCount()"], ["java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock>", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator()"], ["org.apache.hadoop.hbase.io.hfile.BlockCache[]", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.getBlockCaches()", "public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches()"], ["void", "org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.setMaxSize(long)", "public void setMaxSize(long)"], ["org.apache.hadoop.hbase.io.hfile.CorruptHFileException", "org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String, java.lang.Throwable)", "public org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.io.hfile.CorruptHFileException", "org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String)", "public org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String)"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getTrailerSize()", "public int getTrailerSize()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.FixedFileTrailer", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.readFromStream(org.apache.hadoop.fs.FSDataInputStream, long)", "public static org.apache.hadoop.hbase.io.hfile.FixedFileTrailer readFromStream(org.apache.hadoop.fs.FSDataInputStream, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.expectMajorVersion(int)", "public void expectMajorVersion(int)"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.expectMinorVersion(int)", "public void expectMinorVersion(int)"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.expectAtLeastMajorVersion(int)", "public void expectAtLeastMajorVersion(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getFileInfoOffset()", "public long getFileInfoOffset()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setFileInfoOffset(long)", "public void setFileInfoOffset(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getLoadOnOpenDataOffset()", "public long getLoadOnOpenDataOffset()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setLoadOnOpenOffset(long)", "public void setLoadOnOpenOffset(long)"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getDataIndexCount()", "public int getDataIndexCount()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setDataIndexCount(int)", "public void setDataIndexCount(int)"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getMetaIndexCount()", "public int getMetaIndexCount()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setMetaIndexCount(int)", "public void setMetaIndexCount(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getTotalUncompressedBytes()", "public long getTotalUncompressedBytes()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setTotalUncompressedBytes(long)", "public void setTotalUncompressedBytes(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getEntryCount()", "public long getEntryCount()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setEntryCount(long)", "public void setEntryCount(long)"], ["org.apache.hadoop.hbase.io.compress.Compression$Algorithm", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getCompressionCodec()", "public org.apache.hadoop.hbase.io.compress.Compression$Algorithm getCompressionCodec()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setCompressionCodec(org.apache.hadoop.hbase.io.compress.Compression$Algorithm)", "public void setCompressionCodec(org.apache.hadoop.hbase.io.compress.Compression$Algorithm)"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getNumDataIndexLevels()", "public int getNumDataIndexLevels()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setNumDataIndexLevels(int)", "public void setNumDataIndexLevels(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getLastDataBlockOffset()", "public long getLastDataBlockOffset()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setLastDataBlockOffset(long)", "public void setLastDataBlockOffset(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getFirstDataBlockOffset()", "public long getFirstDataBlockOffset()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setFirstDataBlockOffset(long)", "public void setFirstDataBlockOffset(long)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getComparatorClassName()", "public java.lang.String getComparatorClassName()"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getMajorVersion()", "public int getMajorVersion()"], ["int", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getMinorVersion()", "public int getMinorVersion()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setComparatorClass(java.lang.Class<? extends org.apache.hadoop.hbase.KeyValue$KVComparator>)", "public void setComparatorClass(java.lang.Class<? extends org.apache.hadoop.hbase.KeyValue$KVComparator>)"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.createComparator(java.lang.String)", "public static org.apache.hadoop.hbase.KeyValue$KVComparator createComparator(java.lang.String) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getUncompressedDataIndexSize()", "public long getUncompressedDataIndexSize()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setUncompressedDataIndexSize(long)", "public void setUncompressedDataIndexSize(long)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.getEncryptionKey()", "public byte[] getEncryptionKey()"], ["void", "org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.setEncryptionKey(byte[])", "public void setEncryptionKey(byte[])"], ["org.apache.hadoop.hbase.io.hfile.HFile$FileInfo", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo()", "public org.apache.hadoop.hbase.io.hfile.HFile$FileInfo()"], ["org.apache.hadoop.hbase.io.hfile.HFile$FileInfo", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.append(byte[], byte[], boolean)", "public org.apache.hadoop.hbase.io.hfile.HFile$FileInfo append(byte[], byte[], boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.clear()", "public void clear()"], ["java.util.Comparator<? super byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.comparator()", "public java.util.Comparator<? super byte[]> comparator()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.containsKey(java.lang.Object)", "public boolean containsKey(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.containsValue(java.lang.Object)", "public boolean containsValue(java.lang.Object)"], ["java.util.Set<java.util.Map$Entry<byte[], byte[]>>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.entrySet()", "public java.util.Set<java.util.Map$Entry<byte[], byte[]>> entrySet()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.firstKey()", "public byte[] firstKey()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.get(java.lang.Object)", "public byte[] get(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.hashCode()", "public int hashCode()"], ["java.util.SortedMap<byte[], byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.headMap(byte[])", "public java.util.SortedMap<byte[], byte[]> headMap(byte[])"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.isEmpty()", "public boolean isEmpty()"], ["java.util.Set<byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.keySet()", "public java.util.Set<byte[]> keySet()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.lastKey()", "public byte[] lastKey()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.put(byte[], byte[])", "public byte[] put(byte[], byte[])"], ["void", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.putAll(java.util.Map<? extends byte[], ? extends byte[]>)", "public void putAll(java.util.Map<? extends byte[], ? extends byte[]>)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.remove(java.lang.Object)", "public byte[] remove(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.size()", "public int size()"], ["java.util.SortedMap<byte[], byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.subMap(byte[], byte[])", "public java.util.SortedMap<byte[], byte[]> subMap(byte[], byte[])"], ["java.util.SortedMap<byte[], byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.tailMap(byte[])", "public java.util.SortedMap<byte[], byte[]> tailMap(byte[])"], ["java.util.Collection<byte[]>", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.values()", "public java.util.Collection<byte[]> values()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.lastKey()", "public java.lang.Object lastKey()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.firstKey()", "public java.lang.Object firstKey()"], ["java.util.SortedMap", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.tailMap(java.lang.Object)", "public java.util.SortedMap tailMap(java.lang.Object)"], ["java.util.SortedMap", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.headMap(java.lang.Object)", "public java.util.SortedMap headMap(java.lang.Object)"], ["java.util.SortedMap", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.subMap(java.lang.Object, java.lang.Object)", "public java.util.SortedMap subMap(java.lang.Object, java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.remove(java.lang.Object)", "public java.lang.Object remove(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.put(java.lang.Object, java.lang.Object)", "public java.lang.Object put(java.lang.Object, java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFile$FileInfo.get(java.lang.Object)", "public java.lang.Object get(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.withPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.withOutputStream(org.apache.hadoop.fs.FSDataOutputStream)", "public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withOutputStream(org.apache.hadoop.fs.FSDataOutputStream)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.withComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.withFavoredNodes(java.net.InetSocketAddress[])", "public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withFavoredNodes(java.net.InetSocketAddress[])"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["org.apache.hadoop.hbase.io.hfile.HFile$Writer", "org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create()", "public org.apache.hadoop.hbase.io.hfile.HFile$Writer create() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFile", "org.apache.hadoop.hbase.io.hfile.HFile()", "public org.apache.hadoop.hbase.io.hfile.HFile()"], ["long", "org.apache.hadoop.hbase.io.hfile.HFile.getChecksumFailuresCount()", "public static final long getChecksumFailuresCount()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFile.getFormatVersion(org.apache.hadoop.conf.Configuration)", "public static int getFormatVersion(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile.getWriterFactoryNoCache(org.apache.hadoop.conf.Configuration)", "public static final org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory getWriterFactoryNoCache(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory", "org.apache.hadoop.hbase.io.hfile.HFile.getWriterFactory(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public static final org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory getWriterFactory(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig)"], ["org.apache.hadoop.hbase.io.hfile.HFile$Reader", "org.apache.hadoop.hbase.io.hfile.HFile.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.io.hfile.HFile$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFile$Reader", "org.apache.hadoop.hbase.io.hfile.HFile.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.io.hfile.HFile$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile.isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile.isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus)", "public static boolean isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFile.isReservedFileInfoKey(byte[])", "public static boolean isReservedFileInfoKey(byte[])"], ["java.lang.String[]", "org.apache.hadoop.hbase.io.hfile.HFile.getSupportedCompressionAlgorithms()", "public static java.lang.String[] getSupportedCompressionAlgorithms()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFile.checkFormatVersion(int)", "public static void checkFormatVersion(int) throws java.lang.IllegalArgumentException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFile.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(java.nio.ByteBuffer, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock deserialize(java.nio.ByteBuffer, boolean) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock$1.getDeserialiserIdentifier()", "public int getDeserialiserIdentifier()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(java.nio.ByteBuffer)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock deserialize(java.nio.ByteBuffer) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(java.nio.ByteBuffer, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable deserialize(java.nio.ByteBuffer, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(java.nio.ByteBuffer)", "public org.apache.hadoop.hbase.io.hfile.Cacheable deserialize(java.nio.ByteBuffer) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader$1.nextBlock()", "public org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlock() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader$1.nextBlockWithBlockType(org.apache.hadoop.hbase.io.hfile.BlockType)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlockWithBlockType(org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader", "org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader(long, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader(long, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator", "org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.blockRange(long, long)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator blockRange(long, long)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$PrefetchedHeader", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl$1.initialValue()", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$PrefetchedHeader initialValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl$1.initialValue()", "public java.lang.Object initialValue()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl(org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl(org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(long, long, int, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock readBlockData(long, long, int, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.getBlockDecodingContext()", "public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getBlockDecodingContext()"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.getDefaultBlockDecodingContext()", "public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getDefaultBlockDecodingContext()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.closeStreams()", "public void closeStreams() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$BufferGrabbingByteArrayOutputStream", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$BufferGrabbingByteArrayOutputStream()", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$BufferGrabbingByteArrayOutputStream()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$BufferGrabbingByteArrayOutputStream.write(byte[], int, int)", "public void write(byte[], int, int)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$BufferGrabbingByteArrayOutputStream.getBuffer()", "public byte[] getBuffer()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State[]", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State.values()", "public static org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State[] values()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["java.io.DataOutputStream", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.startWriting(org.apache.hadoop.hbase.io.hfile.BlockType)", "public java.io.DataOutputStream startWriting(org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.write(org.apache.hadoop.hbase.Cell)", "public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.writeHeaderAndData(org.apache.hadoop.fs.FSDataOutputStream)", "public void writeHeaderAndData(org.apache.hadoop.fs.FSDataOutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.release()", "public void release()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.isWriting()", "public boolean isWriting()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.blockSizeWritten()", "public int blockSizeWritten()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.writeBlock(org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockWritable, org.apache.hadoop.fs.FSDataOutputStream)", "public void writeBlock(org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockWritable, org.apache.hadoop.fs.FSDataOutputStream) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.getBlockForCaching(org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock getBlockForCaching(org.apache.hadoop.hbase.io.hfile.CacheConfig)"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType()"], ["short", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getDataBlockEncodingId()", "public short getDataBlockEncodingId()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getOnDiskSizeWithHeader()", "public int getOnDiskSizeWithHeader()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getOnDiskSizeWithoutHeader()", "public int getOnDiskSizeWithoutHeader()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getUncompressedSizeWithoutHeader()", "public int getUncompressedSizeWithoutHeader()"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getPrevBlockOffset()", "public long getPrevBlockOffset()"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getBufferWithoutHeader()", "public java.nio.ByteBuffer getBufferWithoutHeader()"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getBufferReadOnly()", "public java.nio.ByteBuffer getBufferReadOnly()"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getBufferReadOnlyWithHeader()", "public java.nio.ByteBuffer getBufferReadOnlyWithHeader()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileBlock.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlock.isUnpacked()", "public boolean isUnpacked()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock.assumeUncompressed()", "public void assumeUncompressed() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock.expectType(org.apache.hadoop.hbase.io.hfile.BlockType)", "public void expectType(org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getOffset()", "public long getOffset()"], ["java.io.DataInputStream", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getByteStream()", "public java.io.DataInputStream getByteStream()"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlock.heapSize()", "public long heapSize()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlock.readWithExtra(java.io.InputStream, byte[], int, int, int)", "public static boolean readWithExtra(java.io.InputStream, byte[], int, int, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getNextBlockOnDiskSizeWithHeader()", "public int getNextBlockOnDiskSizeWithHeader()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getSerializedLength()", "public int getSerializedLength()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock.serialize(java.nio.ByteBuffer)", "public void serialize(java.nio.ByteBuffer)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlock.serializeExtraInfo(java.nio.ByteBuffer)", "public void serializeExtraInfo(java.nio.ByteBuffer)"], ["org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable>", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getDeserializer()", "public org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> getDeserializer()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlock.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getDataBlockEncoding()", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.headerSize()", "public int headerSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlock.headerSize(boolean)", "public static int headerSize(boolean)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getDummyHeaderForVersion()", "public byte[] getDummyHeaderForVersion()"], ["org.apache.hadoop.hbase.io.hfile.HFileContext", "org.apache.hadoop.hbase.io.hfile.HFileBlock.getHFileContext()", "public org.apache.hadoop.hbase.io.hfile.HFileContext getHFileContext()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.add(byte[], long, int)", "public void add(byte[], long, int)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.clear()", "public void clear()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getEntryBySubEntry(long)", "public int getEntryBySubEntry(long)"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getMidKeyMetadata()", "public byte[] getMidKeyMetadata() throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getNumEntries()", "public int getNumEntries()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getBlockKey(int)", "public byte[] getBlockKey(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getBlockOffset(int)", "public long getBlockOffset(int)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getOnDiskDataSize(int)", "public int getOnDiskDataSize(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk.getCumulativeNumKV(int)", "public long getCumulativeNumKV(int)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader(org.apache.hadoop.hbase.KeyValue$KVComparator, int, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader)", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader(org.apache.hadoop.hbase.KeyValue$KVComparator, int, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader(org.apache.hadoop.hbase.KeyValue$KVComparator, int)", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader(org.apache.hadoop.hbase.KeyValue$KVComparator, int)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.isEmpty()", "public boolean isEmpty()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.ensureNonEmpty()", "public void ensureNonEmpty()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.seekToDataBlock(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock seekToDataBlock(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding)", "public org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo loadDataBlockWithScanInfo(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.midkey()", "public byte[] midkey() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.getRootBlockKey(int)", "public byte[] getRootBlockKey(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.getRootBlockOffset(int)", "public long getRootBlockOffset(int)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.getRootBlockDataSize(int)", "public int getRootBlockDataSize(int)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.getRootBlockCount()", "public int getRootBlockCount()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.rootBlockContainingKey(byte[], int, int)", "public int rootBlockContainingKey(byte[], int, int)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.rootBlockContainingKey(org.apache.hadoop.hbase.Cell)", "public int rootBlockContainingKey(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.readRootIndex(java.io.DataInput, int)", "public void readRootIndex(java.io.DataInput, int) throws java.io.IOException"], ["java.io.DataInputStream", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.readRootIndex(org.apache.hadoop.hbase.io.hfile.HFileBlock, int)", "public java.io.DataInputStream readRootIndex(org.apache.hadoop.hbase.io.hfile.HFileBlock, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.readMultiLevelIndexRoot(org.apache.hadoop.hbase.io.hfile.HFileBlock, int)", "public void readMultiLevelIndexRoot(org.apache.hadoop.hbase.io.hfile.HFileBlock, int) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.heapSize()", "public long heapSize()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter()", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter(org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, java.lang.String)", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter(org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, java.lang.String)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.setMaxChunkSize(int)", "public void setMaxChunkSize(int)"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeIndexBlocks(org.apache.hadoop.fs.FSDataOutputStream)", "public long writeIndexBlocks(org.apache.hadoop.fs.FSDataOutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeSingleLevelIndex(java.io.DataOutput, java.lang.String)", "public void writeSingleLevelIndex(java.io.DataOutput, java.lang.String) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.getNumRootEntries()", "public final int getNumRootEntries()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.getNumLevels()", "public int getNumLevels()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.shouldWriteBlock(boolean)", "public boolean shouldWriteBlock(boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeInlineBlock(java.io.DataOutput)", "public void writeInlineBlock(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.blockWritten(long, int, int)", "public void blockWritten(long, int, int)"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.getInlineBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getInlineBlockType()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.addEntry(byte[], long, int)", "public void addEntry(byte[], long, int)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.ensureSingleLevel()", "public void ensureSingleLevel() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.getCacheOnWrite()", "public boolean getCacheOnWrite()"], ["long", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.getTotalUncompressedSize()", "public long getTotalUncompressedSize()"], ["org.apache.hadoop.hbase.io.hfile.HFileBlockIndex", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex()", "public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.getMaxChunkSize(org.apache.hadoop.conf.Configuration)", "public static int getMaxChunkSize(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding)", "public org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding)"], ["org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.createFromFileInfo(org.apache.hadoop.hbase.io.hfile.HFile$FileInfo)", "public static org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder createFromFileInfo(org.apache.hadoop.hbase.io.hfile.HFile$FileInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer)", "public void saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.getDataBlockEncoding()", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.useEncodedScanner(boolean)", "public boolean useEncodedScanner(boolean)"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.getEffectiveEncodingInCache(boolean)", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream)", "public int encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.useEncodedScanner()", "public boolean useEncodedScanner()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.newDataBlockEncodingContext(byte[], org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext newDataBlockEncodingContext(byte[], org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.newDataBlockDecodingContext(org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext newDataBlockDecodingContext(org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream)", "public void startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType)", "public void endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$KeyValueStatsCollector.collect(org.apache.hadoop.hbase.Cell)", "public void collect(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$KeyValueStatsCollector.finish()", "public void finish()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$KeyValueStatsCollector.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter(com.yammer.metrics.core.MetricsRegistry, java.io.PrintStream)", "public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter(com.yammer.metrics.core.MetricsRegistry, java.io.PrintStream)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter.processHistogram(com.yammer.metrics.core.MetricName, com.yammer.metrics.core.Histogram, java.io.PrintStream)", "public void processHistogram(com.yammer.metrics.core.MetricName, com.yammer.metrics.core.Histogram, java.io.PrintStream)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter.processHistogram(com.yammer.metrics.core.MetricName, com.yammer.metrics.core.Histogram, java.lang.Object)", "public void processHistogram(com.yammer.metrics.core.MetricName, com.yammer.metrics.core.Histogram, java.lang.Object) throws java.lang.Exception"], ["org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter()", "public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter()"], ["org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.parseOptions(java.lang.String[])", "public boolean parseOptions(java.lang.String[]) throws org.apache.commons.cli.ParseException, java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.run(java.lang.String[])", "public int run(java.lang.String[])"], ["void", "org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$1.run()", "public void run()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(byte[], int, int)", "public int seekTo(byte[], int, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(byte[], int, int)", "public int reseekTo(byte[], int, int) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(org.apache.hadoop.hbase.Cell)", "public int seekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(org.apache.hadoop.hbase.Cell)", "public int reseekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(org.apache.hadoop.hbase.Cell, boolean)", "public int seekTo(org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekBefore(byte[], int, int)", "public boolean seekBefore(byte[], int, int) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekBefore(org.apache.hadoop.hbase.Cell)", "public boolean seekBefore(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.getEffectiveDataBlockEncoding()", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveDataBlockEncoding()"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.isSeeked()", "public boolean isSeeked()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.seekTo()", "public boolean seekTo() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.next()", "public boolean next() throws java.io.IOException"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getKey()", "public java.nio.ByteBuffer getKey()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, byte[], int, int)", "public int compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, byte[], int, int)"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getValue()", "public java.nio.ByteBuffer getValue()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getKeyValue()", "public org.apache.hadoop.hbase.Cell getKeyValue()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getKeyString()", "public java.lang.String getKeyString()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getValueString()", "public java.lang.String getValueString()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell)", "public int compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2(org.apache.hadoop.hbase.io.hfile.HFileReaderV2, boolean, boolean, boolean)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.getKeyValue()", "public org.apache.hadoop.hbase.Cell getKeyValue()"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.getKey()", "public java.nio.ByteBuffer getKey()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, byte[], int, int)", "public int compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, byte[], int, int)"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.getValue()", "public java.nio.ByteBuffer getValue()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next()", "public boolean next() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo()", "public boolean seekTo() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.getKeyString()", "public java.lang.String getKeyString()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.getValueString()", "public java.lang.String getValueString()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell)", "public int compareKey(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV2", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.FixedFileTrailer, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV2(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.FixedFileTrailer, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getScanner(boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean)"], ["java.nio.ByteBuffer", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getMetaBlock(java.lang.String, boolean)", "public java.nio.ByteBuffer getMetaBlock(java.lang.String, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(long, long, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock readBlock(long, long, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.hasMVCCInfo()", "public boolean hasMVCCInfo()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getLastKey()", "public byte[] getLastKey()"], ["byte[]", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.midkey()", "public byte[] midkey() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.close(boolean)", "public void close(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getEffectiveEncodingInCache(boolean)", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean)"], ["java.io.DataInput", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getGeneralBloomFilterMetadata()", "public java.io.DataInput getGeneralBloomFilterMetadata() throws java.io.IOException"], ["java.io.DataInput", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getDeleteBloomFilterMetadata()", "public java.io.DataInput getDeleteBloomFilterMetadata() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.isFileInfoLoaded()", "public boolean isFileInfoLoaded()"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getMajorVersion()", "public int getMajorVersion()"], ["org.apache.hadoop.hbase.io.hfile.HFileContext", "org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getFileContext()", "public org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext()"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV3$EncodedScannerV3", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3$EncodedScannerV3(org.apache.hadoop.hbase.io.hfile.HFileReaderV3, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV3$EncodedScannerV3(org.apache.hadoop.hbase.io.hfile.HFileReaderV3, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV3$ScannerV3", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3$ScannerV3(org.apache.hadoop.hbase.io.hfile.HFileReaderV3, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV3$ScannerV3(org.apache.hadoop.hbase.io.hfile.HFileReaderV3, boolean, boolean, boolean)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3$ScannerV3.getKeyValue()", "public org.apache.hadoop.hbase.Cell getKeyValue()"], ["org.apache.hadoop.hbase.io.hfile.HFileReaderV3", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.FixedFileTrailer, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.HFileReaderV3(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.FixedFileTrailer, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.fs.HFileSystem, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3.getScanner(boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean)"], ["int", "org.apache.hadoop.hbase.io.hfile.HFileReaderV3.getMajorVersion()", "public int getMajorVersion()"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2$1.getBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType()"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2$1.writeToBlock(java.io.DataOutput)", "public void writeToBlock(java.io.DataOutput) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFile$Writer", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2$WriterFactoryV2.createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileWriterV2", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileWriterV2(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.appendMetaBlock(java.lang.String, org.apache.hadoop.io.Writable)", "public void appendMetaBlock(java.lang.String, org.apache.hadoop.io.Writable)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.append(org.apache.hadoop.hbase.Cell)", "public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.addInlineBlockWriter(org.apache.hadoop.hbase.io.hfile.InlineBlockWriter)", "public void addInlineBlockWriter(org.apache.hadoop.hbase.io.hfile.InlineBlockWriter)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.addGeneralBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter)", "public void addGeneralBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter)"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.addDeleteFamilyBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter)", "public void addDeleteFamilyBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter)"], ["org.apache.hadoop.hbase.io.hfile.HFileContext", "org.apache.hadoop.hbase.io.hfile.HFileWriterV2.getFileContext()", "public org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext()"], ["org.apache.hadoop.hbase.io.hfile.HFile$Writer", "org.apache.hadoop.hbase.io.hfile.HFileWriterV3$WriterFactoryV3.createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.HFileWriterV3", "org.apache.hadoop.hbase.io.hfile.HFileWriterV3(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.hfile.HFileWriterV3(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.io.hfile.HFileContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.HFileWriterV3.append(org.apache.hadoop.hbase.Cell)", "public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache", "org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache(org.apache.hadoop.hbase.io.hfile.LruBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache)", "public org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache(org.apache.hadoop.hbase.io.hfile.LruBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache.getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["org.apache.hadoop.hbase.io.hfile.InvalidHFileException", "org.apache.hadoop.hbase.io.hfile.InvalidHFileException()", "public org.apache.hadoop.hbase.io.hfile.InvalidHFileException()"], ["org.apache.hadoop.hbase.io.hfile.InvalidHFileException", "org.apache.hadoop.hbase.io.hfile.InvalidHFileException(java.lang.String)", "public org.apache.hadoop.hbase.io.hfile.InvalidHFileException(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getBlockPriority()", "public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority()"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getOffset()", "public long getOffset()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getSize()", "public long getSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getCachedTime()", "public long getCachedTime()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.getFilename()", "public java.lang.String getFilename()"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock)", "public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock)"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1.hasNext()", "public boolean hasNext()"], ["org.apache.hadoop.hbase.io.hfile.CachedBlock", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1.next()", "public org.apache.hadoop.hbase.io.hfile.CachedBlock next()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1.remove()", "public void remove()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$1.next()", "public java.lang.Object next()"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket(org.apache.hadoop.hbase.io.hfile.LruBlockCache, java.lang.String, long, long, long)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket(org.apache.hadoop.hbase.io.hfile.LruBlockCache, java.lang.String, long, long, long)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)", "public void add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.free(long)", "public long free(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.overflow()", "public long overflow()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.totalSize()", "public long totalSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.compareTo(org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket)", "public int compareTo(org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.hashCode()", "public int hashCode()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.evict()", "public void evict()"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread.run()", "public void run()"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache", "org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long)"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache", "org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean)"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache", "org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache", "org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.io.hfile.LruBlockCache", "org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, int, float, int, float, float, float, float, float, boolean)", "public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, int, float, int, float, float, float, float, float, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.setMaxSize(long)", "public void setMaxSize(long)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public boolean containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["int", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.evictBlocksByHfileName(java.lang.String)", "public int evictBlocksByHfileName(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getMaxSize()", "public long getMaxSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getCurrentSize()", "public long getCurrentSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getFreeSize()", "public long getFreeSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.size()", "public long size()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getBlockCount()", "public long getBlockCount()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.logStats()", "public void logStats()"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getStats()", "public org.apache.hadoop.hbase.io.hfile.CacheStats getStats()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.heapSize()", "public long heapSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.calculateOverhead(long, long, int)", "public static long calculateOverhead(long, long, int)"], ["java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock>", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.shutdown()", "public void shutdown()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.clearCache()", "public void clearCache()"], ["java.util.Map<org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, java.lang.Integer>", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getEncodingCountsForTest()", "public java.util.Map<org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, java.lang.Integer> getEncodingCountsForTest()"], ["void", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache)", "public void setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache)"], ["org.apache.hadoop.hbase.io.hfile.BlockCache[]", "org.apache.hadoop.hbase.io.hfile.LruBlockCache.getBlockCaches()", "public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches()"], ["org.apache.hadoop.hbase.io.hfile.LruCachedBlock", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long)", "public org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long)"], ["org.apache.hadoop.hbase.io.hfile.LruCachedBlock", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long, boolean)", "public org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.access(long)", "public void access(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.getCachedTime()", "public long getCachedTime()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.heapSize()", "public long heapSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.compareTo(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)", "public int compareTo(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)"], ["int", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.getBuffer()", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBuffer()"], ["org.apache.hadoop.hbase.io.hfile.BlockCacheKey", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.getCacheKey()", "public org.apache.hadoop.hbase.io.hfile.BlockCacheKey getCacheKey()"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.getPriority()", "public org.apache.hadoop.hbase.io.hfile.BlockPriority getPriority()"], ["int", "org.apache.hadoop.hbase.io.hfile.LruCachedBlock.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue", "org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue(long, long)", "public org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue(long, long)"], ["void", "org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue.add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)", "public void add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock)"], ["org.apache.hadoop.hbase.io.hfile.LruCachedBlock", "org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue.poll()", "public org.apache.hadoop.hbase.io.hfile.LruCachedBlock poll()"], ["org.apache.hadoop.hbase.io.hfile.LruCachedBlock", "org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue.pollLast()", "public org.apache.hadoop.hbase.io.hfile.LruCachedBlock pollLast()"], ["long", "org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue.heapSize()", "public long heapSize()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$1.hasNext()", "public boolean hasNext()"], ["org.apache.hadoop.hbase.io.hfile.CachedBlock", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$1.next()", "public org.apache.hadoop.hbase.io.hfile.CachedBlock next()"], ["void", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$1.remove()", "public void remove()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$1.next()", "public java.lang.Object next()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.asyncDecode(net.spy.memcached.CachedData)", "public boolean asyncDecode(net.spy.memcached.CachedData)"], ["net.spy.memcached.CachedData", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.encode(org.apache.hadoop.hbase.io.hfile.HFileBlock)", "public net.spy.memcached.CachedData encode(org.apache.hadoop.hbase.io.hfile.HFileBlock)"], ["org.apache.hadoop.hbase.io.hfile.HFileBlock", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.decode(net.spy.memcached.CachedData)", "public org.apache.hadoop.hbase.io.hfile.HFileBlock decode(net.spy.memcached.CachedData)"], ["int", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.getMaxSize()", "public int getMaxSize()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.decode(net.spy.memcached.CachedData)", "public java.lang.Object decode(net.spy.memcached.CachedData)"], ["net.spy.memcached.CachedData", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache$HFileBlockTranscoder.encode(java.lang.Object)", "public net.spy.memcached.CachedData encode(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["int", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.evictBlocksByHfileName(java.lang.String)", "public int evictBlocksByHfileName(java.lang.String)"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getStats()", "public org.apache.hadoop.hbase.io.hfile.CacheStats getStats()"], ["void", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.shutdown()", "public void shutdown()"], ["long", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.size()", "public long size()"], ["long", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getFreeSize()", "public long getFreeSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getCurrentSize()", "public long getCurrentSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getBlockCount()", "public long getBlockCount()"], ["java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock>", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator()"], ["org.apache.hadoop.hbase.io.hfile.BlockCache[]", "org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.getBlockCaches()", "public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches()"], ["int", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream)", "public int encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.useEncodedScanner()", "public boolean useEncodedScanner()"], ["void", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer)", "public void saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer)"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.getDataBlockEncoding()", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding()"], ["org.apache.hadoop.hbase.io.encoding.DataBlockEncoding", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.getEffectiveEncodingInCache(boolean)", "public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.newDataBlockEncodingContext(byte[], org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext newDataBlockEncodingContext(byte[], org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.newDataBlockDecodingContext(org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext newDataBlockDecodingContext(org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["void", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream)", "public void startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType)", "public void endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException"], ["java.lang.Thread", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor$1.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["org.apache.hadoop.hbase.io.hfile.PrefetchExecutor", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor()", "public org.apache.hadoop.hbase.io.hfile.PrefetchExecutor()"], ["void", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor.request(org.apache.hadoop.fs.Path, java.lang.Runnable)", "public static void request(org.apache.hadoop.fs.Path, java.lang.Runnable)"], ["void", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor.complete(org.apache.hadoop.fs.Path)", "public static void complete(org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor.cancel(org.apache.hadoop.fs.Path)", "public static void cancel(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.PrefetchExecutor.isCompleted(org.apache.hadoop.fs.Path)", "public static boolean isCompleted(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket(long)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket(long)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.isUninstantiated()", "public boolean isUninstantiated()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.sizeIndex()", "public int sizeIndex()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.getItemAllocationSize()", "public int getItemAllocationSize()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.hasFreeSpace()", "public boolean hasFreeSpace()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.isCompletelyFree()", "public boolean isCompletelyFree()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.freeCount()", "public int freeCount()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.usedCount()", "public int usedCount()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.getFreeBytes()", "public int getFreeBytes()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.getUsedBytes()", "public int getUsedBytes()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.getBaseOffset()", "public long getBaseOffset()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.allocate()", "public long allocate()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket.addAllocation(long)", "public void addAllocation(long) throws org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.instantiateBucket(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket)", "public void instantiateBucket(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.sizeIndex()", "public int sizeIndex()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.allocateBlock()", "public long allocateBlock()"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.findAndRemoveCompletelyFreeBucket()", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket findAndRemoveCompletelyFreeBucket()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.freeBlock(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket, long)", "public void freeBlock(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket, long)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.statistics()", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics statistics()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.freeCount()", "public long freeCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.usedCount()", "public long usedCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.totalCount()", "public long totalCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.freeBytes()", "public long freeBytes()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.usedBytes()", "public long usedBytes()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.totalBytes()", "public long totalBytes()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.itemSize()", "public long itemSize()"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics(long, long, long)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics(long, long, long)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics()", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics.setTo(long, long, long)", "public void setTo(long, long, long)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.roundUpToBucketSizeInfo(int)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo roundUpToBucketSizeInfo(int)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.getUsedSize()", "public long getUsedSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.getFreeSize()", "public long getFreeSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.getTotalSize()", "public long getTotalSize()"], ["synchronized", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.long allocateBlock(int)", "public synchronized long allocateBlock(int) throws org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException"], ["synchronized", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.int freeBlock(long)", "public synchronized int freeBlock(long)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.sizeIndexOfAllocation(long)", "public int sizeIndexOfAllocation(long)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.sizeOfAllocation(long)", "public int sizeOfAllocation(long)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket[]", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.getBuckets()", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket[] getBuckets()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.freeBlock(long[])", "public long freeBlock(long[])"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$1.compare(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public int compare(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getBlockPriority()", "public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority()"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getOffset()", "public long getOffset()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getSize()", "public long getSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getCachedTime()", "public long getCachedTime()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.getFilename()", "public java.lang.String getFilename()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock)", "public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2.hasNext()", "public boolean hasNext()"], ["org.apache.hadoop.hbase.io.hfile.CachedBlock", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2.next()", "public org.apache.hadoop.hbase.io.hfile.CachedBlock next()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2.remove()", "public void remove()"], ["java.lang.Object", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2.next()", "public java.lang.Object next()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry$1.compare(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry)", "public int compare(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry.getLength()", "public int getLength()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry.access(long)", "public void access(long)"], ["org.apache.hadoop.hbase.io.hfile.BlockPriority", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry.getPriority()", "public org.apache.hadoop.hbase.io.hfile.BlockPriority getPriority()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry.getCachedTime()", "public long getCachedTime()"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache, long, long, long)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache, long, long, long)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)", "public void add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.free(long)", "public long free(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.overflow()", "public long overflow()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.totalSize()", "public long totalSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.compareTo(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup)", "public int compareTo(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long, boolean)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long, boolean)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry.getData()", "public org.apache.hadoop.hbase.io.hfile.Cacheable getData()"], ["org.apache.hadoop.hbase.io.hfile.BlockCacheKey", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry.getKey()", "public org.apache.hadoop.hbase.io.hfile.BlockCacheKey getKey()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry.access(long)", "public void access(long)"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry.writeToCache(org.apache.hadoop.hbase.io.hfile.bucket.IOEngine, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator, org.apache.hadoop.hbase.io.hfile.bucket.UniqueIndexMap<java.lang.Integer>, java.util.concurrent.atomic.AtomicLong)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry writeToCache(org.apache.hadoop.hbase.io.hfile.bucket.IOEngine, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator, org.apache.hadoop.hbase.io.hfile.bucket.UniqueIndexMap<java.lang.Integer>, java.util.concurrent.atomic.AtomicLong) throws org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException, java.io.IOException, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$WriterThread.run()", "public void run()"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String) throws java.io.FileNotFoundException, java.io.IOException"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketCache", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String, int)", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String, int) throws java.io.FileNotFoundException, java.io.IOException"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getMaxSize()", "public long getMaxSize()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getIoEngine()", "public java.lang.String getIoEngine()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.cacheBlockWithWait(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)", "public void cacheBlockWithWait(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean)"], ["org.apache.hadoop.hbase.io.hfile.Cacheable", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean)"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)", "public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.logStats()", "public void logStats()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getFailedBlockAdditions()", "public long getFailedBlockAdditions()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getRealCacheSize()", "public long getRealCacheSize()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.shutdown()", "public void shutdown()"], ["org.apache.hadoop.hbase.io.hfile.CacheStats", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getStats()", "public org.apache.hadoop.hbase.io.hfile.CacheStats getStats()"], ["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getAllocator()", "public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator getAllocator()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.heapSize()", "public long heapSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.size()", "public long size()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getFreeSize()", "public long getFreeSize()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getBlockCount()", "public long getBlockCount()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getCurrentSize()", "public long getCurrentSize()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.evictBlocksByHfileName(java.lang.String)", "public int evictBlocksByHfileName(java.lang.String)"], ["java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock>", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator()"], ["org.apache.hadoop.hbase.io.hfile.BlockCache[]", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.getBlockCaches()", "public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.ioHit(long)", "public void ioHit(long)"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.getIOHitsPerSecond()", "public long getIOHitsPerSecond()"], ["double", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.getIOTimePerHit()", "public double getIOTimePerHit()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.reset()", "public void reset()"], ["org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine(long, boolean)", "public org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine(long, boolean) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.isPersistent()", "public boolean isPersistent()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.read(java.nio.ByteBuffer, long)", "public int read(java.nio.ByteBuffer, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.write(java.nio.ByteBuffer, long)", "public void write(java.nio.ByteBuffer, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.sync()", "public void sync()"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.shutdown()", "public void shutdown()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException.bucketIndex()", "public int bucketIndex()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException.requestedSize()", "public int requestedSize()"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue$1.compare(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>, java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)", "public int compare(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>, java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue(long, long)", "public org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue(long, long)"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue.add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)", "public void add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>)"], ["java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue.poll()", "public java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry> poll()"], ["java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry>", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue.pollLast()", "public java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntry> pollLast()"], ["long", "org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue.cacheSize()", "public long cacheSize()"], ["org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine(java.lang.String, long)", "public org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine(java.lang.String, long) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.isPersistent()", "public boolean isPersistent()"], ["int", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.read(java.nio.ByteBuffer, long)", "public int read(java.nio.ByteBuffer, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.write(java.nio.ByteBuffer, long)", "public void write(java.nio.ByteBuffer, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.sync()", "public void sync() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.shutdown()", "public void shutdown()"], ["org.apache.hadoop.hbase.io.hfile.bucket.UniqueIndexMap", "org.apache.hadoop.hbase.io.hfile.bucket.UniqueIndexMap()", "public org.apache.hadoop.hbase.io.hfile.bucket.UniqueIndexMap()"], ["org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, int)", "public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, int)"], ["org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)"], ["org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)", "public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)"], ["org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)", "public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)"], ["void", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.dispatch(org.apache.hadoop.hbase.ipc.CallRunner)", "public void dispatch(org.apache.hadoop.hbase.ipc.CallRunner) throws java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.getQueueLength()", "public int getQueueLength()"], ["java.util.List<java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner>>", "org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.getQueues()", "public java.util.List<java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner>> getQueues()"], ["org.apache.hadoop.hbase.ipc.RpcServer$Call", "org.apache.hadoop.hbase.ipc.CallRunner.getCall()", "public org.apache.hadoop.hbase.ipc.RpcServer$Call getCall()"], ["void", "org.apache.hadoop.hbase.ipc.CallRunner.run()", "public void run()"], ["org.apache.hadoop.hbase.ipc.EmptyServiceNameException", "org.apache.hadoop.hbase.ipc.EmptyServiceNameException()", "public org.apache.hadoop.hbase.ipc.EmptyServiceNameException()"], ["void", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run()", "public void run()"], ["org.apache.hadoop.hbase.ipc.FifoRpcScheduler", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler(org.apache.hadoop.conf.Configuration, int)", "public org.apache.hadoop.hbase.ipc.FifoRpcScheduler(org.apache.hadoop.conf.Configuration, int)"], ["void", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context)", "public void init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context)"], ["void", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.stop()", "public void stop()"], ["void", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.dispatch(org.apache.hadoop.hbase.ipc.CallRunner)", "public void dispatch(org.apache.hadoop.hbase.ipc.CallRunner) throws java.io.IOException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.getGeneralQueueLength()", "public int getGeneralQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.getPriorityQueueLength()", "public int getPriorityQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.getReplicationQueueLength()", "public int getReplicationQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.FifoRpcScheduler.getActiveRpcHandlerCount()", "public int getActiveRpcHandlerCount()"], ["org.apache.hadoop.hbase.ipc.MetricsHBaseServer", "org.apache.hadoop.hbase.ipc.MetricsHBaseServer(java.lang.String, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper)", "public org.apache.hadoop.hbase.ipc.MetricsHBaseServer(java.lang.String, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper)"], ["void", "org.apache.hadoop.hbase.ipc.MetricsHBaseServer.exception(java.lang.Throwable)", "public void exception(java.lang.Throwable)"], ["org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource", "org.apache.hadoop.hbase.ipc.MetricsHBaseServer.getMetricsSource()", "public org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource getMetricsSource()"], ["long", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getTotalQueueSize()", "public long getTotalQueueSize()"], ["int", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getGeneralQueueLength()", "public int getGeneralQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getReplicationQueueLength()", "public int getReplicationQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getPriorityQueueLength()", "public int getPriorityQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getNumOpenConnections()", "public int getNumOpenConnections()"], ["int", "org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.getActiveRpcHandlerCount()", "public int getActiveRpcHandlerCount()"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int)", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int)"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, float, float, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object...)"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, int, int, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[], java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[])", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, int, int, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[], java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[])"], ["org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, int, int, float, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[], java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[])", "public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, int, int, float, java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[], java.lang.Class<? extends java.util.concurrent.BlockingQueue>, java.lang.Object[])"], ["void", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.dispatch(org.apache.hadoop.hbase.ipc.CallRunner)", "public void dispatch(org.apache.hadoop.hbase.ipc.CallRunner) throws java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.getQueueLength()", "public int getQueueLength()"], ["void", "org.apache.hadoop.hbase.ipc.RpcExecutor$1.run()", "public void run()"], ["int", "org.apache.hadoop.hbase.ipc.RpcExecutor$2.getNextQueue()", "public int getNextQueue()"], ["org.apache.hadoop.hbase.ipc.RpcExecutor$QueueBalancer", "org.apache.hadoop.hbase.ipc.RpcExecutor$QueueBalancer()", "public org.apache.hadoop.hbase.ipc.RpcExecutor$QueueBalancer()"], ["org.apache.hadoop.hbase.ipc.RpcExecutor$RandomQueueBalancer", "org.apache.hadoop.hbase.ipc.RpcExecutor$RandomQueueBalancer(int)", "public org.apache.hadoop.hbase.ipc.RpcExecutor$RandomQueueBalancer(int)"], ["int", "org.apache.hadoop.hbase.ipc.RpcExecutor$RandomQueueBalancer.getNextQueue()", "public int getNextQueue()"], ["org.apache.hadoop.hbase.ipc.RpcExecutor", "org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int)", "public org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int)"], ["org.apache.hadoop.hbase.ipc.RpcExecutor", "org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable)"], ["void", "org.apache.hadoop.hbase.ipc.RpcExecutor.start(int)", "public void start(int)"], ["void", "org.apache.hadoop.hbase.ipc.RpcExecutor.stop()", "public void stop()"], ["int", "org.apache.hadoop.hbase.ipc.RpcExecutor.getActiveHandlerCount()", "public int getActiveHandlerCount()"], ["org.apache.hadoop.hbase.ipc.RpcExecutor$QueueBalancer", "org.apache.hadoop.hbase.ipc.RpcExecutor.getBalancer(int)", "public static org.apache.hadoop.hbase.ipc.RpcExecutor$QueueBalancer getBalancer(int)"], ["org.apache.hadoop.hbase.ipc.RpcScheduler", "org.apache.hadoop.hbase.ipc.RpcScheduler()", "public org.apache.hadoop.hbase.ipc.RpcScheduler()"], ["java.net.InetSocketAddress", "org.apache.hadoop.hbase.ipc.RpcSchedulerContext.getListenerAddress()", "public java.net.InetSocketAddress getListenerAddress()"], ["org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface", "org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface(com.google.protobuf.BlockingService, java.lang.Class<?>)", "public org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface(com.google.protobuf.BlockingService, java.lang.Class<?>)"], ["java.lang.Class<?>", "org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface.getServiceInterface()", "public java.lang.Class<?> getServiceInterface()"], ["com.google.protobuf.BlockingService", "org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface.getBlockingService()", "public com.google.protobuf.BlockingService getBlockingService()"], ["java.lang.String", "org.apache.hadoop.hbase.ipc.RpcServer$Call.toString()", "public java.lang.String toString()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.void endDelay(java.lang.Object)", "public synchronized void endDelay(java.lang.Object) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.void endDelay()", "public synchronized void endDelay() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.void startDelay(boolean)", "public synchronized void startDelay(boolean)"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.void endDelayThrowing(java.lang.Throwable)", "public synchronized void endDelayThrowing(java.lang.Throwable) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.boolean isDelayed()", "public synchronized boolean isDelayed()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.boolean isReturnValueDelayed()", "public synchronized boolean isReturnValueDelayed()"], ["boolean", "org.apache.hadoop.hbase.ipc.RpcServer$Call.isClientCellBlockSupport()", "public boolean isClientCellBlockSupport()"], ["long", "org.apache.hadoop.hbase.ipc.RpcServer$Call.disconnectSince()", "public long disconnectSince()"], ["long", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getSize()", "public long getSize()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Call.void sendResponseIfReady()", "public synchronized void sendResponseIfReady() throws java.io.IOException"], ["org.apache.hadoop.security.UserGroupInformation", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getRemoteUser()", "public org.apache.hadoop.security.UserGroupInformation getRemoteUser()"], ["org.apache.hadoop.hbase.security.User", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getRequestUser()", "public org.apache.hadoop.hbase.security.User getRequestUser()"], ["java.lang.String", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getRequestUserName()", "public java.lang.String getRequestUserName()"], ["java.net.InetAddress", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getRemoteAddress()", "public java.net.InetAddress getRemoteAddress()"], ["org.apache.hadoop.hbase.protobuf.generated.RPCProtos$VersionInfo", "org.apache.hadoop.hbase.ipc.RpcServer$Call.getClientVersionInfo()", "public org.apache.hadoop.hbase.protobuf.generated.RPCProtos$VersionInfo getClientVersionInfo()"], ["java.lang.Object", "org.apache.hadoop.hbase.ipc.RpcServer$Connection$1.run()", "public java.lang.Object run() throws javax.security.sasl.SaslException"], ["org.apache.hadoop.hbase.ipc.RpcServer$Connection", "org.apache.hadoop.hbase.ipc.RpcServer$Connection(org.apache.hadoop.hbase.ipc.RpcServer, java.nio.channels.SocketChannel, long)", "public org.apache.hadoop.hbase.ipc.RpcServer$Connection(org.apache.hadoop.hbase.ipc.RpcServer, java.nio.channels.SocketChannel, long)"], ["java.lang.String", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.getHostAddress()", "public java.lang.String getHostAddress()"], ["java.net.InetAddress", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.getHostInetAddress()", "public java.net.InetAddress getHostInetAddress()"], ["int", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.getRemotePort()", "public int getRemotePort()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.setLastContact(long)", "public void setLastContact(long)"], ["org.apache.hadoop.hbase.protobuf.generated.RPCProtos$VersionInfo", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.getVersionInfo()", "public org.apache.hadoop.hbase.protobuf.generated.RPCProtos$VersionInfo getVersionInfo()"], ["int", "org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess()", "public int readAndProcess() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.startAdd()", "public void startAdd()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.java.nio.channels.SelectionKey registerChannel(java.nio.channels.SocketChannel)", "public synchronized java.nio.channels.SelectionKey registerChannel(java.nio.channels.SocketChannel) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.void finishAdd()", "public synchronized void finishAdd()"], ["org.apache.hadoop.hbase.ipc.RpcServer$Listener", "org.apache.hadoop.hbase.ipc.RpcServer$Listener(org.apache.hadoop.hbase.ipc.RpcServer, java.lang.String)", "public org.apache.hadoop.hbase.ipc.RpcServer$Listener(org.apache.hadoop.hbase.ipc.RpcServer, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Listener.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Responder.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer$Responder.registerForWrite(org.apache.hadoop.hbase.ipc.RpcServer$Connection)", "public void registerForWrite(org.apache.hadoop.hbase.ipc.RpcServer$Connection)"], ["org.apache.hadoop.hbase.ipc.RpcServer", "org.apache.hadoop.hbase.ipc.RpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler)", "public org.apache.hadoop.hbase.ipc.RpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.setSocketSendBufSize(int)", "public void setSocketSendBufSize(int)"], ["boolean", "org.apache.hadoop.hbase.ipc.RpcServer.isStarted()", "public boolean isStarted()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer.void start()", "public synchronized void start()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.refreshAuthManager(org.apache.hadoop.security.authorize.PolicyProvider)", "public void refreshAuthManager(org.apache.hadoop.security.authorize.PolicyProvider)"], ["org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>", "org.apache.hadoop.hbase.ipc.RpcServer.getSecretManager()", "public org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier> getSecretManager()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.setSecretManager(org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>)", "public void setSecretManager(org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>)"], ["org.apache.hadoop.hbase.util.Pair<com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner>", "org.apache.hadoop.hbase.ipc.RpcServer.call(com.google.protobuf.BlockingService, com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler)", "public org.apache.hadoop.hbase.util.Pair<com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner> call(com.google.protobuf.BlockingService, com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer.void stop()", "public synchronized void stop()"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer.void join()", "public synchronized void join() throws java.lang.InterruptedException"], ["synchronized", "org.apache.hadoop.hbase.ipc.RpcServer.java.net.InetSocketAddress getListenerAddress()", "public synchronized java.net.InetSocketAddress getListenerAddress()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.setErrorHandler(org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler)", "public void setErrorHandler(org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler)"], ["org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler", "org.apache.hadoop.hbase.ipc.RpcServer.getErrorHandler()", "public org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler getErrorHandler()"], ["org.apache.hadoop.hbase.ipc.MetricsHBaseServer", "org.apache.hadoop.hbase.ipc.RpcServer.getMetrics()", "public org.apache.hadoop.hbase.ipc.MetricsHBaseServer getMetrics()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.addCallSize(long)", "public void addCallSize(long)"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.authorize(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.protobuf.generated.RPCProtos$ConnectionHeader, java.net.InetAddress)", "public void authorize(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.protobuf.generated.RPCProtos$ConnectionHeader, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException"], ["org.apache.hadoop.hbase.ipc.RpcCallContext", "org.apache.hadoop.hbase.ipc.RpcServer.getCurrentCall()", "public static org.apache.hadoop.hbase.ipc.RpcCallContext getCurrentCall()"], ["boolean", "org.apache.hadoop.hbase.ipc.RpcServer.isInRpcCallContext()", "public static boolean isInRpcCallContext()"], ["org.apache.hadoop.hbase.security.User", "org.apache.hadoop.hbase.ipc.RpcServer.getRequestUser()", "public static org.apache.hadoop.hbase.security.User getRequestUser()"], ["java.lang.String", "org.apache.hadoop.hbase.ipc.RpcServer.getRequestUserName()", "public static java.lang.String getRequestUserName()"], ["java.net.InetAddress", "org.apache.hadoop.hbase.ipc.RpcServer.getRemoteAddress()", "public static java.net.InetAddress getRemoteAddress()"], ["java.net.InetAddress", "org.apache.hadoop.hbase.ipc.RpcServer.getRemoteIp()", "public static java.net.InetAddress getRemoteIp()"], ["void", "org.apache.hadoop.hbase.ipc.RpcServer.bind(java.net.ServerSocket, java.net.InetSocketAddress, int)", "public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.ipc.RpcScheduler", "org.apache.hadoop.hbase.ipc.RpcServer.getScheduler()", "public org.apache.hadoop.hbase.ipc.RpcScheduler getScheduler()"], ["org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$CallPriorityComparator", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$CallPriorityComparator(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction)", "public org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$CallPriorityComparator(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction)"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$CallPriorityComparator.compare(org.apache.hadoop.hbase.ipc.CallRunner, org.apache.hadoop.hbase.ipc.CallRunner)", "public int compare(org.apache.hadoop.hbase.ipc.CallRunner, org.apache.hadoop.hbase.ipc.CallRunner)"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$CallPriorityComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.ipc.SimpleRpcScheduler", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable, int)", "public org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable, int)"], ["org.apache.hadoop.hbase.ipc.SimpleRpcScheduler", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, int)", "public org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, int)"], ["void", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context)", "public void init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context)"], ["void", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.stop()", "public void stop()"], ["void", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.dispatch(org.apache.hadoop.hbase.ipc.CallRunner)", "public void dispatch(org.apache.hadoop.hbase.ipc.CallRunner) throws java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.getGeneralQueueLength()", "public int getGeneralQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.getPriorityQueueLength()", "public int getPriorityQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.getReplicationQueueLength()", "public int getReplicationQueueLength()"], ["int", "org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.getActiveRpcHandlerCount()", "public int getActiveRpcHandlerCount()"], ["org.apache.hadoop.hbase.mapred.Driver", "org.apache.hadoop.hbase.mapred.Driver()", "public org.apache.hadoop.hbase.mapred.Driver()"], ["void", "org.apache.hadoop.hbase.mapred.Driver.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Throwable"], ["org.apache.hadoop.hbase.mapred.GroupingTableMap", "org.apache.hadoop.hbase.mapred.GroupingTableMap()", "public org.apache.hadoop.hbase.mapred.GroupingTableMap()"], ["void", "org.apache.hadoop.hbase.mapred.GroupingTableMap.initJob(java.lang.String, java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, org.apache.hadoop.mapred.JobConf)", "public static void initJob(java.lang.String, java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.GroupingTableMap.configure(org.apache.hadoop.mapred.JobConf)", "public void configure(org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.GroupingTableMap.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.GroupingTableMap.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.HRegionPartitioner", "org.apache.hadoop.hbase.mapred.HRegionPartitioner()", "public org.apache.hadoop.hbase.mapred.HRegionPartitioner()"], ["void", "org.apache.hadoop.hbase.mapred.HRegionPartitioner.configure(org.apache.hadoop.mapred.JobConf)", "public void configure(org.apache.hadoop.mapred.JobConf)"], ["int", "org.apache.hadoop.hbase.mapred.HRegionPartitioner.getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, V2, int)", "public int getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, V2, int)"], ["int", "org.apache.hadoop.hbase.mapred.HRegionPartitioner.getPartition(java.lang.Object, java.lang.Object, int)", "public int getPartition(java.lang.Object, java.lang.Object, int)"], ["org.apache.hadoop.hbase.mapred.IdentityTableMap", "org.apache.hadoop.hbase.mapred.IdentityTableMap()", "public org.apache.hadoop.hbase.mapred.IdentityTableMap()"], ["void", "org.apache.hadoop.hbase.mapred.IdentityTableMap.initJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, org.apache.hadoop.mapred.JobConf)", "public static void initJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.IdentityTableMap.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.IdentityTableMap.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.IdentityTableReduce", "org.apache.hadoop.hbase.mapred.IdentityTableReduce()", "public org.apache.hadoop.hbase.mapred.IdentityTableReduce()"], ["void", "org.apache.hadoop.hbase.mapred.IdentityTableReduce.reduce(org.apache.hadoop.hbase.io.ImmutableBytesWritable, java.util.Iterator<org.apache.hadoop.hbase.client.Put>, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>, org.apache.hadoop.mapred.Reporter)", "public void reduce(org.apache.hadoop.hbase.io.ImmutableBytesWritable, java.util.Iterator<org.apache.hadoop.hbase.client.Put>, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.IdentityTableReduce.reduce(java.lang.Object, java.util.Iterator, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)", "public void reduce(java.lang.Object, java.util.Iterator, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters[]", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters.values()", "public static org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters[] values()"], ["org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapred.OutputCollector<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper.configure(org.apache.hadoop.mapred.JobConf)", "public void configure(org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.RowCounter", "org.apache.hadoop.hbase.mapred.RowCounter()", "public org.apache.hadoop.hbase.mapred.RowCounter()"], ["org.apache.hadoop.mapred.JobConf", "org.apache.hadoop.hbase.mapred.RowCounter.createSubmittableJob(java.lang.String[])", "public org.apache.hadoop.mapred.JobConf createSubmittableJob(java.lang.String[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.mapred.RowCounter.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapred.RowCounter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapred.TableInputFormat", "org.apache.hadoop.hbase.mapred.TableInputFormat()", "public org.apache.hadoop.hbase.mapred.TableInputFormat()"], ["void", "org.apache.hadoop.hbase.mapred.TableInputFormat.configure(org.apache.hadoop.mapred.JobConf)", "public void configure(org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.TableInputFormat.validateInput(org.apache.hadoop.mapred.JobConf)", "public void validateInput(org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.createKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable createKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.createValue()", "public org.apache.hadoop.hbase.client.Result createValue()"], ["long", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.getPos()", "public long getPos() throws java.io.IOException"], ["float", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.getProgress()", "public float getProgress() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)", "public boolean next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.createValue()", "public java.lang.Object createValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.createKey()", "public java.lang.Object createKey()"], ["boolean", "org.apache.hadoop.hbase.mapred.TableInputFormatBase$1.next(java.lang.Object, java.lang.Object)", "public boolean next(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableInputFormatBase", "org.apache.hadoop.hbase.mapred.TableInputFormatBase()", "public org.apache.hadoop.hbase.mapred.TableInputFormatBase()"], ["org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapred.TableInputFormatBase.getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter)", "public org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result> getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["org.apache.hadoop.mapred.InputSplit[]", "org.apache.hadoop.hbase.mapred.TableInputFormatBase.getSplits(org.apache.hadoop.mapred.JobConf, int)", "public org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableMapReduceUtil", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil()", "public org.apache.hadoop.hbase.mapred.TableMapReduceUtil()"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf)", "public static void initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf)"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean)", "public static void initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean)"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean, java.lang.Class<? extends org.apache.hadoop.mapred.InputFormat>)", "public static void initTableMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean, java.lang.Class<? extends org.apache.hadoop.mapred.InputFormat>)"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableSnapshotMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean, org.apache.hadoop.fs.Path)", "public static void initTableSnapshotMapJob(java.lang.String, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableMap>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapred.JobConf, boolean, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf)", "public static void initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf, java.lang.Class)", "public static void initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf, java.lang.Class) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf, java.lang.Class, boolean)", "public static void initTableReduceJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapred.TableReduce>, org.apache.hadoop.mapred.JobConf, java.lang.Class, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(org.apache.hadoop.mapred.JobConf)", "public static void initCredentials(org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.limitNumReduceTasks(java.lang.String, org.apache.hadoop.mapred.JobConf)", "public static void limitNumReduceTasks(java.lang.String, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.limitNumMapTasks(java.lang.String, org.apache.hadoop.mapred.JobConf)", "public static void limitNumMapTasks(java.lang.String, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.setNumReduceTasks(java.lang.String, org.apache.hadoop.mapred.JobConf)", "public static void setNumReduceTasks(java.lang.String, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.setNumMapTasks(java.lang.String, org.apache.hadoop.mapred.JobConf)", "public static void setNumMapTasks(java.lang.String, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.setScannerCaching(org.apache.hadoop.mapred.JobConf, int)", "public static void setScannerCaching(org.apache.hadoop.mapred.JobConf, int)"], ["void", "org.apache.hadoop.hbase.mapred.TableMapReduceUtil.addDependencyJars(org.apache.hadoop.mapred.JobConf)", "public static void addDependencyJars(org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter", "org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter(org.apache.hadoop.hbase.client.BufferedMutator)", "public org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter(org.apache.hadoop.hbase.client.BufferedMutator) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.close(org.apache.hadoop.mapred.Reporter)", "public void close(org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put)", "public void write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.write(java.lang.Object, java.lang.Object)", "public void write(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableOutputFormat", "org.apache.hadoop.hbase.mapred.TableOutputFormat()", "public org.apache.hadoop.hbase.mapred.TableOutputFormat()"], ["org.apache.hadoop.mapred.RecordWriter", "org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable)", "public org.apache.hadoop.mapred.RecordWriter getRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableOutputFormat.checkOutputSpecs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf)", "public void checkOutputSpecs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf) throws org.apache.hadoop.fs.FileAlreadyExistsException, org.apache.hadoop.mapred.InvalidJobConfException, java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableRecordReader", "org.apache.hadoop.hbase.mapred.TableRecordReader()", "public org.apache.hadoop.hbase.mapred.TableRecordReader()"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.restart(byte[])", "public void restart(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.init()", "public void init() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.setHTable(org.apache.hadoop.hbase.client.Table)", "public void setHTable(org.apache.hadoop.hbase.client.Table)"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.setInputColumns(byte[][])", "public void setInputColumns(byte[][])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.setStartRow(byte[])", "public void setStartRow(byte[])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.setEndRow(byte[])", "public void setEndRow(byte[])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.setRowFilter(org.apache.hadoop.hbase.filter.Filter)", "public void setRowFilter(org.apache.hadoop.hbase.filter.Filter)"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReader.close()", "public void close()"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapred.TableRecordReader.createKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable createKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapred.TableRecordReader.createValue()", "public org.apache.hadoop.hbase.client.Result createValue()"], ["long", "org.apache.hadoop.hbase.mapred.TableRecordReader.getPos()", "public long getPos()"], ["float", "org.apache.hadoop.hbase.mapred.TableRecordReader.getProgress()", "public float getProgress()"], ["boolean", "org.apache.hadoop.hbase.mapred.TableRecordReader.next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)", "public boolean next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableRecordReader.createValue()", "public java.lang.Object createValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableRecordReader.createKey()", "public java.lang.Object createKey()"], ["boolean", "org.apache.hadoop.hbase.mapred.TableRecordReader.next(java.lang.Object, java.lang.Object)", "public boolean next(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableRecordReaderImpl", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl()", "public org.apache.hadoop.hbase.mapred.TableRecordReaderImpl()"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.restart(byte[])", "public void restart(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.init()", "public void init() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.setHTable(org.apache.hadoop.hbase.client.Table)", "public void setHTable(org.apache.hadoop.hbase.client.Table)"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.setInputColumns(byte[][])", "public void setInputColumns(byte[][])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.setStartRow(byte[])", "public void setStartRow(byte[])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.setEndRow(byte[])", "public void setEndRow(byte[])"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.setRowFilter(org.apache.hadoop.hbase.filter.Filter)", "public void setRowFilter(org.apache.hadoop.hbase.filter.Filter)"], ["void", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.close()", "public void close()"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.createKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable createKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.createValue()", "public org.apache.hadoop.hbase.client.Result createValue()"], ["long", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.getPos()", "public long getPos()"], ["float", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.getProgress()", "public float getProgress()"], ["boolean", "org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)", "public boolean next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader(org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit, org.apache.hadoop.mapred.JobConf)", "public org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader(org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit, org.apache.hadoop.mapred.JobConf) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)", "public boolean next(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.createKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable createKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.createValue()", "public org.apache.hadoop.hbase.client.Result createValue()"], ["long", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.getPos()", "public long getPos() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.close()", "public void close() throws java.io.IOException"], ["float", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.getProgress()", "public float getProgress() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.createValue()", "public java.lang.Object createValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.createKey()", "public java.lang.Object createKey()"], ["boolean", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader.next(java.lang.Object, java.lang.Object)", "public boolean next(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit()", "public org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit()"], ["org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit)", "public org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit)"], ["org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)"], ["long", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit.getLength()", "public long getLength() throws java.io.IOException"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit.getLocations()", "public java.lang.String[] getLocations() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat()", "public org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat()"], ["org.apache.hadoop.mapred.InputSplit[]", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.getSplits(org.apache.hadoop.mapred.JobConf, int)", "public org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf, int) throws java.io.IOException"], ["org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter)", "public org.apache.hadoop.mapred.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result> getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.setInput(org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.fs.Path)", "public static void setInput(org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapred.TableSplit", "org.apache.hadoop.hbase.mapred.TableSplit()", "public org.apache.hadoop.hbase.mapred.TableSplit()"], ["org.apache.hadoop.hbase.mapred.TableSplit", "org.apache.hadoop.hbase.mapred.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapred.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.mapred.TableSplit", "org.apache.hadoop.hbase.mapred.TableSplit(byte[], byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapred.TableSplit(byte[], byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.mapred.TableSplit.getTable()", "public org.apache.hadoop.hbase.TableName getTable()"], ["byte[]", "org.apache.hadoop.hbase.mapred.TableSplit.getTableName()", "public byte[] getTableName()"], ["byte[]", "org.apache.hadoop.hbase.mapred.TableSplit.getStartRow()", "public byte[] getStartRow()"], ["byte[]", "org.apache.hadoop.hbase.mapred.TableSplit.getEndRow()", "public byte[] getEndRow()"], ["java.lang.String", "org.apache.hadoop.hbase.mapred.TableSplit.getRegionLocation()", "public java.lang.String getRegionLocation()"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapred.TableSplit.getLocations()", "public java.lang.String[] getLocations()"], ["long", "org.apache.hadoop.hbase.mapred.TableSplit.getLength()", "public long getLength()"], ["void", "org.apache.hadoop.hbase.mapred.TableSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapred.TableSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapred.TableSplit.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.mapred.TableSplit.compareTo(org.apache.hadoop.hbase.mapred.TableSplit)", "public int compareTo(org.apache.hadoop.hbase.mapred.TableSplit)"], ["boolean", "org.apache.hadoop.hbase.mapred.TableSplit.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.mapred.TableSplit.hashCode()", "public int hashCode()"], ["int", "org.apache.hadoop.hbase.mapred.TableSplit.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters[]", "org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters.values()", "public static org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters[] values()"], ["org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters", "org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.io.Text, org.apache.hadoop.io.IntWritable>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.io.Text, org.apache.hadoop.io.IntWritable>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.CellCounter$IntSumReducer.reduce(Key, java.lang.Iterable<org.apache.hadoop.io.IntWritable>, org.apache.hadoop.mapreduce.Reducer<Key, org.apache.hadoop.io.IntWritable, Key, org.apache.hadoop.io.IntWritable>.Context)", "public void reduce(Key, java.lang.Iterable<org.apache.hadoop.io.IntWritable>, org.apache.hadoop.mapreduce.Reducer<Key, org.apache.hadoop.io.IntWritable, Key, org.apache.hadoop.io.IntWritable>.Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.CellCounter", "org.apache.hadoop.hbase.mapreduce.CellCounter()", "public org.apache.hadoop.hbase.mapreduce.CellCounter()"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.CellCounter.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.CellCounter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.CellCreator", "org.apache.hadoop.hbase.mapreduce.CellCreator(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.mapreduce.CellCreator(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.mapreduce.CellCreator.create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int)", "public org.apache.hadoop.hbase.Cell create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.mapreduce.CellCreator.create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int, java.lang.String)", "public org.apache.hadoop.hbase.Cell create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.mapreduce.CellCreator.create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int, java.util.List<org.apache.hadoop.hbase.Tag>)", "public org.apache.hadoop.hbase.Cell create(byte[], int, int, byte[], int, int, byte[], int, int, long, byte[], int, int, java.util.List<org.apache.hadoop.hbase.Tag>) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.VisibilityExpressionResolver", "org.apache.hadoop.hbase.mapreduce.CellCreator.getVisibilityExpressionResolver()", "public org.apache.hadoop.hbase.mapreduce.VisibilityExpressionResolver getVisibilityExpressionResolver()"], ["org.apache.hadoop.hbase.mapreduce.CopyTable", "org.apache.hadoop.hbase.mapreduce.CopyTable(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.mapreduce.CopyTable(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.CopyTable.createSubmittableJob(java.lang.String[])", "public org.apache.hadoop.mapreduce.Job createSubmittableJob(java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.CopyTable.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.mapreduce.CopyTable.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver$1.getLabelOrdinal(java.lang.String)", "public int getLabelOrdinal(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver$1.getLabel(int)", "public java.lang.String getLabel(int)"], ["org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver()", "public org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.init()", "public void init()"], ["java.util.List<org.apache.hadoop.hbase.Tag>", "org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver.createVisibilityExpTags(java.lang.String)", "public java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.Driver", "org.apache.hadoop.hbase.mapreduce.Driver()", "public org.apache.hadoop.hbase.mapreduce.Driver()"], ["void", "org.apache.hadoop.hbase.mapreduce.Driver.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Throwable"], ["org.apache.hadoop.hbase.mapreduce.Export", "org.apache.hadoop.hbase.mapreduce.Export()", "public org.apache.hadoop.hbase.mapreduce.Export()"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.Export.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.Export.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.GroupingTableMapper", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper()", "public org.apache.hadoop.hbase.mapreduce.GroupingTableMapper()"], ["void", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.initJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, org.apache.hadoop.mapreduce.Job)", "public static void initJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.HFileOutputFormat", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat()", "public org.apache.hadoop.hbase.mapreduce.HFileOutputFormat()"], ["org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue> getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.HTable)", "public static void configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.HTable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$1.write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, V)", "public void write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, V) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$1.close(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void close(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$1.write(java.lang.Object, java.lang.Object)", "public void write(java.lang.Object, java.lang.Object) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2()", "public org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2()"], ["org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.Cell> getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.HTable)", "public static void configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.HTable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.Table, org.apache.hadoop.hbase.client.RegionLocator)", "public static void configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.Table, org.apache.hadoop.hbase.client.RegionLocator) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.client.RegionLocator)", "public static void configureIncrementalLoad(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.client.RegionLocator) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.configureIncrementalLoadMap(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.Table)", "public static void configureIncrementalLoadMap(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.client.Table) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.mapreduce.HLogInputFormat$HLogKeyRecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.HLogInputFormat$HLogKeyRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.HLogInputFormat", "org.apache.hadoop.hbase.mapreduce.HLogInputFormat()", "public org.apache.hadoop.hbase.mapreduce.HLogInputFormat()"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.HLogInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit>", "org.apache.hadoop.hbase.mapreduce.HLogInputFormat.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.HRegionPartitioner", "org.apache.hadoop.hbase.mapreduce.HRegionPartitioner()", "public org.apache.hadoop.hbase.mapreduce.HRegionPartitioner()"], ["int", "org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, VALUE, int)", "public int getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, VALUE, int)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.getPartition(java.lang.Object, java.lang.Object, int)", "public int getPartition(java.lang.Object, java.lang.Object, int)"], ["org.apache.hadoop.hbase.mapreduce.IdentityTableMapper", "org.apache.hadoop.hbase.mapreduce.IdentityTableMapper()", "public org.apache.hadoop.hbase.mapreduce.IdentityTableMapper()"], ["void", "org.apache.hadoop.hbase.mapreduce.IdentityTableMapper.initJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, org.apache.hadoop.mapreduce.Job)", "public static void initJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.IdentityTableMapper.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.IdentityTableMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.IdentityTableReducer", "org.apache.hadoop.hbase.mapreduce.IdentityTableReducer()", "public org.apache.hadoop.hbase.mapreduce.IdentityTableReducer()"], ["void", "org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.reduce(org.apache.hadoop.io.Writable, java.lang.Iterable<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.mapreduce.Reducer<org.apache.hadoop.io.Writable, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.io.Writable, org.apache.hadoop.hbase.client.Mutation>.Context)", "public void reduce(org.apache.hadoop.io.Writable, java.lang.Iterable<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.mapreduce.Reducer<org.apache.hadoop.io.Writable, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.io.Writable, org.apache.hadoop.hbase.client.Mutation>.Context) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.reduce(java.lang.Object, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer$Context)", "public void reduce(java.lang.Object, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.Import$Importer", "org.apache.hadoop.hbase.mapreduce.Import$Importer()", "public org.apache.hadoop.hbase.mapreduce.Import$Importer()"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$Importer.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$Importer.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context)"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$Importer.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter", "org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter()", "public org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter()"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context)"], ["void", "org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.Import", "org.apache.hadoop.hbase.mapreduce.Import()", "public org.apache.hadoop.hbase.mapreduce.Import()"], ["org.apache.hadoop.hbase.filter.Filter", "org.apache.hadoop.hbase.mapreduce.Import.instantiateFilter(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.filter.Filter instantiateFilter(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.mapreduce.Import.filterKv(org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.Cell)", "public static org.apache.hadoop.hbase.Cell filterKv(org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.Import.configureCfRenaming(org.apache.hadoop.conf.Configuration, java.util.Map<java.lang.String, java.lang.String>)", "public static void configureCfRenaming(org.apache.hadoop.conf.Configuration, java.util.Map<java.lang.String, java.lang.String>)"], ["void", "org.apache.hadoop.hbase.mapreduce.Import.addFilterAndArguments(org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.filter.Filter>, java.util.List<java.lang.String>)", "public static void addFilterAndArguments(org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.filter.Filter>, java.util.List<java.lang.String>) throws java.io.IOException"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.Import.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.Import.flushRegionsIfNecessary(org.apache.hadoop.conf.Configuration)", "public static void flushRegionsIfNecessary(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.Import.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException(java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException(java.lang.String)"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getRowKeyOffset()", "public int getRowKeyOffset()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getRowKeyLength()", "public int getRowKeyLength()"], ["long", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getTimestamp(long)", "public long getTimestamp(long) throws org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getIndividualAttributes()", "public java.lang.String[] getIndividualAttributes()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getAttributeKeyOffset()", "public int getAttributeKeyOffset()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getAttributeKeyLength()", "public int getAttributeKeyLength()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellVisibilityColumnOffset()", "public int getCellVisibilityColumnOffset()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellVisibilityColumnLength()", "public int getCellVisibilityColumnLength()"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellVisibility()", "public java.lang.String getCellVisibility()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellTTLColumnOffset()", "public int getCellTTLColumnOffset()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellTTLColumnLength()", "public int getCellTTLColumnLength()"], ["long", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getCellTTL()", "public long getCellTTL()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getColumnOffset(int)", "public int getColumnOffset(int)"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getColumnLength(int)", "public int getColumnLength(int)"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getColumnCount()", "public int getColumnCount()"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getLineBytes()", "public byte[] getLineBytes()"], ["org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser(java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser(java.lang.String, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.hasTimestamp()", "public boolean hasTimestamp()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getTimestampKeyColumnIndex()", "public int getTimestampKeyColumnIndex()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.hasAttributes()", "public boolean hasAttributes()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.hasCellVisibility()", "public boolean hasCellVisibility()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.hasCellTTL()", "public boolean hasCellTTL()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getAttributesKeyColumnIndex()", "public int getAttributesKeyColumnIndex()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getCellVisibilityColumnIndex()", "public int getCellVisibilityColumnIndex()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getCellTTLColumnIndex()", "public int getCellTTLColumnIndex()"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getRowKeyColumnIndex()", "public int getRowKeyColumnIndex()"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getFamily(int)", "public byte[] getFamily(int)"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.getQualifier(int)", "public byte[] getQualifier(int)"], ["org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.parse(byte[], int)", "public org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine parse(byte[], int) throws org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException"], ["org.apache.hadoop.hbase.util.Pair<java.lang.Integer, java.lang.Integer>", "org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser.parseRowKey(byte[], int)", "public org.apache.hadoop.hbase.util.Pair<java.lang.Integer, java.lang.Integer> parseRowKey(byte[], int) throws org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException"], ["org.apache.hadoop.hbase.mapreduce.ImportTsv", "org.apache.hadoop.hbase.mapreduce.ImportTsv()", "public org.apache.hadoop.hbase.mapreduce.ImportTsv()"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.ImportTsv.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException, java.lang.ClassNotFoundException"], ["int", "org.apache.hadoop.hbase.mapreduce.ImportTsv.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapreduce.ImportTsv.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.JarFinder", "org.apache.hadoop.hbase.mapreduce.JarFinder()", "public org.apache.hadoop.hbase.mapreduce.JarFinder()"], ["void", "org.apache.hadoop.hbase.mapreduce.JarFinder.jarDir(java.io.File, java.lang.String, java.util.zip.ZipOutputStream)", "public static void jarDir(java.io.File, java.lang.String, java.util.zip.ZipOutputStream) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.JarFinder.getJar(java.lang.Class)", "public static java.lang.String getJar(java.lang.Class)"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer()", "public org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer()"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.KeyValue", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.deserialize(org.apache.hadoop.hbase.KeyValue)", "public org.apache.hadoop.hbase.KeyValue deserialize(org.apache.hadoop.hbase.KeyValue) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.open(java.io.InputStream)", "public void open(java.io.InputStream) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.deserialize(java.lang.Object)", "public java.lang.Object deserialize(java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer()", "public org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer()"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.open(java.io.OutputStream)", "public void open(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.serialize(org.apache.hadoop.hbase.KeyValue)", "public void serialize(org.apache.hadoop.hbase.KeyValue) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.serialize(java.lang.Object)", "public void serialize(java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSerialization", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization()", "public org.apache.hadoop.hbase.mapreduce.KeyValueSerialization()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.accept(java.lang.Class<?>)", "public boolean accept(java.lang.Class<?>)"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.getDeserializer(java.lang.Class<org.apache.hadoop.hbase.KeyValue>)", "public org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer getDeserializer(java.lang.Class<org.apache.hadoop.hbase.KeyValue>)"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.getSerializer(java.lang.Class<org.apache.hadoop.hbase.KeyValue>)", "public org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer getSerializer(java.lang.Class<org.apache.hadoop.hbase.KeyValue>)"], ["org.apache.hadoop.io.serializer.Deserializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.getDeserializer(java.lang.Class)", "public org.apache.hadoop.io.serializer.Deserializer getDeserializer(java.lang.Class)"], ["org.apache.hadoop.io.serializer.Serializer", "org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.getSerializer(java.lang.Class)", "public org.apache.hadoop.io.serializer.Serializer getSerializer(java.lang.Class)"], ["org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer", "org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer()", "public org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer()"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.bulkFamily(byte[])", "public byte[] bulkFamily(byte[])"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.bulkHFile(byte[], org.apache.hadoop.fs.FileStatus)", "public void bulkHFile(byte[], org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus)", "public void bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.bulkFamily(byte[])", "public java.lang.Object bulkFamily(byte[]) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem>", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call()", "public java.util.List<org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem> call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem>", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$3.call()", "public java.util.List<org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem> call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$3.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Boolean", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$4.call(int)", "public java.lang.Boolean call(int) throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$4.call(int)", "public java.lang.Object call(int) throws java.lang.Exception"], ["org.apache.hadoop.hbase.HColumnDescriptor", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$5.bulkFamily(byte[])", "public org.apache.hadoop.hbase.HColumnDescriptor bulkFamily(byte[])"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$5.bulkHFile(org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.fs.FileStatus)", "public void bulkHFile(org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$5.bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus)", "public void bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$5.bulkFamily(byte[])", "public java.lang.Object bulkFamily(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem(byte[], org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem(byte[], org.apache.hadoop.fs.Path)"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$LoadQueueItem.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles(org.apache.hadoop.conf.Configuration) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.HTable)", "public void doBulkLoad(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.HTable) throws org.apache.hadoop.hbase.TableNotFoundException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.client.Table, org.apache.hadoop.hbase.client.RegionLocator)", "public void doBulkLoad(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.client.Table, org.apache.hadoop.hbase.client.RegionLocator) throws org.apache.hadoop.hbase.TableNotFoundException, java.io.IOException"], ["byte[][]", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.inferBoundaries(java.util.TreeMap<byte[], java.lang.Integer>)", "public static byte[][] inferBoundaries(java.util.TreeMap<byte[], java.lang.Integer>)"], ["int", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat()", "public org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase()", "public org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase()"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter(org.apache.hadoop.conf.Configuration, boolean)", "public org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter(org.apache.hadoop.conf.Configuration, boolean)"], ["void", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter.close(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void close(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter.write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation)", "public void write(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter.write(java.lang.Object, java.lang.Object)", "public void write(java.lang.Object, java.lang.Object) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat()", "public org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat()"], ["void", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)", "public void checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.OutputCommitter", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.OutputCommitter getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>", "org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation> getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$MapRunner.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.close()", "public void close() throws java.io.IOException"], ["float", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.getProgress()", "public float getProgress() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordWriter.close(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void close(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordWriter.write(K2, V2)", "public void write(K2, V2) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.Counter", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter.getCounter(java.lang.Enum<?>)", "public org.apache.hadoop.mapreduce.Counter getCounter(java.lang.Enum<?>)"], ["org.apache.hadoop.mapreduce.Counter", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter.getCounter(java.lang.String, java.lang.String)", "public org.apache.hadoop.mapreduce.Counter getCounter(java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter.progress()", "public void progress()"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter.setStatus(java.lang.String)", "public void setStatus(java.lang.String)"], ["float", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter.getProgress()", "public float getProgress()"], ["org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper()", "public org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper()"], ["int", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.getNumberOfThreads(org.apache.hadoop.mapreduce.JobContext)", "public static int getNumberOfThreads(org.apache.hadoop.mapreduce.JobContext)"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.setNumberOfThreads(org.apache.hadoop.mapreduce.Job, int)", "public static void setNumberOfThreads(org.apache.hadoop.mapreduce.Job, int)"], ["<K2, V2> java.lang.Class<org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>>", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.getMapperClass(org.apache.hadoop.mapreduce.JobContext)", "public static <K2, V2> java.lang.Class<org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>> getMapperClass(org.apache.hadoop.mapreduce.JobContext)"], ["<K2, V2> void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.setMapperClass(org.apache.hadoop.mapreduce.Job, java.lang.Class<? extends org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>>)", "public static <K2, V2> void setMapperClass(org.apache.hadoop.mapreduce.Job, java.lang.Class<? extends org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>>)"], ["void", "org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.run(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>.Context)", "public void run(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, K2, V2>.Context) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationDeserializer.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Mutation", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationDeserializer.deserialize(org.apache.hadoop.hbase.client.Mutation)", "public org.apache.hadoop.hbase.client.Mutation deserialize(org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationDeserializer.open(java.io.InputStream)", "public void open(java.io.InputStream) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationDeserializer.deserialize(java.lang.Object)", "public java.lang.Object deserialize(java.lang.Object) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationSerializer.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationSerializer.open(java.io.OutputStream)", "public void open(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationSerializer.serialize(org.apache.hadoop.hbase.client.Mutation)", "public void serialize(org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationSerializer.serialize(java.lang.Object)", "public void serialize(java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.MutationSerialization", "org.apache.hadoop.hbase.mapreduce.MutationSerialization()", "public org.apache.hadoop.hbase.mapreduce.MutationSerialization()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.MutationSerialization.accept(java.lang.Class<?>)", "public boolean accept(java.lang.Class<?>)"], ["org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.hbase.client.Mutation>", "org.apache.hadoop.hbase.mapreduce.MutationSerialization.getDeserializer(java.lang.Class<org.apache.hadoop.hbase.client.Mutation>)", "public org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.hbase.client.Mutation> getDeserializer(java.lang.Class<org.apache.hadoop.hbase.client.Mutation>)"], ["org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.hbase.client.Mutation>", "org.apache.hadoop.hbase.mapreduce.MutationSerialization.getSerializer(java.lang.Class<org.apache.hadoop.hbase.client.Mutation>)", "public org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.hbase.client.Mutation> getSerializer(java.lang.Class<org.apache.hadoop.hbase.client.Mutation>)"], ["org.apache.hadoop.hbase.mapreduce.PutCombiner", "org.apache.hadoop.hbase.mapreduce.PutCombiner()", "public org.apache.hadoop.hbase.mapreduce.PutCombiner()"], ["org.apache.hadoop.hbase.mapreduce.PutSortReducer", "org.apache.hadoop.hbase.mapreduce.PutSortReducer()", "public org.apache.hadoop.hbase.mapreduce.PutSortReducer()"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$Result94Deserializer.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$Result94Deserializer.deserialize(org.apache.hadoop.hbase.client.Result)", "public org.apache.hadoop.hbase.client.Result deserialize(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$Result94Deserializer.open(java.io.InputStream)", "public void open(java.io.InputStream) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$Result94Deserializer.deserialize(java.lang.Object)", "public java.lang.Object deserialize(java.lang.Object) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultDeserializer.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultDeserializer.deserialize(org.apache.hadoop.hbase.client.Result)", "public org.apache.hadoop.hbase.client.Result deserialize(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultDeserializer.open(java.io.InputStream)", "public void open(java.io.InputStream) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultDeserializer.deserialize(java.lang.Object)", "public java.lang.Object deserialize(java.lang.Object) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultSerializer.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultSerializer.open(java.io.OutputStream)", "public void open(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultSerializer.serialize(org.apache.hadoop.hbase.client.Result)", "public void serialize(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultSerializer.serialize(java.lang.Object)", "public void serialize(java.lang.Object) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.ResultSerialization", "org.apache.hadoop.hbase.mapreduce.ResultSerialization()", "public org.apache.hadoop.hbase.mapreduce.ResultSerialization()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.ResultSerialization.accept(java.lang.Class<?>)", "public boolean accept(java.lang.Class<?>)"], ["org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapreduce.ResultSerialization.getDeserializer(java.lang.Class<org.apache.hadoop.hbase.client.Result>)", "public org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.hbase.client.Result> getDeserializer(java.lang.Class<org.apache.hadoop.hbase.client.Result>)"], ["org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapreduce.ResultSerialization.getSerializer(java.lang.Class<org.apache.hadoop.hbase.client.Result>)", "public org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.hbase.client.Result> getSerializer(java.lang.Class<org.apache.hadoop.hbase.client.Result>)"], ["org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters[]", "org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters.values()", "public static org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters[] values()"], ["org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters", "org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.RowCounter", "org.apache.hadoop.hbase.mapreduce.RowCounter()", "public org.apache.hadoop.hbase.mapreduce.RowCounter()"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.RowCounter.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.RowCounter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner()", "public org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner()"], ["void", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.setStartKey(org.apache.hadoop.conf.Configuration, byte[])", "public static void setStartKey(org.apache.hadoop.conf.Configuration, byte[])"], ["void", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.setEndKey(org.apache.hadoop.conf.Configuration, byte[])", "public static void setEndKey(org.apache.hadoop.conf.Configuration, byte[])"], ["int", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, VALUE, int)", "public int getPartition(org.apache.hadoop.hbase.io.ImmutableBytesWritable, VALUE, int)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.getPartition(java.lang.Object, java.lang.Object, int)", "public int getPartition(java.lang.Object, java.lang.Object, int)"], ["org.apache.hadoop.hbase.mapreduce.TableInputFormat", "org.apache.hadoop.hbase.mapreduce.TableInputFormat()", "public org.apache.hadoop.hbase.mapreduce.TableInputFormat()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.addColumns(org.apache.hadoop.hbase.client.Scan, byte[][])", "public static void addColumns(org.apache.hadoop.hbase.client.Scan, byte[][])"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormat.configureSplitTable(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.TableName)", "public static void configureSplitTable(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["float", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.getProgress()", "public float getProgress() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.TableInputFormatBase", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase()", "public org.apache.hadoop.hbase.mapreduce.TableInputFormatBase()"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.reverseDNS(java.net.InetAddress)", "public java.lang.String reverseDNS(java.net.InetAddress) throws javax.naming.NamingException, java.net.UnknownHostException"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.calculateRebalancedSplits(java.util.List<org.apache.hadoop.mapreduce.InputSplit>, org.apache.hadoop.mapreduce.JobContext, long)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> calculateRebalancedSplits(java.util.List<org.apache.hadoop.mapreduce.InputSplit>, org.apache.hadoop.mapreduce.JobContext, long) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplitKey(byte[], byte[], boolean)", "public static byte[] getSplitKey(byte[], byte[], boolean)"], ["org.apache.hadoop.hbase.client.Scan", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getScan()", "public org.apache.hadoop.hbase.client.Scan getScan()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.setScan(org.apache.hadoop.hbase.client.Scan)", "public void setScan(org.apache.hadoop.hbase.client.Scan)"], ["org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil()", "public org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job)", "public static void initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job)", "public static void initTableMapperJob(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job)", "public static void initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>)", "public static void initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>)", "public static void initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>)", "public static void initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, java.lang.Class<? extends org.apache.hadoop.mapreduce.InputFormat>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean)", "public static void initTableMapperJob(byte[], org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean)", "public static void initTableMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.resetCacheConfig(org.apache.hadoop.conf.Configuration)", "public static void resetCacheConfig(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableSnapshotMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, org.apache.hadoop.fs.Path)", "public static void initTableSnapshotMapperJob(java.lang.String, org.apache.hadoop.hbase.client.Scan, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job)", "public static void initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean)", "public static void initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, boolean)", "public static void initTableMapperJob(java.util.List<org.apache.hadoop.hbase.client.Scan>, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableMapper>, java.lang.Class<?>, java.lang.Class<?>, org.apache.hadoop.mapreduce.Job, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initCredentials(org.apache.hadoop.mapreduce.Job)", "public static void initCredentials(org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initCredentialsForCluster(org.apache.hadoop.mapreduce.Job, java.lang.String)", "public static void initCredentialsForCluster(org.apache.hadoop.mapreduce.Job, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job)", "public static void initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class)", "public static void initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String)", "public static void initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String, boolean)", "public static void initTableReducerJob(java.lang.String, java.lang.Class<? extends org.apache.hadoop.hbase.mapreduce.TableReducer>, org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.limitNumReduceTasks(java.lang.String, org.apache.hadoop.mapreduce.Job)", "public static void limitNumReduceTasks(java.lang.String, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.setNumReduceTasks(java.lang.String, org.apache.hadoop.mapreduce.Job)", "public static void setNumReduceTasks(java.lang.String, org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.setScannerCaching(org.apache.hadoop.mapreduce.Job, int)", "public static void setScannerCaching(org.apache.hadoop.mapreduce.Job, int)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addHBaseDependencyJars(org.apache.hadoop.conf.Configuration)", "public static void addHBaseDependencyJars(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.buildDependencyClasspath(org.apache.hadoop.conf.Configuration)", "public static java.lang.String buildDependencyClasspath(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(org.apache.hadoop.mapreduce.Job)", "public static void addDependencyJars(org.apache.hadoop.mapreduce.Job) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(org.apache.hadoop.conf.Configuration, java.lang.Class<?>...)", "public static void addDependencyJars(org.apache.hadoop.conf.Configuration, java.lang.Class<?>...) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableMapper", "org.apache.hadoop.hbase.mapreduce.TableMapper()", "public org.apache.hadoop.hbase.mapreduce.TableMapper()"], ["org.apache.hadoop.hbase.mapreduce.TableOutputCommitter", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter()", "public org.apache.hadoop.hbase.mapreduce.TableOutputCommitter()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.cleanupJob(org.apache.hadoop.mapreduce.JobContext)", "public void cleanupJob(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public boolean needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.setupJob(org.apache.hadoop.mapreduce.JobContext)", "public void setupJob(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.isRecoverySupported()", "public boolean isRecoverySupported()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter(org.apache.hadoop.hbase.mapreduce.TableOutputFormat)", "public org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter(org.apache.hadoop.hbase.mapreduce.TableOutputFormat) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.close(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void close(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(KEY, org.apache.hadoop.hbase.client.Mutation)", "public void write(KEY, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(java.lang.Object, java.lang.Object)", "public void write(java.lang.Object, java.lang.Object) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.TableOutputFormat", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat()", "public org.apache.hadoop.hbase.mapreduce.TableOutputFormat()"], ["org.apache.hadoop.mapreduce.RecordWriter<KEY, org.apache.hadoop.hbase.client.Mutation>", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat.getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordWriter<KEY, org.apache.hadoop.hbase.client.Mutation> getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat.checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)", "public void checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.OutputCommitter", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat.getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.OutputCommitter getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableOutputFormat.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.mapreduce.TableRecordReader", "org.apache.hadoop.hbase.mapreduce.TableRecordReader()", "public org.apache.hadoop.hbase.mapreduce.TableRecordReader()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.restart(byte[])", "public void restart(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.setHTable(org.apache.hadoop.hbase.client.Table)", "public void setHTable(org.apache.hadoop.hbase.client.Table)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.setTable(org.apache.hadoop.hbase.client.Table)", "public void setTable(org.apache.hadoop.hbase.client.Table)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.setScan(org.apache.hadoop.hbase.client.Scan)", "public void setScan(org.apache.hadoop.hbase.client.Scan)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.close()", "public void close()"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["float", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.getProgress()", "public float getProgress()"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl()", "public org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.restart(byte[])", "public void restart(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.setHTable(org.apache.hadoop.hbase.client.Table)", "public void setHTable(org.apache.hadoop.hbase.client.Table)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.setScan(org.apache.hadoop.hbase.client.Scan)", "public void setScan(org.apache.hadoop.hbase.client.Scan)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.close()", "public void close()"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["float", "org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.getProgress()", "public float getProgress()"], ["org.apache.hadoop.hbase.mapreduce.TableReducer", "org.apache.hadoop.hbase.mapreduce.TableReducer()", "public org.apache.hadoop.hbase.mapreduce.TableReducer()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["float", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.getProgress()", "public float getProgress() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.close()", "public void close() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit()", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit()"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit)", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit)"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)"], ["long", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit.getLength()", "public long getLength() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit.getLocations()", "public java.lang.String[] getLocations() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat()", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat()"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result>", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat.setInput(org.apache.hadoop.mapreduce.Job, java.lang.String, org.apache.hadoop.fs.Path)", "public static void setInput(org.apache.hadoop.mapreduce.Job, java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit()", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit()"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, java.util.List<java.lang.String>)"], ["long", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.getLength()", "public long getLength()"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.getLocations()", "public java.lang.String[] getLocations()"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.getTableDescriptor()", "public org.apache.hadoop.hbase.HTableDescriptor getTableDescriptor()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader()", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader()"], ["org.apache.hadoop.hbase.client.ClientSideRegionScanner", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.getScanner()", "public org.apache.hadoop.hbase.client.ClientSideRegionScanner getScanner()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.initialize(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit, org.apache.hadoop.conf.Configuration)", "public void initialize(org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException"], ["org.apache.hadoop.hbase.io.ImmutableBytesWritable", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.io.ImmutableBytesWritable getCurrentKey()"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.getCurrentValue()", "public org.apache.hadoop.hbase.client.Result getCurrentValue()"], ["long", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.getPos()", "public long getPos()"], ["float", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.getProgress()", "public float getProgress()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.close()", "public void close()"], ["org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl()", "public org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl()"], ["java.util.List<org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit>", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl.getSplits(org.apache.hadoop.conf.Configuration)", "public static java.util.List<org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit> getSplits(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl.getBestLocations(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HDFSBlocksDistribution)", "public static java.util.List<java.lang.String> getBestLocations(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HDFSBlocksDistribution)"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl.setInput(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.fs.Path)", "public static void setInput(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.TableSplit$Version[]", "org.apache.hadoop.hbase.mapreduce.TableSplit$Version.values()", "public static org.apache.hadoop.hbase.mapreduce.TableSplit$Version[] values()"], ["org.apache.hadoop.hbase.mapreduce.TableSplit$Version", "org.apache.hadoop.hbase.mapreduce.TableSplit$Version.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.mapreduce.TableSplit$Version valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit()", "public org.apache.hadoop.hbase.mapreduce.TableSplit()"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(byte[], org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(byte[], org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String, long)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.Scan, byte[], byte[], java.lang.String, long)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(byte[], byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(byte[], byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.TableSplit", "org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String, long)", "public org.apache.hadoop.hbase.mapreduce.TableSplit(org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String, long)"], ["org.apache.hadoop.hbase.client.Scan", "org.apache.hadoop.hbase.mapreduce.TableSplit.getScan()", "public org.apache.hadoop.hbase.client.Scan getScan() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.TableSplit.getTableName()", "public byte[] getTableName()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.mapreduce.TableSplit.getTable()", "public org.apache.hadoop.hbase.TableName getTable()"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.TableSplit.getStartRow()", "public byte[] getStartRow()"], ["byte[]", "org.apache.hadoop.hbase.mapreduce.TableSplit.getEndRow()", "public byte[] getEndRow()"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.TableSplit.getRegionLocation()", "public java.lang.String getRegionLocation()"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapreduce.TableSplit.getLocations()", "public java.lang.String[] getLocations()"], ["long", "org.apache.hadoop.hbase.mapreduce.TableSplit.getLength()", "public long getLength()"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TableSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.TableSplit.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.mapreduce.TableSplit.compareTo(org.apache.hadoop.hbase.mapreduce.TableSplit)", "public int compareTo(org.apache.hadoop.hbase.mapreduce.TableSplit)"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TableSplit.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.mapreduce.TableSplit.hashCode()", "public int hashCode()"], ["int", "org.apache.hadoop.hbase.mapreduce.TableSplit.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.mapreduce.TextSortReducer", "org.apache.hadoop.hbase.mapreduce.TextSortReducer()", "public org.apache.hadoop.hbase.mapreduce.TextSortReducer()"], ["long", "org.apache.hadoop.hbase.mapreduce.TextSortReducer.getTs()", "public long getTs()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TextSortReducer.getSkipBadLines()", "public boolean getSkipBadLines()"], ["org.apache.hadoop.mapreduce.Counter", "org.apache.hadoop.hbase.mapreduce.TextSortReducer.getBadLineCount()", "public org.apache.hadoop.mapreduce.Counter getBadLineCount()"], ["void", "org.apache.hadoop.hbase.mapreduce.TextSortReducer.incrementBadLineCount(int)", "public void incrementBadLineCount(int)"], ["org.apache.hadoop.hbase.mapreduce.TsvImporterMapper", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper()", "public org.apache.hadoop.hbase.mapreduce.TsvImporterMapper()"], ["long", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.getTs()", "public long getTs()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.getSkipBadLines()", "public boolean getSkipBadLines()"], ["org.apache.hadoop.mapreduce.Counter", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.getBadLineCount()", "public org.apache.hadoop.mapreduce.Counter getBadLineCount()"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.incrementBadLineCount(int)", "public void incrementBadLineCount(int)"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>.Context)", "public void map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper()", "public org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper()"], ["boolean", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper.getSkipBadLines()", "public boolean getSkipBadLines()"], ["org.apache.hadoop.mapreduce.Counter", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper.getBadLineCount()", "public org.apache.hadoop.mapreduce.Counter getBadLineCount()"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper.incrementBadLineCount(int)", "public void incrementBadLineCount(int)"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper.map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.io.Text>.Context)", "public void map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.io.Text>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALKeyRecordReader.getCurrentKey()", "public org.apache.hadoop.hbase.wal.WALKey getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALKeyRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.nextKeyValue()", "public boolean nextKeyValue() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.getCurrentValue()", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["float", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.getProgress()", "public float getProgress() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.close()", "public void close() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit()", "public org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit()"], ["org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit(java.lang.String, long, long, long)", "public org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit(java.lang.String, long, long, long)"], ["long", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.getLength()", "public long getLength() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.String[]", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.getLocations()", "public java.lang.String[] getLocations() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.getLogFileName()", "public java.lang.String getLogFileName()"], ["long", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.getStartTime()", "public long getStartTime()"], ["long", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.getEndTime()", "public long getEndTime()"], ["void", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.mapreduce.WALInputFormat", "org.apache.hadoop.hbase.mapreduce.WALInputFormat()", "public org.apache.hadoop.hbase.mapreduce.WALInputFormat()"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.mapreduce.WALInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit>", "org.apache.hadoop.hbase.mapreduce.WALInputFormat.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper.map(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context)", "public void map(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALMapper.map(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context)", "public void map(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALMapper.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Mutation>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer$WALMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.WALPlayer", "org.apache.hadoop.hbase.mapreduce.WALPlayer(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.mapreduce.WALPlayer(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.WALPlayer.createSubmittableJob(java.lang.String[])", "public org.apache.hadoop.mapreduce.Job createSubmittableJob(java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.WALPlayer.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.mapreduce.WALPlayer.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$1.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$1.isAborted()", "public boolean isAborted()"], ["java.lang.Void", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$1.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Void connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$1.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Object connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters[]", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters.values()", "public static org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters[] values()"], ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier()", "public org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier()"], ["void", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier.map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>.Context)", "public void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Put>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication()", "public org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication()"], ["org.apache.hadoop.mapreduce.Job", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[])", "public static org.apache.hadoop.mapreduce.Job createSubmittableJob(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.ActiveMasterManager.setInfoPort(int)", "public void setInfoPort(int)"], ["void", "org.apache.hadoop.hbase.master.ActiveMasterManager.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.ActiveMasterManager.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.ActiveMasterManager.stop()", "public void stop()"], ["org.apache.hadoop.hbase.master.AssignCallable", "org.apache.hadoop.hbase.master.AssignCallable(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public org.apache.hadoop.hbase.master.AssignCallable(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, boolean)"], ["java.lang.Object", "org.apache.hadoop.hbase.master.AssignCallable.call()", "public java.lang.Object call() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$1.process()", "public void process() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.AssignmentManager$10.call()", "public java.lang.Object call()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$2.process()", "public void process() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$3.run()", "public void run()"], ["java.lang.String", "org.apache.hadoop.hbase.master.AssignmentManager$4.getRegionName()", "public java.lang.String getRegionName()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$4.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$5.run()", "public void run()"], ["java.lang.String", "org.apache.hadoop.hbase.master.AssignmentManager$6.getRegionName()", "public java.lang.String getRegionName()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$6.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$7.process()", "public void process() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager$8.process()", "public void process() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.AssignmentManager$9.call()", "public java.lang.Object call()"], ["org.apache.hadoop.hbase.master.AssignmentManager", "org.apache.hadoop.hbase.master.AssignmentManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.ServerManager, org.apache.hadoop.hbase.master.LoadBalancer, org.apache.hadoop.hbase.executor.ExecutorService, org.apache.hadoop.hbase.master.MetricsMaster, org.apache.hadoop.hbase.master.TableLockManager)", "public org.apache.hadoop.hbase.master.AssignmentManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.ServerManager, org.apache.hadoop.hbase.master.LoadBalancer, org.apache.hadoop.hbase.executor.ExecutorService, org.apache.hadoop.hbase.master.MetricsMaster, org.apache.hadoop.hbase.master.TableLockManager) throws org.apache.zookeeper.KeeperException, java.io.IOException, org.apache.hadoop.hbase.CoordinatedStateException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.registerListener(org.apache.hadoop.hbase.master.AssignmentListener)", "public void registerListener(org.apache.hadoop.hbase.master.AssignmentListener)"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.unregisterListener(org.apache.hadoop.hbase.master.AssignmentListener)", "public boolean unregisterListener(org.apache.hadoop.hbase.master.AssignmentListener)"], ["org.apache.hadoop.hbase.TableStateManager", "org.apache.hadoop.hbase.master.AssignmentManager.getTableStateManager()", "public org.apache.hadoop.hbase.TableStateManager getTableStateManager()"], ["org.apache.hadoop.hbase.master.RegionStates", "org.apache.hadoop.hbase.master.AssignmentManager.getRegionStates()", "public org.apache.hadoop.hbase.master.RegionStates getRegionStates()"], ["org.apache.hadoop.hbase.master.RegionPlan", "org.apache.hadoop.hbase.master.AssignmentManager.getRegionReopenPlan(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.master.RegionPlan getRegionReopenPlan(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.addPlan(java.lang.String, org.apache.hadoop.hbase.master.RegionPlan)", "public void addPlan(java.lang.String, org.apache.hadoop.hbase.master.RegionPlan)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.addPlans(java.util.Map<java.lang.String, org.apache.hadoop.hbase.master.RegionPlan>)", "public void addPlans(java.util.Map<java.lang.String, org.apache.hadoop.hbase.master.RegionPlan>)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.setRegionsToReopen(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void setRegionsToReopen(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)"], ["org.apache.hadoop.hbase.util.Pair<java.lang.Integer, java.lang.Integer>", "org.apache.hadoop.hbase.master.AssignmentManager.getReopenStatus(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.util.Pair<java.lang.Integer, java.lang.Integer> getReopenStatus(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.isFailoverCleanupDone()", "public boolean isFailoverCleanupDone()"], ["java.util.concurrent.locks.Lock", "org.apache.hadoop.hbase.master.AssignmentManager.acquireRegionLock(java.lang.String)", "public java.util.concurrent.locks.Lock acquireRegionLock(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.removeClosedRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void removeClosedRegion(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.regionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public void regionOffline(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.offlineDisabledRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void offlineDisabledRegion(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.assign(org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void assign(org.apache.hadoop.hbase.HRegionInfo, boolean)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.assign(org.apache.hadoop.hbase.HRegionInfo, boolean, boolean)", "public void assign(org.apache.hadoop.hbase.HRegionInfo, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.unassign(org.apache.hadoop.hbase.HRegionInfo)", "public void unassign(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.unassign(org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.ServerName)", "public void unassign(org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.unassign(org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void unassign(org.apache.hadoop.hbase.HRegionInfo, boolean)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.deleteClosingOrClosedNode(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)", "public void deleteClosingOrClosedNode(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)"], ["int", "org.apache.hadoop.hbase.master.AssignmentManager.getNumRegionsOpened()", "public int getNumRegionsOpened()"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.waitForAssignment(org.apache.hadoop.hbase.HRegionInfo)", "public boolean waitForAssignment(org.apache.hadoop.hbase.HRegionInfo) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.assignMeta(org.apache.hadoop.hbase.HRegionInfo)", "public void assignMeta(org.apache.hadoop.hbase.HRegionInfo) throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.assign(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>)", "public void assign(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.assign(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void assign(java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException, java.lang.InterruptedException"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.AssignmentManager.replicaRegionsNotRecordedInMeta(java.util.Set<org.apache.hadoop.hbase.HRegionInfo>, org.apache.hadoop.hbase.master.MasterServices)", "public static java.util.List<org.apache.hadoop.hbase.HRegionInfo> replicaRegionsNotRecordedInMeta(java.util.Set<org.apache.hadoop.hbase.HRegionInfo>, org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.updateRegionsInTransitionMetrics()", "public void updateRegionsInTransitionMetrics()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo)", "public void waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo, long)", "public boolean waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo, long) throws java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.isCarryingMeta(org.apache.hadoop.hbase.ServerName)", "public boolean isCarryingMeta(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.isCarryingMetaReplica(org.apache.hadoop.hbase.ServerName, int)", "public boolean isCarryingMetaReplica(org.apache.hadoop.hbase.ServerName, int)"], ["boolean", "org.apache.hadoop.hbase.master.AssignmentManager.isCarryingMetaReplica(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo)", "public boolean isCarryingMetaReplica(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo)"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.AssignmentManager.processServerShutdown(org.apache.hadoop.hbase.ServerName)", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> processServerShutdown(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.balance(org.apache.hadoop.hbase.master.RegionPlan)", "public void balance(org.apache.hadoop.hbase.master.RegionPlan)"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.stop()", "public void stop()"], ["void", "org.apache.hadoop.hbase.master.AssignmentManager.shutdown()", "public void shutdown()"], ["java.util.Set<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.AssignmentManager.getReplicasToClose()", "public java.util.Set<org.apache.hadoop.hbase.HRegionInfo> getReplicasToClose()"], ["org.apache.hadoop.hbase.master.LoadBalancer", "org.apache.hadoop.hbase.master.AssignmentManager.getBalancer()", "public org.apache.hadoop.hbase.master.LoadBalancer getBalancer()"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.AssignmentManager.getSnapShotOfAssignment(java.util.Collection<org.apache.hadoop.hbase.HRegionInfo>)", "public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> getSnapShotOfAssignment(java.util.Collection<org.apache.hadoop.hbase.HRegionInfo>)"], ["org.apache.hadoop.hbase.master.AssignmentVerificationReport", "org.apache.hadoop.hbase.master.AssignmentVerificationReport()", "public org.apache.hadoop.hbase.master.AssignmentVerificationReport()"], ["void", "org.apache.hadoop.hbase.master.AssignmentVerificationReport.fillUp(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>)", "public void fillUp(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>)"], ["void", "org.apache.hadoop.hbase.master.AssignmentVerificationReport.fillUpDispersion(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public void fillUpDispersion(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)"], ["java.util.List<java.lang.Float>", "org.apache.hadoop.hbase.master.AssignmentVerificationReport.getDispersionInformation()", "public java.util.List<java.lang.Float> getDispersionInformation()"], ["void", "org.apache.hadoop.hbase.master.AssignmentVerificationReport.print(boolean)", "public void print(boolean)"], ["void", "org.apache.hadoop.hbase.master.BulkAssigner$1.uncaughtException(java.lang.Thread, java.lang.Throwable)", "public void uncaughtException(java.lang.Thread, java.lang.Throwable)"], ["org.apache.hadoop.hbase.master.BulkAssigner", "org.apache.hadoop.hbase.master.BulkAssigner(org.apache.hadoop.hbase.Server)", "public org.apache.hadoop.hbase.master.BulkAssigner(org.apache.hadoop.hbase.Server)"], ["boolean", "org.apache.hadoop.hbase.master.BulkAssigner.bulkAssign()", "public boolean bulkAssign() throws java.lang.InterruptedException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.BulkAssigner.bulkAssign(boolean)", "public boolean bulkAssign(boolean) throws java.lang.InterruptedException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.BulkReOpen$1.run()", "public void run()"], ["org.apache.hadoop.hbase.master.BulkReOpen", "org.apache.hadoop.hbase.master.BulkReOpen(org.apache.hadoop.hbase.Server, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>, org.apache.hadoop.hbase.master.AssignmentManager)", "public org.apache.hadoop.hbase.master.BulkReOpen(org.apache.hadoop.hbase.Server, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>, org.apache.hadoop.hbase.master.AssignmentManager)"], ["boolean", "org.apache.hadoop.hbase.master.BulkReOpen.bulkReOpen()", "public boolean bulkReOpen() throws java.lang.InterruptedException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.CatalogJanitor$1.processRow(org.apache.hadoop.hbase.client.Result)", "public boolean processRow(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.master.CatalogJanitor$SplitParentFirstComparator.compare(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public int compare(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)"], ["int", "org.apache.hadoop.hbase.master.CatalogJanitor$SplitParentFirstComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.master.CatalogJanitor.setEnabled(boolean)", "public boolean setEnabled(boolean)"], ["boolean", "org.apache.hadoop.hbase.master.CatalogJanitor.cleanMergeQualifier(org.apache.hadoop.hbase.HRegionInfo)", "public boolean cleanMergeQualifier(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$1.compare(java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>, java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>)", "public int compare(java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>, java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>)"], ["int", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["T", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher$HBaseDatagramChannelFactory.newChannel()", "public T newChannel()"], ["java.lang.String", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher$HBaseDatagramChannelFactory.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher()", "public org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher()"], ["void", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher.connect(org.apache.hadoop.conf.Configuration)", "public void connect(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher.publish(org.apache.hadoop.hbase.ClusterStatus)", "public void publish(org.apache.hadoop.hbase.ClusterStatus)"], ["void", "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher.close()", "public void close()"], ["org.apache.hadoop.hbase.master.ClusterStatusPublisher", "org.apache.hadoop.hbase.master.ClusterStatusPublisher(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher>)", "public org.apache.hadoop.hbase.master.ClusterStatusPublisher(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher>) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.master.DeadServer$1.compare(org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>)", "public int compare(org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>)"], ["int", "org.apache.hadoop.hbase.master.DeadServer$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.master.DeadServer", "org.apache.hadoop.hbase.master.DeadServer()", "public org.apache.hadoop.hbase.master.DeadServer()"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.boolean cleanPreviousInstance(org.apache.hadoop.hbase.ServerName)", "public synchronized boolean cleanPreviousInstance(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.boolean isDeadServer(org.apache.hadoop.hbase.ServerName)", "public synchronized boolean isDeadServer(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.boolean areDeadServersInProgress()", "public synchronized boolean areDeadServersInProgress()"], ["java.util.Set<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.DeadServer.copyServerNames()", "public synchronized java.util.Set<org.apache.hadoop.hbase.ServerName> copyServerNames()"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.void add(org.apache.hadoop.hbase.ServerName)", "public synchronized void add(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.void finish(org.apache.hadoop.hbase.ServerName)", "public synchronized void finish(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.int size()", "public synchronized int size()"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.boolean isEmpty()", "public synchronized boolean isEmpty()"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.void cleanAllPreviousInstances(org.apache.hadoop.hbase.ServerName)", "public synchronized void cleanAllPreviousInstances(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>>", "org.apache.hadoop.hbase.master.DeadServer.copyDeadServersSince(long)", "public synchronized java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.lang.Long>> copyDeadServersSince(long)"], ["synchronized", "org.apache.hadoop.hbase.master.DeadServer.java.util.Date getTimeOfDeath(org.apache.hadoop.hbase.ServerName)", "public synchronized java.util.Date getTimeOfDeath(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.GeneralBulkAssigner$1.uncaughtException(java.lang.Thread, java.lang.Throwable)", "public void uncaughtException(java.lang.Thread, java.lang.Throwable)"], ["void", "org.apache.hadoop.hbase.master.GeneralBulkAssigner$SingleServerBulkAssigner.run()", "public void run()"], ["org.apache.hadoop.hbase.master.GeneralBulkAssigner", "org.apache.hadoop.hbase.master.GeneralBulkAssigner(org.apache.hadoop.hbase.Server, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>, org.apache.hadoop.hbase.master.AssignmentManager, boolean)", "public org.apache.hadoop.hbase.master.GeneralBulkAssigner(org.apache.hadoop.hbase.Server, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>, org.apache.hadoop.hbase.master.AssignmentManager, boolean)"], ["void", "org.apache.hadoop.hbase.master.HMaster$1.run()", "public void run()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster$2.processRow(org.apache.hadoop.hbase.client.Result)", "public boolean processRow(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.master.HMaster$3.compare(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public int compare(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)"], ["int", "org.apache.hadoop.hbase.master.HMaster$3.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.master.HMaster$InitializationMonitor.run()", "public void run()"], ["org.apache.hadoop.hbase.master.HMaster$ProcedureConf[]", "org.apache.hadoop.hbase.master.HMaster$ProcedureConf.values()", "public static org.apache.hadoop.hbase.master.HMaster$ProcedureConf[] values()"], ["org.apache.hadoop.hbase.master.HMaster$ProcedureConf", "org.apache.hadoop.hbase.master.HMaster$ProcedureConf.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.HMaster$ProcedureConf valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.master.HMaster$RedirectServlet", "org.apache.hadoop.hbase.master.HMaster$RedirectServlet()", "public org.apache.hadoop.hbase.master.HMaster$RedirectServlet()"], ["void", "org.apache.hadoop.hbase.master.HMaster$RedirectServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.master.HMaster(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)", "public org.apache.hadoop.hbase.master.HMaster(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.master.MasterRpcServices", "org.apache.hadoop.hbase.master.HMaster.getMasterRpcServices()", "public org.apache.hadoop.hbase.master.MasterRpcServices getMasterRpcServices()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.balanceSwitch(boolean)", "public boolean balanceSwitch(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.TableDescriptors", "org.apache.hadoop.hbase.master.HMaster.getTableDescriptors()", "public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors()"], ["org.apache.hadoop.hbase.master.ServerManager", "org.apache.hadoop.hbase.master.HMaster.getServerManager()", "public org.apache.hadoop.hbase.master.ServerManager getServerManager()"], ["org.apache.hadoop.hbase.master.MasterFileSystem", "org.apache.hadoop.hbase.master.HMaster.getMasterFileSystem()", "public org.apache.hadoop.hbase.master.MasterFileSystem getMasterFileSystem()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isMasterProcedureExecutorEnabled()", "public boolean isMasterProcedureExecutorEnabled()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.balance()", "public boolean balance() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.setCatalogJanitorEnabled(boolean)", "public void setCatalogJanitorEnabled(boolean)"], ["void", "org.apache.hadoop.hbase.master.HMaster.dispatchMergingRegions(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void dispatchMergingRegions(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.createTable(org.apache.hadoop.hbase.HTableDescriptor, byte[][])", "public long createTable(org.apache.hadoop.hbase.HTableDescriptor, byte[][]) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.deleteTable(org.apache.hadoop.hbase.TableName)", "public long deleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.truncateTable(org.apache.hadoop.hbase.TableName, boolean)", "public void truncateTable(org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.deleteColumn(org.apache.hadoop.hbase.TableName, byte[])", "public void deleteColumn(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.enableTable(org.apache.hadoop.hbase.TableName)", "public long enableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.disableTable(org.apache.hadoop.hbase.TableName)", "public long disableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.modifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void modifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.checkTableModifiable(org.apache.hadoop.hbase.TableName)", "public void checkTableModifiable(org.apache.hadoop.hbase.TableName) throws java.io.IOException, org.apache.hadoop.hbase.TableNotFoundException, org.apache.hadoop.hbase.TableNotDisabledException"], ["org.apache.hadoop.hbase.ClusterStatus", "org.apache.hadoop.hbase.master.HMaster.getClusterStatus()", "public org.apache.hadoop.hbase.ClusterStatus getClusterStatus() throws java.io.InterruptedIOException"], ["java.lang.String", "org.apache.hadoop.hbase.master.HMaster.getLoadedCoprocessors()", "public static java.lang.String getLoadedCoprocessors()"], ["long", "org.apache.hadoop.hbase.master.HMaster.getMasterStartTime()", "public long getMasterStartTime()"], ["long", "org.apache.hadoop.hbase.master.HMaster.getMasterActiveTime()", "public long getMasterActiveTime()"], ["int", "org.apache.hadoop.hbase.master.HMaster.getRegionServerInfoPort(org.apache.hadoop.hbase.ServerName)", "public int getRegionServerInfoPort(org.apache.hadoop.hbase.ServerName)"], ["java.lang.String[]", "org.apache.hadoop.hbase.master.HMaster.getMasterCoprocessors()", "public java.lang.String[] getMasterCoprocessors()"], ["void", "org.apache.hadoop.hbase.master.HMaster.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher", "org.apache.hadoop.hbase.master.HMaster.getZooKeeper()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher getZooKeeper()"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost", "org.apache.hadoop.hbase.master.HMaster.getMasterCoprocessorHost()", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost getMasterCoprocessorHost()"], ["org.apache.hadoop.hbase.quotas.MasterQuotaManager", "org.apache.hadoop.hbase.master.HMaster.getMasterQuotaManager()", "public org.apache.hadoop.hbase.quotas.MasterQuotaManager getMasterQuotaManager()"], ["org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>", "org.apache.hadoop.hbase.master.HMaster.getMasterProcedureExecutor()", "public org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> getMasterProcedureExecutor()"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.master.HMaster.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["org.apache.hadoop.hbase.master.AssignmentManager", "org.apache.hadoop.hbase.master.HMaster.getAssignmentManager()", "public org.apache.hadoop.hbase.master.AssignmentManager getAssignmentManager()"], ["org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer", "org.apache.hadoop.hbase.master.HMaster.getRegionServerFatalLogBuffer()", "public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer()"], ["void", "org.apache.hadoop.hbase.master.HMaster.shutdown()", "public void shutdown()"], ["void", "org.apache.hadoop.hbase.master.HMaster.stopMaster()", "public void stopMaster()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isActiveMaster()", "public boolean isActiveMaster()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isInitialized()", "public boolean isInitialized()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isServerShutdownHandlerEnabled()", "public boolean isServerShutdownHandlerEnabled()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isInitializationStartsMetaRegionAssignment()", "public boolean isInitializationStartsMetaRegionAssignment()"], ["void", "org.apache.hadoop.hbase.master.HMaster.assignRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void assignRegion(org.apache.hadoop.hbase.HRegionInfo)"], ["double", "org.apache.hadoop.hbase.master.HMaster.getAverageLoad()", "public double getAverageLoad()"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.registerService(com.google.protobuf.Service)", "public boolean registerService(com.google.protobuf.Service)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.master.HMaster.constructMaster(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)", "public static org.apache.hadoop.hbase.master.HMaster constructMaster(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)"], ["void", "org.apache.hadoop.hbase.master.HMaster.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["org.apache.hadoop.hbase.master.cleaner.HFileCleaner", "org.apache.hadoop.hbase.master.HMaster.getHFileCleaner()", "public org.apache.hadoop.hbase.master.cleaner.HFileCleaner getHFileCleaner()"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotManager", "org.apache.hadoop.hbase.master.HMaster.getSnapshotManagerForTesting()", "public org.apache.hadoop.hbase.master.snapshot.SnapshotManager getSnapshotManagerForTesting()"], ["void", "org.apache.hadoop.hbase.master.HMaster.createNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void createNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.modifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void modifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.HMaster.deleteNamespace(java.lang.String)", "public void deleteNamespace(java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.NamespaceDescriptor", "org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(java.lang.String)", "public org.apache.hadoop.hbase.NamespaceDescriptor getNamespaceDescriptor(java.lang.String) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>", "org.apache.hadoop.hbase.master.HMaster.listNamespaceDescriptors()", "public java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor> listNamespaceDescriptors() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HTableDescriptor>", "org.apache.hadoop.hbase.master.HMaster.listTableDescriptorsByNamespace(java.lang.String)", "public java.util.List<org.apache.hadoop.hbase.HTableDescriptor> listTableDescriptorsByNamespace(java.lang.String) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.TableName>", "org.apache.hadoop.hbase.master.HMaster.listTableNamesByNamespace(java.lang.String)", "public java.util.List<org.apache.hadoop.hbase.TableName> listTableNamesByNamespace(java.lang.String) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HTableDescriptor>", "org.apache.hadoop.hbase.master.HMaster.listTableDescriptors(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.TableName>, boolean)", "public java.util.List<org.apache.hadoop.hbase.HTableDescriptor> listTableDescriptors(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.TableName>, boolean) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.TableName>", "org.apache.hadoop.hbase.master.HMaster.listTableNames(java.lang.String, java.lang.String, boolean)", "public java.util.List<org.apache.hadoop.hbase.TableName> listTableNames(java.lang.String, java.lang.String, boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.getLastMajorCompactionTimestamp(org.apache.hadoop.hbase.TableName)", "public long getLastMajorCompactionTimestamp(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.HMaster.getLastMajorCompactionTimestampForRegion(byte[])", "public long getLastMajorCompactionTimestampForRegion(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.HMaster.isBalancerOn()", "public boolean isBalancerOn()"], ["java.lang.String", "org.apache.hadoop.hbase.master.HMaster.getLoadBalancerClassName()", "public java.lang.String getLoadBalancerClassName()"], ["org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster", "org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)", "public org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run()", "public void run()"], ["org.apache.hadoop.hbase.master.HMasterCommandLine", "org.apache.hadoop.hbase.master.HMasterCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>)", "public org.apache.hadoop.hbase.master.HMasterCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>)"], ["int", "org.apache.hadoop.hbase.master.HMasterCommandLine.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$1.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$10.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$11.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$12.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$13.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$14.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$15.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$16.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$17.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$18.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$19.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$2.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$20.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$21.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$22.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$23.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$24.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$25.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$26.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$27.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$28.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$29.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$3.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$30.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$31.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$32.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$33.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$34.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$35.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$36.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$37.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$38.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$39.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$4.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$40.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$41.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$42.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$43.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$44.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$45.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$46.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$47.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$48.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$49.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$5.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$50.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$51.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$52.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$53.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$54.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$55.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$56.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$57.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$58.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$59.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$59.postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$6.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$60.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$60.postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$61.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$62.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$63.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$64.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$65.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$66.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$67.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$68.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$69.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$7.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$70.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$71.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$72.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$73.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$74.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$75.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$76.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$77.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$78.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$79.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$8.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$80.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$81.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$82.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$83.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$84.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$85.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$86.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$87.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$88.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$89.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$9.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$90.call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperation", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperation()", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperation()"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperation.postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment)"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperationWithResult.setResult(T)", "public void setResult(T)"], ["T", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$CoprocessorOperationWithResult.getResult()", "public T getResult()"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.master.MasterServices", "org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment.getMasterServices()", "public org.apache.hadoop.hbase.master.MasterServices getMasterServices()"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost", "org.apache.hadoop.hbase.master.MasterCoprocessorHost(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public boolean preCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteNamespace(java.lang.String)", "public boolean preDeleteNamespace(java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteNamespace(java.lang.String)", "public void postDeleteNamespace(java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public boolean preModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preGetNamespaceDescriptor(java.lang.String)", "public void preGetNamespaceDescriptor(java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postGetNamespaceDescriptor(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void postGetNamespaceDescriptor(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public boolean preListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void postListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTable(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTable(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateTable(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTable(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTableHandler(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTableHandler(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateTableHandler(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTableHandler(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteTable(org.apache.hadoop.hbase.TableName)", "public void preDeleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteTable(org.apache.hadoop.hbase.TableName)", "public void postDeleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteTableHandler(org.apache.hadoop.hbase.TableName)", "public void preDeleteTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteTableHandler(org.apache.hadoop.hbase.TableName)", "public void postDeleteTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preTruncateTable(org.apache.hadoop.hbase.TableName)", "public void preTruncateTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postTruncateTable(org.apache.hadoop.hbase.TableName)", "public void postTruncateTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preTruncateTableHandler(org.apache.hadoop.hbase.TableName)", "public void preTruncateTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postTruncateTableHandler(org.apache.hadoop.hbase.TableName)", "public void postTruncateTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preAddColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public boolean preAddColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postAddColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preAddColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public boolean preAddColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postAddColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postAddColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preModifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public boolean preModifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postModifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preModifyColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public boolean preModifyColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postModifyColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void postModifyColumnHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteColumn(org.apache.hadoop.hbase.TableName, byte[])", "public boolean preDeleteColumn(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteColumn(org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumn(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteColumnHandler(org.apache.hadoop.hbase.TableName, byte[])", "public boolean preDeleteColumnHandler(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteColumnHandler(org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumnHandler(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preEnableTable(org.apache.hadoop.hbase.TableName)", "public void preEnableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postEnableTable(org.apache.hadoop.hbase.TableName)", "public void postEnableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preEnableTableHandler(org.apache.hadoop.hbase.TableName)", "public void preEnableTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postEnableTableHandler(org.apache.hadoop.hbase.TableName)", "public void postEnableTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDisableTable(org.apache.hadoop.hbase.TableName)", "public void preDisableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDisableTable(org.apache.hadoop.hbase.TableName)", "public void postDisableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDisableTableHandler(org.apache.hadoop.hbase.TableName)", "public void preDisableTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDisableTableHandler(org.apache.hadoop.hbase.TableName)", "public void postDisableTableHandler(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preMove(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public boolean preMove(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postMove(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void postMove(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preAssign(org.apache.hadoop.hbase.HRegionInfo)", "public boolean preAssign(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postAssign(org.apache.hadoop.hbase.HRegionInfo)", "public void postAssign(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preUnassign(org.apache.hadoop.hbase.HRegionInfo, boolean)", "public boolean preUnassign(org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postUnassign(org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void postUnassign(org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preRegionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public void preRegionOffline(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postRegionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public void postRegionOffline(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preBalance()", "public boolean preBalance() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postBalance(java.util.List<org.apache.hadoop.hbase.master.RegionPlan>)", "public void postBalance(java.util.List<org.apache.hadoop.hbase.master.RegionPlan>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preBalanceSwitch(boolean)", "public boolean preBalanceSwitch(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postBalanceSwitch(boolean, boolean)", "public void postBalanceSwitch(boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preShutdown()", "public void preShutdown() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preStopMaster()", "public void preStopMaster() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preMasterInitialization()", "public void preMasterInitialization() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postStartMaster()", "public void postStartMaster() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preListSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preListSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postListSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postListSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCloneSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preCloneSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCloneSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postCloneSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preRestoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preRestoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postRestoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void postRestoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preDeleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preDeleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postDeleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void postDeleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public boolean preGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>)", "public void postGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public boolean preGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preGetTableNames(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public boolean preGetTableNames(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postGetTableNames(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableNames(java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preTableFlush(org.apache.hadoop.hbase.TableName)", "public void preTableFlush(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postTableFlush(org.apache.hadoop.hbase.TableName)", "public void postTableFlush(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.preSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.postSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.CoprocessorEnvironment", "org.apache.hadoop.hbase.master.MasterCoprocessorHost.createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.MasterDumpServlet", "org.apache.hadoop.hbase.master.MasterDumpServlet()", "public org.apache.hadoop.hbase.master.MasterDumpServlet()"], ["void", "org.apache.hadoop.hbase.master.MasterDumpServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.MasterFileSystem$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.master.MasterFileSystem$2.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.master.MasterFileSystem", "org.apache.hadoop.hbase.master.MasterFileSystem(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.MasterFileSystem(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.master.MasterFileSystem.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.master.MasterFileSystem.getOldLogDir()", "public org.apache.hadoop.fs.Path getOldLogDir()"], ["boolean", "org.apache.hadoop.hbase.master.MasterFileSystem.checkFileSystem()", "public boolean checkFileSystem()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.master.MasterFileSystem.getRootDir()", "public org.apache.hadoop.fs.Path getRootDir()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.master.MasterFileSystem.getTempDir()", "public org.apache.hadoop.fs.Path getTempDir()"], ["org.apache.hadoop.hbase.ClusterId", "org.apache.hadoop.hbase.master.MasterFileSystem.getClusterId()", "public org.apache.hadoop.hbase.ClusterId getClusterId()"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(org.apache.hadoop.hbase.ServerName)", "public void splitLog(org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(org.apache.hadoop.hbase.ServerName)", "public void splitMetaLog(org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(java.util.Set<org.apache.hadoop.hbase.ServerName>)", "public void splitMetaLog(java.util.Set<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.prepareLogReplay(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>)", "public void prepareLogReplay(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>)", "public void splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.fs.PathFilter)", "public void splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.setInfoFamilyCachingForMeta(org.apache.hadoop.hbase.HTableDescriptor, boolean)", "public static void setInfoFamilyCachingForMeta(org.apache.hadoop.hbase.HTableDescriptor, boolean)"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.deleteRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void deleteRegion(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.deleteTable(org.apache.hadoop.hbase.TableName)", "public void deleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.master.MasterFileSystem.moveTableToTemp(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.fs.Path moveTableToTemp(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.updateRegionInfo(org.apache.hadoop.hbase.HRegionInfo)", "public void updateRegionInfo(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.deleteFamilyFromFS(org.apache.hadoop.hbase.HRegionInfo, byte[])", "public void deleteFamilyFromFS(org.apache.hadoop.hbase.HRegionInfo, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.stop()", "public void stop()"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.master.MasterFileSystem.deleteColumn(org.apache.hadoop.hbase.TableName, byte[])", "public org.apache.hadoop.hbase.HTableDescriptor deleteColumn(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.master.MasterFileSystem.modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public org.apache.hadoop.hbase.HTableDescriptor modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.master.MasterFileSystem.addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public org.apache.hadoop.hbase.HTableDescriptor addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.MasterFileSystem.setLogRecoveryMode()", "public void setLogRecoveryMode() throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode", "org.apache.hadoop.hbase.master.MasterFileSystem.getLogRecoveryMode()", "public org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode getLogRecoveryMode()"], ["void", "org.apache.hadoop.hbase.master.MasterRpcServices$1.run(com.google.protobuf.Message)", "public void run(com.google.protobuf.Message)"], ["void", "org.apache.hadoop.hbase.master.MasterRpcServices$1.run(java.lang.Object)", "public void run(java.lang.Object)"], ["org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode[]", "org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode.values()", "public static org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode[] values()"], ["org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode", "org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.master.MasterRpcServices", "org.apache.hadoop.hbase.master.MasterRpcServices(org.apache.hadoop.hbase.master.HMaster)", "public org.apache.hadoop.hbase.master.MasterRpcServices(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getLastFlushedSequenceId(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdRequest)", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdResponse getLastFlushedSequenceId(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerReportResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.regionServerReport(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerReportRequest)", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerReportResponse regionServerReport(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerReportRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStartupResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.regionServerStartup(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStartupRequest)", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStartupResponse regionServerStartup(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStartupRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.reportRSFatalError(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorRequest)", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorResponse reportRSFatalError(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AddColumnResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.addColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AddColumnRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AddColumnResponse addColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AddColumnRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AssignRegionResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.assignRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AssignRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AssignRegionResponse assignRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$AssignRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$BalanceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.balance(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$BalanceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$BalanceResponse balance(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$BalanceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateNamespaceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.createNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateNamespaceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateNamespaceResponse createNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateNamespaceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.createTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateTableResponse createTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$CreateTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteColumnResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.deleteColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteColumnRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteColumnResponse deleteColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteColumnRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteNamespaceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.deleteNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteNamespaceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteNamespaceResponse deleteNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteNamespaceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteSnapshotResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.deleteSnapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteSnapshotRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteSnapshotResponse deleteSnapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteSnapshotRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.deleteTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteTableResponse deleteTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DeleteTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$TruncateTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.truncateTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$TruncateTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$TruncateTableResponse truncateTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$TruncateTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DisableTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.disableTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DisableTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DisableTableResponse disableTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DisableTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DispatchMergingRegionsResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.dispatchMergingRegions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DispatchMergingRegionsRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DispatchMergingRegionsResponse dispatchMergingRegions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$DispatchMergingRegionsRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableCatalogJanitorResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.enableCatalogJanitor(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableCatalogJanitorRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableCatalogJanitorResponse enableCatalogJanitor(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableCatalogJanitorRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.enableTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableTableResponse enableTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$EnableTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.execMasterService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse execMasterService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.execProcedure(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureResponse execProcedure(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.execProcedureWithRet(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureResponse execProcedureWithRet(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ExecProcedureRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetClusterStatusResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getClusterStatus(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetClusterStatusRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetClusterStatusResponse getClusterStatus(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetClusterStatusRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetCompletedSnapshotsResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getCompletedSnapshots(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetCompletedSnapshotsRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetCompletedSnapshotsResponse getCompletedSnapshots(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetCompletedSnapshotsRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetNamespaceDescriptorResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getNamespaceDescriptor(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetNamespaceDescriptorRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetNamespaceDescriptorResponse getNamespaceDescriptor(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetNamespaceDescriptorRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetSchemaAlterStatusResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getSchemaAlterStatus(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetSchemaAlterStatusRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetSchemaAlterStatusResponse getSchemaAlterStatus(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetSchemaAlterStatusRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableDescriptorsResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getTableDescriptors(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableDescriptorsRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableDescriptorsResponse getTableDescriptors(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableDescriptorsRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableNamesResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getTableNames(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableNamesRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableNamesResponse getTableNames(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetTableNamesRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isCatalogJanitorEnabled(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledResponse isCatalogJanitorEnabled(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsMasterRunningResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isMasterRunning(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsMasterRunningRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsMasterRunningResponse isMasterRunning(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsMasterRunningRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsProcedureDoneResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isProcedureDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsProcedureDoneRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsProcedureDoneResponse isProcedureDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsProcedureDoneRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsRestoreSnapshotDoneResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isRestoreSnapshotDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsRestoreSnapshotDoneRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsRestoreSnapshotDoneResponse isRestoreSnapshotDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsRestoreSnapshotDoneRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsSnapshotDoneResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isSnapshotDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsSnapshotDoneRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsSnapshotDoneResponse isSnapshotDone(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsSnapshotDoneRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetProcedureResultResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getProcedureResult(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetProcedureResultRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetProcedureResultResponse getProcedureResult(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$GetProcedureResultRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListNamespaceDescriptorsResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.listNamespaceDescriptors(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListNamespaceDescriptorsRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListNamespaceDescriptorsResponse listNamespaceDescriptors(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListNamespaceDescriptorsRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.listTableDescriptorsByNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceResponse listTableDescriptorsByNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableNamesByNamespaceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.listTableNamesByNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableNamesByNamespaceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableNamesByNamespaceResponse listTableNamesByNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ListTableNamesByNamespaceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyColumnResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.modifyColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyColumnRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyColumnResponse modifyColumn(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyColumnRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyNamespaceResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.modifyNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyNamespaceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyNamespaceResponse modifyNamespace(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyNamespaceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyTableResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.modifyTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyTableRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyTableResponse modifyTable(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ModifyTableRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MoveRegionResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.moveRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MoveRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MoveRegionResponse moveRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MoveRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$OfflineRegionResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.offlineRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$OfflineRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$OfflineRegionResponse offlineRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$OfflineRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RestoreSnapshotResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.restoreSnapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RestoreSnapshotRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RestoreSnapshotResponse restoreSnapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RestoreSnapshotRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RunCatalogScanResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.runCatalogScan(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RunCatalogScanRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RunCatalogScanResponse runCatalogScan(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$RunCatalogScanRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetBalancerRunningResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.setBalancerRunning(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetBalancerRunningRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetBalancerRunningResponse setBalancerRunning(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetBalancerRunningRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ShutdownResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.shutdown(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ShutdownRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ShutdownResponse shutdown(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$ShutdownRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SnapshotResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.snapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SnapshotRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SnapshotResponse snapshot(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SnapshotRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$StopMasterResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.stopMaster(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$StopMasterRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$StopMasterResponse stopMaster(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$StopMasterRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$UnassignRegionResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.unassignRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$UnassignRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$UnassignRegionResponse unassignRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$UnassignRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionStateTransition(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionResponse reportRegionStateTransition(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getLastMajorCompactionTimestamp(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse getLastMajorCompactionTimestamp(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.getLastMajorCompactionTimestampForRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampForRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse getLastMajorCompactionTimestampForRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MajorCompactionTimestampForRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsBalancerEnabledResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.isBalancerEnabled(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsBalancerEnabledRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsBalancerEnabledResponse isBalancerEnabled(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$IsBalancerEnabledRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaResponse", "org.apache.hadoop.hbase.master.MasterRpcServices.setQuota(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaResponse setQuota(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.master.MasterStatusServlet", "org.apache.hadoop.hbase.master.MasterStatusServlet()", "public org.apache.hadoop.hbase.master.MasterStatusServlet()"], ["void", "org.apache.hadoop.hbase.master.MasterStatusServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.MetricsAssignmentManager", "org.apache.hadoop.hbase.master.MetricsAssignmentManager()", "public org.apache.hadoop.hbase.master.MetricsAssignmentManager()"], ["void", "org.apache.hadoop.hbase.master.MetricsAssignmentManager.updateAssignmentTime(long)", "public void updateAssignmentTime(long)"], ["void", "org.apache.hadoop.hbase.master.MetricsAssignmentManager.updateBulkAssignTime(long)", "public void updateBulkAssignTime(long)"], ["void", "org.apache.hadoop.hbase.master.MetricsAssignmentManager.updateRITCount(int)", "public void updateRITCount(int)"], ["void", "org.apache.hadoop.hbase.master.MetricsAssignmentManager.updateRITCountOverThreshold(int)", "public void updateRITCountOverThreshold(int)"], ["void", "org.apache.hadoop.hbase.master.MetricsAssignmentManager.updateRITOldestAge(long)", "public void updateRITOldestAge(long)"], ["org.apache.hadoop.hbase.master.MetricsMaster", "org.apache.hadoop.hbase.master.MetricsMaster(org.apache.hadoop.hbase.master.MetricsMasterWrapper)", "public org.apache.hadoop.hbase.master.MetricsMaster(org.apache.hadoop.hbase.master.MetricsMasterWrapper)"], ["org.apache.hadoop.hbase.master.MetricsMasterSource", "org.apache.hadoop.hbase.master.MetricsMaster.getMetricsSource()", "public org.apache.hadoop.hbase.master.MetricsMasterSource getMetricsSource()"], ["void", "org.apache.hadoop.hbase.master.MetricsMaster.incrementRequests(long)", "public void incrementRequests(long)"], ["org.apache.hadoop.hbase.master.MetricsMasterFileSystem", "org.apache.hadoop.hbase.master.MetricsMasterFileSystem()", "public org.apache.hadoop.hbase.master.MetricsMasterFileSystem()"], ["synchronized", "org.apache.hadoop.hbase.master.MetricsMasterFileSystem.void addSplit(long, long)", "public synchronized void addSplit(long, long)"], ["synchronized", "org.apache.hadoop.hbase.master.MetricsMasterFileSystem.void addMetaWALSplit(long, long)", "public synchronized void addMetaWALSplit(long, long)"], ["org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl(org.apache.hadoop.hbase.master.HMaster)", "public org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl(org.apache.hadoop.hbase.master.HMaster)"], ["double", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getAverageLoad()", "public double getAverageLoad()"], ["java.lang.String", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getClusterId()", "public java.lang.String getClusterId()"], ["java.lang.String", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getZookeeperQuorum()", "public java.lang.String getZookeeperQuorum()"], ["java.lang.String[]", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getCoprocessors()", "public java.lang.String[] getCoprocessors()"], ["long", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getStartTime()", "public long getStartTime()"], ["long", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getActiveTime()", "public long getActiveTime()"], ["java.lang.String", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getRegionServers()", "public java.lang.String getRegionServers()"], ["int", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getNumRegionServers()", "public int getNumRegionServers()"], ["java.lang.String", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getDeadRegionServers()", "public java.lang.String getDeadRegionServers()"], ["int", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getNumDeadRegionServers()", "public int getNumDeadRegionServers()"], ["java.lang.String", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getServerName()", "public java.lang.String getServerName()"], ["boolean", "org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.getIsActiveMaster()", "public boolean getIsActiveMaster()"], ["org.apache.hadoop.hbase.master.MetricsSnapshot", "org.apache.hadoop.hbase.master.MetricsSnapshot()", "public org.apache.hadoop.hbase.master.MetricsSnapshot()"], ["void", "org.apache.hadoop.hbase.master.MetricsSnapshot.addSnapshot(long)", "public void addSnapshot(long)"], ["void", "org.apache.hadoop.hbase.master.MetricsSnapshot.addSnapshotRestore(long)", "public void addSnapshotRestore(long)"], ["void", "org.apache.hadoop.hbase.master.MetricsSnapshot.addSnapshotClone(long)", "public void addSnapshotClone(long)"], ["void", "org.apache.hadoop.hbase.master.OfflineCallback$ExistCallback.processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat)", "public void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat)"], ["void", "org.apache.hadoop.hbase.master.OfflineCallback.processResult(int, java.lang.String, java.lang.Object, java.lang.String)", "public void processResult(int, java.lang.String, java.lang.Object, java.lang.String)"], ["org.apache.hadoop.hbase.master.RackManager", "org.apache.hadoop.hbase.master.RackManager()", "public org.apache.hadoop.hbase.master.RackManager()"], ["org.apache.hadoop.hbase.master.RackManager", "org.apache.hadoop.hbase.master.RackManager(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.RackManager(org.apache.hadoop.conf.Configuration)"], ["java.lang.String", "org.apache.hadoop.hbase.master.RackManager.getRack(org.apache.hadoop.hbase.ServerName)", "public java.lang.String getRack(org.apache.hadoop.hbase.ServerName)"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.master.RackManager.getRack(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public java.util.List<java.lang.String> getRack(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix(int, int)", "public org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix(int, int)"], ["float[][]", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix.transform(float[][])", "public float[][] transform(float[][])"], ["float[][]", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix.invert(float[][])", "public float[][] invert(float[][])"], ["int[]", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix.invertIndices(int[])", "public int[] invertIndices(int[])"], ["org.apache.hadoop.hbase.master.RegionPlacementMaintainer", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.RegionPlacementMaintainer", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration, boolean, boolean)", "public org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.setTargetTableName(java.lang.String[])", "public void setTargetTableName(java.lang.String[])"], ["org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.getRegionAssignmentSnapshot()", "public org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta getRegionAssignmentSnapshot() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.master.AssignmentVerificationReport>", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.verifyRegionPlacement(boolean)", "public java.util.List<org.apache.hadoop.hbase.master.AssignmentVerificationReport> verifyRegionPlacement(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.getNewAssignmentPlan()", "public org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan getNewAssignmentPlan() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.printAssignmentPlan(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public static void printAssignmentPlan(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.updateAssignmentPlanToMeta(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public void updateAssignmentPlanToMeta(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.updateAssignmentPlan(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public void updateAssignmentPlan(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan) throws java.io.IOException"], ["java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer>", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.getRegionsMovement(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer> getRegionsMovement(org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.checkDifferencesWithOldPlan(java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer>, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan)", "public void checkDifferencesWithOldPlan(java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer>, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.printDispersionScores(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, int, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan, boolean)", "public void printDispersionScores(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, int, org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan, boolean)"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.printLocalityAndDispersionForCurrentPlan(java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>)", "public void printLocalityAndDispersionForCurrentPlan(java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.getFavoredNodeList(java.lang.String)", "public static java.util.List<org.apache.hadoop.hbase.ServerName> getFavoredNodeList(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.RegionPlacementMaintainer.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.RegionPlan$RegionPlanComparator", "org.apache.hadoop.hbase.master.RegionPlan$RegionPlanComparator()", "public org.apache.hadoop.hbase.master.RegionPlan$RegionPlanComparator()"], ["int", "org.apache.hadoop.hbase.master.RegionPlan$RegionPlanComparator.compare(org.apache.hadoop.hbase.master.RegionPlan, org.apache.hadoop.hbase.master.RegionPlan)", "public int compare(org.apache.hadoop.hbase.master.RegionPlan, org.apache.hadoop.hbase.master.RegionPlan)"], ["int", "org.apache.hadoop.hbase.master.RegionPlan$RegionPlanComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.master.RegionPlan", "org.apache.hadoop.hbase.master.RegionPlan(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.master.RegionPlan(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.RegionPlan.setDestination(org.apache.hadoop.hbase.ServerName)", "public void setDestination(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.master.RegionPlan.getSource()", "public org.apache.hadoop.hbase.ServerName getSource()"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.master.RegionPlan.getDestination()", "public org.apache.hadoop.hbase.ServerName getDestination()"], ["java.lang.String", "org.apache.hadoop.hbase.master.RegionPlan.getRegionName()", "public java.lang.String getRegionName()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.master.RegionPlan.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["int", "org.apache.hadoop.hbase.master.RegionPlan.compareTo(org.apache.hadoop.hbase.master.RegionPlan)", "public int compareTo(org.apache.hadoop.hbase.master.RegionPlan)"], ["int", "org.apache.hadoop.hbase.master.RegionPlan.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.master.RegionPlan.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.master.RegionPlan.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.master.RegionPlan.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.RegionStates.getRegionAssignments()", "public synchronized java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName> getRegionAssignments()"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.org.apache.hadoop.hbase.ServerName getRegionServerOfRegion(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized org.apache.hadoop.hbase.ServerName getRegionServerOfRegion(org.apache.hadoop.hbase.HRegionInfo)"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.master.RegionState>", "org.apache.hadoop.hbase.master.RegionStates.getRegionsInTransition()", "public synchronized java.util.Map<java.lang.String, org.apache.hadoop.hbase.master.RegionState> getRegionsInTransition()"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.boolean isRegionInTransition(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized boolean isRegionInTransition(org.apache.hadoop.hbase.HRegionInfo)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.boolean isRegionInTransition(java.lang.String)", "public synchronized boolean isRegionInTransition(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.boolean isRegionsInTransition()", "public synchronized boolean isRegionsInTransition()"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.boolean isRegionOnline(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized boolean isRegionOnline(org.apache.hadoop.hbase.HRegionInfo)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.boolean isRegionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized boolean isRegionOffline(org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.master.RegionStates.isRegionInState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State...)", "public boolean isRegionInState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State...)"], ["boolean", "org.apache.hadoop.hbase.master.RegionStates.isRegionInState(java.lang.String, org.apache.hadoop.hbase.master.RegionState$State...)", "public boolean isRegionInState(java.lang.String, org.apache.hadoop.hbase.master.RegionState$State...)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.void waitForUpdate(long)", "public synchronized void waitForUpdate(long) throws java.lang.InterruptedException"], ["org.apache.hadoop.hbase.master.RegionState", "org.apache.hadoop.hbase.master.RegionStates.getRegionTransitionState(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.master.RegionState getRegionTransitionState(org.apache.hadoop.hbase.HRegionInfo)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.org.apache.hadoop.hbase.master.RegionState getRegionTransitionState(java.lang.String)", "public synchronized org.apache.hadoop.hbase.master.RegionState getRegionTransitionState(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.createRegionStates(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void createRegionStates(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)"], ["org.apache.hadoop.hbase.master.RegionState", "org.apache.hadoop.hbase.master.RegionStates.createRegionState(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.master.RegionState createRegionState(org.apache.hadoop.hbase.HRegionInfo)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.org.apache.hadoop.hbase.master.RegionState createRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public synchronized org.apache.hadoop.hbase.master.RegionState createRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.master.RegionState", "org.apache.hadoop.hbase.master.RegionStates.updateRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State)", "public org.apache.hadoop.hbase.master.RegionState updateRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State)"], ["org.apache.hadoop.hbase.master.RegionState", "org.apache.hadoop.hbase.master.RegionStates.updateRegionState(org.apache.hadoop.hbase.RegionTransition, org.apache.hadoop.hbase.master.RegionState$State)", "public org.apache.hadoop.hbase.master.RegionState updateRegionState(org.apache.hadoop.hbase.RegionTransition, org.apache.hadoop.hbase.master.RegionState$State)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.org.apache.hadoop.hbase.master.RegionState transitionOpenFromPendingOpenOrOpeningOnServer(org.apache.hadoop.hbase.RegionTransition, org.apache.hadoop.hbase.master.RegionState, org.apache.hadoop.hbase.ServerName)", "public synchronized org.apache.hadoop.hbase.master.RegionState transitionOpenFromPendingOpenOrOpeningOnServer(org.apache.hadoop.hbase.RegionTransition, org.apache.hadoop.hbase.master.RegionState, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.master.RegionState", "org.apache.hadoop.hbase.master.RegionStates.updateRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.master.RegionState updateRegionState(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)", "public void regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long)", "public void regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.void logSplit(org.apache.hadoop.hbase.ServerName)", "public synchronized void logSplit(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.logSplit(org.apache.hadoop.hbase.HRegionInfo)", "public void logSplit(org.apache.hadoop.hbase.HRegionInfo)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.void clearLastAssignment(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized void clearLastAssignment(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.regionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public void regionOffline(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.RegionStates.regionOffline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State)", "public void regionOffline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.RegionState$State)"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.RegionStates.serverOffline(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName)", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> serverOffline(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName)"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.RegionStates.getRegionsOfTable(org.apache.hadoop.hbase.TableName)", "public synchronized java.util.List<org.apache.hadoop.hbase.HRegionInfo> getRegionsOfTable(org.apache.hadoop.hbase.TableName)"], ["java.util.Map<org.apache.hadoop.hbase.master.RegionState$State, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.RegionStates.getRegionByStateOfTable(org.apache.hadoop.hbase.TableName)", "public synchronized java.util.Map<org.apache.hadoop.hbase.master.RegionState$State, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> getRegionByStateOfTable(org.apache.hadoop.hbase.TableName)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.void waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized void waitOnRegionToClearRegionsInTransition(org.apache.hadoop.hbase.HRegionInfo) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.master.RegionStates.tableDeleted(org.apache.hadoop.hbase.TableName)", "public void tableDeleted(org.apache.hadoop.hbase.TableName)"], ["java.util.Set<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.RegionStates.getServerRegions(org.apache.hadoop.hbase.ServerName)", "public synchronized java.util.Set<org.apache.hadoop.hbase.HRegionInfo> getServerRegions(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.RegionStates.void deleteRegion(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized void deleteRegion(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.hbase.master.ServerManager", "org.apache.hadoop.hbase.master.ServerManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.ServerManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.ServerManager.registerListener(org.apache.hadoop.hbase.master.ServerListener)", "public void registerListener(org.apache.hadoop.hbase.master.ServerListener)"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.unregisterListener(org.apache.hadoop.hbase.master.ServerListener)", "public boolean unregisterListener(org.apache.hadoop.hbase.master.ServerListener)"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds", "org.apache.hadoop.hbase.master.ServerManager.getLastFlushedSequenceId(byte[])", "public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getLastFlushedSequenceId(byte[])"], ["org.apache.hadoop.hbase.ServerLoad", "org.apache.hadoop.hbase.master.ServerManager.getLoad(org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.ServerLoad getLoad(org.apache.hadoop.hbase.ServerName)"], ["double", "org.apache.hadoop.hbase.master.ServerManager.getAverageLoad()", "public double getAverageLoad()"], ["int", "org.apache.hadoop.hbase.master.ServerManager.countOfRegionServers()", "public int countOfRegionServers()"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerLoad>", "org.apache.hadoop.hbase.master.ServerManager.getOnlineServers()", "public java.util.Map<org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerLoad> getOnlineServers()"], ["org.apache.hadoop.hbase.master.DeadServer", "org.apache.hadoop.hbase.master.ServerManager.getDeadServers()", "public org.apache.hadoop.hbase.master.DeadServer getDeadServers()"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.areDeadServersInProgress()", "public boolean areDeadServersInProgress()"], ["synchronized", "org.apache.hadoop.hbase.master.ServerManager.void expireServer(org.apache.hadoop.hbase.ServerName)", "public synchronized void expireServer(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.ServerManager.void processDeadServer(org.apache.hadoop.hbase.ServerName)", "public synchronized void processDeadServer(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.ServerManager.void processDeadServer(org.apache.hadoop.hbase.ServerName, boolean)", "public synchronized void processDeadServer(org.apache.hadoop.hbase.ServerName, boolean)"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.removeServerFromDrainList(org.apache.hadoop.hbase.ServerName)", "public boolean removeServerFromDrainList(org.apache.hadoop.hbase.ServerName)"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.addServerToDrainList(org.apache.hadoop.hbase.ServerName)", "public boolean addServerToDrainList(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.regionserver.RegionOpeningState", "org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public org.apache.hadoop.hbase.regionserver.RegionOpeningState sendRegionOpen(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.RegionOpeningState>", "org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.util.Triple<org.apache.hadoop.hbase.HRegionInfo, java.lang.Integer, java.util.List<org.apache.hadoop.hbase.ServerName>>>)", "public java.util.List<org.apache.hadoop.hbase.regionserver.RegionOpeningState> sendRegionOpen(org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.util.Triple<org.apache.hadoop.hbase.HRegionInfo, java.lang.Integer, java.util.List<org.apache.hadoop.hbase.ServerName>>>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int, org.apache.hadoop.hbase.ServerName, boolean)", "public boolean sendRegionClose(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int, org.apache.hadoop.hbase.ServerName, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int)", "public boolean sendRegionClose(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.ServerManager.sendRegionWarmup(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo)", "public void sendRegionWarmup(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.master.ServerManager.closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, long)", "public static void closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, long) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.master.ServerManager.sendRegionsMerge(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void sendRegionsMerge(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.isServerReachable(org.apache.hadoop.hbase.ServerName)", "public boolean isServerReachable(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.ServerManager.waitForRegionServers(org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public void waitForRegionServers(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.lang.InterruptedException"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.ServerManager.getOnlineServersList()", "public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServersList()"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.ServerManager.getDrainingServersList()", "public java.util.List<org.apache.hadoop.hbase.ServerName> getDrainingServersList()"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.isServerOnline(org.apache.hadoop.hbase.ServerName)", "public boolean isServerOnline(org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.master.ServerManager.boolean isServerDead(org.apache.hadoop.hbase.ServerName)", "public synchronized boolean isServerDead(org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.ServerManager.shutdownCluster()", "public void shutdownCluster()"], ["boolean", "org.apache.hadoop.hbase.master.ServerManager.isClusterShutdown()", "public boolean isClusterShutdown()"], ["void", "org.apache.hadoop.hbase.master.ServerManager.stop()", "public void stop()"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.ServerManager.createDestinationServersList(org.apache.hadoop.hbase.ServerName)", "public java.util.List<org.apache.hadoop.hbase.ServerName> createDestinationServersList(org.apache.hadoop.hbase.ServerName)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.ServerManager.createDestinationServersList()", "public java.util.List<org.apache.hadoop.hbase.ServerName> createDestinationServersList()"], ["boolean", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta$1.visit(org.apache.hadoop.hbase.client.Result)", "public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta(org.apache.hadoop.hbase.client.Connection)", "public org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta(org.apache.hadoop.hbase.client.Connection)"], ["org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta(org.apache.hadoop.hbase.client.Connection, java.util.Set<org.apache.hadoop.hbase.TableName>, boolean)", "public org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta(org.apache.hadoop.hbase.client.Connection, java.util.Set<org.apache.hadoop.hbase.TableName>, boolean)"], ["void", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.initialize()", "public void initialize() throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getRegionNameToRegionInfoMap()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.HRegionInfo> getRegionNameToRegionInfoMap()"], ["java.util.Map<org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getTableToRegionMap()", "public java.util.Map<org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> getTableToRegionMap()"], ["java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getRegionToRegionServerMap()", "public java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName> getRegionToRegionServerMap()"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getRegionServerToRegionMap()", "public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> getRegionServerToRegionMap()"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getExistingAssignmentPlan()", "public org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan getExistingAssignmentPlan()"], ["java.util.Set<org.apache.hadoop.hbase.TableName>", "org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.getTableSet()", "public java.util.Set<org.apache.hadoop.hbase.TableName> getTableSet()"], ["org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective[]", "org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective.values()", "public static org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective[] values()"], ["org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective", "org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective valueOf(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.master.SplitLogManager$Task.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.SplitLogManager$Task", "org.apache.hadoop.hbase.master.SplitLogManager$Task()", "public org.apache.hadoop.hbase.master.SplitLogManager$Task()"], ["boolean", "org.apache.hadoop.hbase.master.SplitLogManager$Task.isOrphan()", "public boolean isOrphan()"], ["boolean", "org.apache.hadoop.hbase.master.SplitLogManager$Task.isUnassigned()", "public boolean isUnassigned()"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager$Task.heartbeatNoDetails(long)", "public void heartbeatNoDetails(long)"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager$Task.heartbeat(long, int, org.apache.hadoop.hbase.ServerName)", "public void heartbeat(long, int, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager$Task.setUnassigned()", "public void setUnassigned()"], ["org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch", "org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch()", "public org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch()"], ["java.lang.String", "org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus[]", "org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus.values()", "public static org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus[] values()"], ["org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus", "org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus valueOf(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor", "org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor(org.apache.hadoop.hbase.master.SplitLogManager, int, org.apache.hadoop.hbase.Stoppable)", "public org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor(org.apache.hadoop.hbase.master.SplitLogManager, int, org.apache.hadoop.hbase.Stoppable)"], ["org.apache.hadoop.hbase.master.SplitLogManager", "org.apache.hadoop.hbase.master.SplitLogManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.master.SplitLogManager(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus[]", "org.apache.hadoop.hbase.master.SplitLogManager.getFileList(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter)", "public static org.apache.hadoop.fs.FileStatus[] getFileList(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(org.apache.hadoop.fs.Path)", "public long splitLogDistributed(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(java.util.List<org.apache.hadoop.fs.Path>)", "public long splitLogDistributed(java.util.List<org.apache.hadoop.fs.Path>) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(java.util.Set<org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter)", "public long splitLogDistributed(java.util.Set<org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager.stop()", "public void stop()"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager.setRecoveryMode(boolean)", "public void setRecoveryMode(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.SplitLogManager.markRegionsRecovering(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>)", "public void markRegionsRecovering(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.InterruptedIOException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.SplitLogManager.isLogReplaying()", "public boolean isLogReplaying()"], ["boolean", "org.apache.hadoop.hbase.master.SplitLogManager.isLogSplitting()", "public boolean isLogSplitting()"], ["org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode", "org.apache.hadoop.hbase.master.SplitLogManager.getRecoveryMode()", "public org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode getRecoveryMode()"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager$NullTableLock.acquire()", "public void acquire() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager$NullTableLock.release()", "public void release() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager()", "public org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager()"], ["org.apache.hadoop.hbase.master.TableLockManager$TableLock", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.writeLock(org.apache.hadoop.hbase.TableName, java.lang.String)", "public org.apache.hadoop.hbase.master.TableLockManager$TableLock writeLock(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["org.apache.hadoop.hbase.master.TableLockManager$TableLock", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.readLock(org.apache.hadoop.hbase.TableName, java.lang.String)", "public org.apache.hadoop.hbase.master.TableLockManager$TableLock readLock(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.reapAllExpiredLocks()", "public void reapAllExpiredLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.reapWriteLocks()", "public void reapWriteLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.tableDeleted(org.apache.hadoop.hbase.TableName)", "public void tableDeleted(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$NullTableLockManager.visitAllLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public void visitAllLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$1.handleMetadata(byte[])", "public void handleMetadata(byte[])"], ["org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$TableLockImpl", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$TableLockImpl(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName, long, boolean, java.lang.String)", "public org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$TableLockImpl(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName, long, boolean, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$TableLockImpl.acquire()", "public void acquire() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager$TableLockImpl.release()", "public void release() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName, long, long, long)", "public org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName, long, long, long)"], ["org.apache.hadoop.hbase.master.TableLockManager$TableLock", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.writeLock(org.apache.hadoop.hbase.TableName, java.lang.String)", "public org.apache.hadoop.hbase.master.TableLockManager$TableLock writeLock(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["org.apache.hadoop.hbase.master.TableLockManager$TableLock", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.readLock(org.apache.hadoop.hbase.TableName, java.lang.String)", "public org.apache.hadoop.hbase.master.TableLockManager$TableLock readLock(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.visitAllLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public void visitAllLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.reapWriteLocks()", "public void reapWriteLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.reapAllExpiredLocks()", "public void reapAllExpiredLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.tableDeleted(org.apache.hadoop.hbase.TableName)", "public void tableDeleted(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.TableLockManager", "org.apache.hadoop.hbase.master.TableLockManager()", "public org.apache.hadoop.hbase.master.TableLockManager()"], ["org.apache.hadoop.hbase.master.TableLockManager", "org.apache.hadoop.hbase.master.TableLockManager.createTableLockManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName)", "public static org.apache.hadoop.hbase.master.TableLockManager createTableLockManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$TableLock", "org.apache.hadoop.hbase.master.TableLockManager.fromBytes(byte[])", "public static org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$TableLock fromBytes(byte[])"], ["org.apache.hadoop.hbase.master.TableNamespaceManager", "org.apache.hadoop.hbase.master.TableNamespaceManager(org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.TableNamespaceManager(org.apache.hadoop.hbase.master.MasterServices)"], ["void", "org.apache.hadoop.hbase.master.TableNamespaceManager.start()", "public void start() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.TableNamespaceManager.org.apache.hadoop.hbase.NamespaceDescriptor get(java.lang.String)", "public synchronized org.apache.hadoop.hbase.NamespaceDescriptor get(java.lang.String) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.TableNamespaceManager.void create(org.apache.hadoop.hbase.NamespaceDescriptor)", "public synchronized void create(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.TableNamespaceManager.void update(org.apache.hadoop.hbase.NamespaceDescriptor)", "public synchronized void update(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.TableNamespaceManager.void remove(java.lang.String)", "public synchronized void remove(java.lang.String) throws java.io.IOException"], ["java.util.NavigableSet<org.apache.hadoop.hbase.NamespaceDescriptor>", "org.apache.hadoop.hbase.master.TableNamespaceManager.list()", "public synchronized java.util.NavigableSet<org.apache.hadoop.hbase.NamespaceDescriptor> list() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.TableNamespaceManager.boolean isTableAvailableAndInitialized()", "public synchronized boolean isTableAvailableAndInitialized() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.TableNamespaceManager.getMaxTables(org.apache.hadoop.hbase.NamespaceDescriptor)", "public static long getMaxTables(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.master.TableNamespaceManager.getMaxRegions(org.apache.hadoop.hbase.NamespaceDescriptor)", "public static long getMaxRegions(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.UnAssignCallable", "org.apache.hadoop.hbase.master.UnAssignCallable(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.master.UnAssignCallable(org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo)"], ["java.lang.Object", "org.apache.hadoop.hbase.master.UnAssignCallable.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.balancer.BalancerChore", "org.apache.hadoop.hbase.master.balancer.BalancerChore(org.apache.hadoop.hbase.master.HMaster)", "public org.apache.hadoop.hbase.master.balancer.BalancerChore(org.apache.hadoop.hbase.master.HMaster)"], ["int", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$1.compare(java.lang.Integer, java.lang.Integer)", "public int compare(java.lang.Integer, java.lang.Integer)"], ["int", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type[]", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type.values()", "public static org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type[] values()"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action(org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type)", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action(org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action$Type)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action.undoAction()", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action undoAction()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$AssignRegionAction", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$AssignRegionAction(int, int)", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$AssignRegionAction(int, int)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$AssignRegionAction.undoAction()", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action undoAction()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$AssignRegionAction.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$MoveRegionAction", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$MoveRegionAction(int, int, int)", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$MoveRegionAction(int, int, int)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$MoveRegionAction.undoAction()", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action undoAction()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$MoveRegionAction.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$SwapRegionsAction", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$SwapRegionsAction(int, int, int, int)", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$SwapRegionsAction(int, int, int, int)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$SwapRegionsAction.undoAction()", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action undoAction()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$SwapRegionsAction.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.doAction(org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action)", "public void doAction(org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster$Action)"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$DefaultRackManager.getRack(org.apache.hadoop.hbase.ServerName)", "public java.lang.String getRack(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer()", "public org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer()"], ["boolean", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.tablesOnMaster(org.apache.hadoop.conf.Configuration)", "public static boolean tablesOnMaster(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.shouldBeOnMaster(org.apache.hadoop.hbase.HRegionInfo)", "public boolean shouldBeOnMaster(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)", "public void setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.setMasterServices(org.apache.hadoop.hbase.master.MasterServices)", "public void setMasterServices(org.apache.hadoop.hbase.master.MasterServices)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.setRackManager(org.apache.hadoop.hbase.master.RackManager)", "public void setRackManager(org.apache.hadoop.hbase.master.RackManager)"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.immediateAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName> immediateAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.randomAssignment(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public org.apache.hadoop.hbase.ServerName randomAssignment(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.retainAssignment(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> retainAssignment(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.initialize()", "public void initialize() throws org.apache.hadoop.hbase.HBaseIOException"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)", "public void regionOnline(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.regionOffline(org.apache.hadoop.hbase.HRegionInfo)", "public void regionOffline(org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.stop(java.lang.String)", "public void stop(java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.onConfigurationChange(org.apache.hadoop.conf.Configuration)", "public void onConfigurationChange(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.balancer.ClusterLoadState", "org.apache.hadoop.hbase.master.balancer.ClusterLoadState(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)", "public org.apache.hadoop.hbase.master.balancer.ClusterLoadState(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)"], ["org.apache.hadoop.hbase.master.balancer.ClusterStatusChore", "org.apache.hadoop.hbase.master.balancer.ClusterStatusChore(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.LoadBalancer)", "public org.apache.hadoop.hbase.master.balancer.ClusterStatusChore(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.LoadBalancer)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.hbase.master.RackManager)", "public org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.hbase.master.RackManager)"], ["void", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.updateMetaWithFavoredNodesInfo(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>>, org.apache.hadoop.hbase.client.Connection)", "public static void updateMetaWithFavoredNodesInfo(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>>, org.apache.hadoop.hbase.client.Connection) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.updateMetaWithFavoredNodesInfo(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>>, org.apache.hadoop.conf.Configuration)", "public static void updateMetaWithFavoredNodesInfo(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>>, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.ServerName[]", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.getFavoredNodesList(byte[])", "public static org.apache.hadoop.hbase.ServerName[] getFavoredNodesList(byte[]) throws com.google.protobuf.InvalidProtocolBufferException"], ["byte[]", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.getFavoredNodes(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public static byte[] getFavoredNodes(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName[]>", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.placeSecondaryAndTertiaryWithRestrictions(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>)", "public java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName[]> placeSecondaryAndTertiaryWithRestrictions(java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>)"], ["void", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.initialize()", "public void initialize()"], ["java.lang.String", "org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.getFavoredNodesAsString(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public static java.lang.String getFavoredNodesAsString(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer()", "public org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer()"], ["void", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["java.util.List<org.apache.hadoop.hbase.master.RegionPlan>", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)", "public java.util.List<org.apache.hadoop.hbase.master.RegionPlan> balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)"], ["java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>> roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.HRegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.randomAssignment(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public org.apache.hadoop.hbase.ServerName randomAssignment(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.getFavoredNodes(org.apache.hadoop.hbase.HRegionInfo)", "public java.util.List<org.apache.hadoop.hbase.ServerName> getFavoredNodes(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position[]", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position.values()", "public static org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position[] values()"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan()", "public org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan()"], ["synchronized", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.void updateFavoredNodesMap(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public synchronized void updateFavoredNodesMap(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.getFavoredNodes(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized java.util.List<org.apache.hadoop.hbase.ServerName> getFavoredNodes(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.getFavoredServerPosition(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.hbase.ServerName)", "public static org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan$Position getFavoredServerPosition(java.util.List<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.hbase.ServerName)"], ["java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>>", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.getAssignmentMap()", "public synchronized java.util.Map<org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>> getAssignmentMap()"], ["synchronized", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.void updateAssignmentPlan(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public synchronized void updateAssignmentPlan(org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["boolean", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.hashCode()", "public int hashCode()"], ["org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory", "org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory()", "public org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory()"], ["java.lang.Class<? extends org.apache.hadoop.hbase.master.LoadBalancer>", "org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory.getDefaultLoadBalancerClass()", "public static java.lang.Class<? extends org.apache.hadoop.hbase.master.LoadBalancer> getDefaultLoadBalancerClass()"], ["org.apache.hadoop.hbase.master.LoadBalancer", "org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory.getLoadBalancer(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.master.LoadBalancer getLoadBalancer(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.balancer.MetricsBalancer", "org.apache.hadoop.hbase.master.balancer.MetricsBalancer()", "public org.apache.hadoop.hbase.master.balancer.MetricsBalancer()"], ["void", "org.apache.hadoop.hbase.master.balancer.MetricsBalancer.balanceCluster(long)", "public void balanceCluster(long)"], ["void", "org.apache.hadoop.hbase.master.balancer.MetricsBalancer.incrMiscInvocations()", "public void incrMiscInvocations()"], ["int", "org.apache.hadoop.hbase.master.balancer.RegionInfoComparator.compare(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public int compare(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)"], ["int", "org.apache.hadoop.hbase.master.balancer.RegionInfoComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder$1.load(org.apache.hadoop.hbase.HRegionInfo)", "public java.util.List<org.apache.hadoop.hbase.ServerName> load(org.apache.hadoop.hbase.HRegionInfo) throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder$1.load(java.lang.Object)", "public java.lang.Object load(java.lang.Object) throws java.lang.Exception"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.setServices(org.apache.hadoop.hbase.master.MasterServices)", "public void setServices(org.apache.hadoop.hbase.master.MasterServices)"], ["void", "org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)", "public void setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)"], ["int", "org.apache.hadoop.hbase.master.balancer.ServerAndLoad.compareTo(org.apache.hadoop.hbase.master.balancer.ServerAndLoad)", "public int compareTo(org.apache.hadoop.hbase.master.balancer.ServerAndLoad)"], ["int", "org.apache.hadoop.hbase.master.balancer.ServerAndLoad.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.master.balancer.ServerAndLoad.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.master.balancer.ServerAndLoad.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer$BalanceInfo", "org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer$BalanceInfo(int, int)", "public org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer$BalanceInfo(int, int)"], ["org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer", "org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer()", "public org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer()"], ["java.util.List<org.apache.hadoop.hbase.master.RegionPlan>", "org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)", "public java.util.List<org.apache.hadoop.hbase.master.RegionPlan> balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)"], ["org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaHostCostFunction", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaHostCostFunction(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaHostCostFunction(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaRackCostFunction", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaRackCostFunction(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer$RegionReplicaRackCostFunction(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer()", "public org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer()"], ["void", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.onConfigurationChange(org.apache.hadoop.conf.Configuration)", "public void onConfigurationChange(org.apache.hadoop.conf.Configuration)"], ["synchronized", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.void setConf(org.apache.hadoop.conf.Configuration)", "public synchronized void setConf(org.apache.hadoop.conf.Configuration)"], ["synchronized", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.void setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)", "public synchronized void setClusterStatus(org.apache.hadoop.hbase.ClusterStatus)"], ["synchronized", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.void setMasterServices(org.apache.hadoop.hbase.master.MasterServices)", "public synchronized void setMasterServices(org.apache.hadoop.hbase.master.MasterServices)"], ["java.util.List<org.apache.hadoop.hbase.master.RegionPlan>", "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)", "public synchronized java.util.List<org.apache.hadoop.hbase.master.RegionPlan> balanceCluster(java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>>)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate$1.apply(org.apache.hadoop.fs.FileStatus)", "public boolean apply(org.apache.hadoop.fs.FileStatus)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate$1.apply(java.lang.Object)", "public boolean apply(java.lang.Object)"], ["org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate", "org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate()", "public org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate()"], ["java.lang.Iterable<org.apache.hadoop.fs.FileStatus>", "org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate.getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)", "public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)"], ["org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate", "org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate()", "public org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate()"], ["void", "org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate", "org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate()", "public org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate()"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate.isFileDeletable(org.apache.hadoop.fs.FileStatus)", "public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate.isLogDeletable(org.apache.hadoop.fs.FileStatus)", "public boolean isLogDeletable(org.apache.hadoop.fs.FileStatus)"], ["org.apache.hadoop.hbase.master.cleaner.CleanerChore", "org.apache.hadoop.hbase.master.cleaner.CleanerChore(java.lang.String, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String)", "public org.apache.hadoop.hbase.master.cleaner.CleanerChore(java.lang.String, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.cleaner.CleanerChore.cleanup()", "public void cleanup()"], ["org.apache.hadoop.hbase.master.cleaner.HFileCleaner", "org.apache.hadoop.hbase.master.cleaner.HFileCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.master.cleaner.HFileCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["java.util.List<org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate>", "org.apache.hadoop.hbase.master.cleaner.HFileCleaner.getDelegatesForTesting()", "public java.util.List<org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate> getDelegatesForTesting()"], ["org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner", "org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner()", "public org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner()"], ["synchronized", "org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner.boolean isFileDeletable(org.apache.hadoop.fs.FileStatus)", "public synchronized boolean isFileDeletable(org.apache.hadoop.fs.FileStatus)"], ["void", "org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.master.cleaner.LogCleaner", "org.apache.hadoop.hbase.master.cleaner.LogCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.master.cleaner.LogCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner()", "public org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner()"], ["void", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner.isFileDeletable(org.apache.hadoop.fs.FileStatus)", "public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus)"], ["org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner()", "public org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner()"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.isLogDeletable(org.apache.hadoop.fs.FileStatus)", "public boolean isLogDeletable(org.apache.hadoop.fs.FileStatus)"], ["void", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority[]", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority.values()", "public static org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority[] values()"], ["org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority valueOf(java.lang.String)"], ["int", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler$ClosedPriority.getValue()", "public int getValue()"], ["org.apache.hadoop.hbase.master.handler.ClosedRegionHandler", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.master.handler.ClosedRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo)"], ["int", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.getPriority()", "public int getPriority()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.getHRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getHRegionInfo()"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process()", "public void process()"], ["java.lang.Void", "org.apache.hadoop.hbase.master.handler.CreateTableHandler$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.handler.CreateTableHandler$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.handler.CreateTableHandler", "org.apache.hadoop.hbase.master.handler.CreateTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterFileSystem, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.CreateTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterFileSystem, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.master.handler.CreateTableHandler", "org.apache.hadoop.hbase.master.handler.CreateTableHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.CreateTableHandler prepare() throws org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException, org.apache.hadoop.hbase.TableExistsException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.CreateTableHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.CreateTableHandler.process()", "public void process()"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.handler.CreateTableHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.handler.DeleteTableHandler", "org.apache.hadoop.hbase.master.handler.DeleteTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.DeleteTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.DeleteTableHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run()", "public void run()"], ["org.apache.hadoop.hbase.master.handler.DisableTableHandler", "org.apache.hadoop.hbase.master.handler.DisableTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)", "public org.apache.hadoop.hbase.master.handler.DisableTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)"], ["org.apache.hadoop.hbase.master.handler.DisableTableHandler", "org.apache.hadoop.hbase.master.handler.DisableTableHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.DisableTableHandler prepare() throws org.apache.hadoop.hbase.TableNotFoundException, org.apache.hadoop.hbase.TableNotEnabledException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.DisableTableHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.DisableTableHandler.process()", "public void process()"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.handler.DisableTableHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler", "org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.CatalogJanitor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.CatalogJanitor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, boolean)"], ["void", "org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.handler.EnableTableHandler", "org.apache.hadoop.hbase.master.handler.EnableTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)", "public org.apache.hadoop.hbase.master.handler.EnableTableHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)"], ["org.apache.hadoop.hbase.master.handler.EnableTableHandler", "org.apache.hadoop.hbase.master.handler.EnableTableHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)", "public org.apache.hadoop.hbase.master.handler.EnableTableHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.master.TableLockManager, boolean)"], ["org.apache.hadoop.hbase.master.handler.EnableTableHandler", "org.apache.hadoop.hbase.master.handler.EnableTableHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.EnableTableHandler prepare() throws org.apache.hadoop.hbase.TableNotFoundException, org.apache.hadoop.hbase.TableNotDisabledException, java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.EnableTableHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.EnableTableHandler.process()", "public void process()"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.handler.EnableTableHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.handler.LogReplayHandler", "org.apache.hadoop.hbase.master.handler.LogReplayHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.master.handler.LogReplayHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.LogReplayHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.LogReplayHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler", "org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName)"], ["void", "org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.handler.ModifyTableHandler", "org.apache.hadoop.hbase.master.handler.ModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.ModifyTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.ModifyTableHandler.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority[]", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority.values()", "public static org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority[] values()"], ["org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority valueOf(java.lang.String)"], ["int", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler$OpenedPriority.getValue()", "public int getValue()"], ["org.apache.hadoop.hbase.master.handler.OpenedRegionHandler", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public org.apache.hadoop.hbase.master.handler.OpenedRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.AssignmentManager, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["int", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.getPriority()", "public int getPriority()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.getHRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getHRegionInfo()"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.process()", "public void process()"], ["org.apache.hadoop.hbase.master.handler.ServerShutdownHandler", "org.apache.hadoop.hbase.master.handler.ServerShutdownHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName, boolean)", "public org.apache.hadoop.hbase.master.handler.ServerShutdownHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.DeadServer, org.apache.hadoop.hbase.ServerName, boolean)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.getInformativeName()", "public java.lang.String getInformativeName()"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process()", "public void process() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.processDeadRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.AssignmentManager)", "public static boolean processDeadRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.master.AssignmentManager) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler", "org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler", "org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler(org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler(org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.handler.TableEventHandler", "org.apache.hadoop.hbase.master.handler.TableEventHandler(org.apache.hadoop.hbase.executor.EventType, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.TableEventHandler(org.apache.hadoop.hbase.executor.EventType, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.master.handler.TableEventHandler", "org.apache.hadoop.hbase.master.handler.TableEventHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.TableEventHandler prepare() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.handler.TableEventHandler.process()", "public void process()"], ["boolean", "org.apache.hadoop.hbase.master.handler.TableEventHandler.reOpenAllRegions(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public boolean reOpenAllRegions(java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.master.handler.TableEventHandler.getTableDescriptor()", "public org.apache.hadoop.hbase.HTableDescriptor getTableDescriptor() throws java.io.FileNotFoundException, java.io.IOException"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.handler.TableEventHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler", "org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices)"], ["java.lang.String", "org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.handler.TruncateTableHandler", "org.apache.hadoop.hbase.master.handler.TruncateTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, boolean)", "public org.apache.hadoop.hbase.master.handler.TruncateTableHandler(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.MasterServices, boolean)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure()", "public org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure()"], ["org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$2.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$2.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$3.createHdfsRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> createHdfsRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.CreateTableProcedure", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.CreateTableProcedure", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.CreateTableProcedure", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch)", "public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["void", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure()", "public org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure()"], ["org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, byte[])", "public org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure$2.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure$2.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch)", "public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$BulkDisabler$1.run()", "public void run()"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$BulkDisabler", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$BulkDisabler(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$BulkDisabler(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult[]", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult.values()", "public static org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult[] values()"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$MarkRegionOfflineOpResult valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean)", "public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.DisableTableProcedure", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch)", "public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.EnableTableProcedure", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.EnableTableProcedure", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean)", "public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.EnableTableProcedure", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch)", "public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.isOnlineSchemaChangeAllowed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public static boolean isOnlineSchemaChangeAllowed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.checkTableModifiable(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName)", "public static void checkTableModifiable(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.deleteColumnFamilyFromFileSystem(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>, byte[])", "public static void deleteColumnFamilyFromFileSystem(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>, byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.reOpenAllRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public static boolean reOpenAllRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$MasterProcedureStoreListener", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$MasterProcedureStoreListener(org.apache.hadoop.hbase.master.HMaster)", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$MasterProcedureStoreListener(org.apache.hadoop.hbase.master.HMaster)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$MasterProcedureStoreListener.abortProcess()", "public void abortProcess()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$WALStoreLeaseRecovery$1.progress()", "public boolean progress()"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$WALStoreLeaseRecovery", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$WALStoreLeaseRecovery(org.apache.hadoop.hbase.master.HMaster)", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$WALStoreLeaseRecovery(org.apache.hadoop.hbase.master.HMaster)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$WALStoreLeaseRecovery.recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public void recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv(org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv(org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.security.User", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.getRequestUser()", "public org.apache.hadoop.hbase.security.User getRequestUser() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.MasterServices", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.getMasterServices()", "public org.apache.hadoop.hbase.master.MasterServices getMasterServices()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.getMasterConfiguration()", "public org.apache.hadoop.conf.Configuration getMasterConfiguration()"], ["org.apache.hadoop.hbase.master.MasterCoprocessorHost", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.getMasterCoprocessorHost()", "public org.apache.hadoop.hbase.master.MasterCoprocessorHost getMasterCoprocessorHost()"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.getProcedureQueue()", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue getProcedureQueue()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.isRunning()", "public boolean isRunning()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.isInitialized()", "public boolean isInitialized()"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue(int)", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue(int)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.addFront(org.apache.hadoop.hbase.procedure2.Procedure)", "public void addFront(org.apache.hadoop.hbase.procedure2.Procedure)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.addBack(org.apache.hadoop.hbase.procedure2.Procedure)", "public void addBack(org.apache.hadoop.hbase.procedure2.Procedure)"], ["java.lang.Long", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.poll()", "public java.lang.Long poll()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.isAvailable()", "public boolean isAvailable()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.isEmpty()", "public boolean isEmpty()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.isLocked()", "public boolean isLocked()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.tryRead(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName, java.lang.String)", "public boolean tryRead(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.releaseRead(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName)", "public void releaseRead(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName)"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.tryWrite(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName, java.lang.String)", "public boolean tryWrite(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.releaseWrite(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName)", "public void releaseWrite(org.apache.hadoop.hbase.master.TableLockManager, org.apache.hadoop.hbase.TableName)"], ["int", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.getPriority()", "public int getPriority()"], ["java.lang.String", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue$TableRunQueue.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.TableLockManager)", "public org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.TableLockManager)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.addFront(org.apache.hadoop.hbase.procedure2.Procedure)", "public void addFront(org.apache.hadoop.hbase.procedure2.Procedure)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.addBack(org.apache.hadoop.hbase.procedure2.Procedure)", "public void addBack(org.apache.hadoop.hbase.procedure2.Procedure)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.yield(org.apache.hadoop.hbase.procedure2.Procedure)", "public void yield(org.apache.hadoop.hbase.procedure2.Procedure)"], ["java.lang.Long", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.poll()", "public java.lang.Long poll()"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.signalAll()", "public void signalAll()"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.clear()", "public void clear()"], ["int", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.size()", "public int size()"], ["java.lang.String", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.completionCleanup(org.apache.hadoop.hbase.procedure2.Procedure)", "public void completionCleanup(org.apache.hadoop.hbase.procedure2.Procedure)"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.tryAcquireTableRead(org.apache.hadoop.hbase.TableName, java.lang.String)", "public boolean tryAcquireTableRead(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.releaseTableRead(org.apache.hadoop.hbase.TableName)", "public void releaseTableRead(org.apache.hadoop.hbase.TableName)"], ["boolean", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.tryAcquireTableWrite(org.apache.hadoop.hbase.TableName, java.lang.String)", "public boolean tryAcquireTableWrite(org.apache.hadoop.hbase.TableName, java.lang.String)"], ["void", "org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.releaseTableWrite(org.apache.hadoop.hbase.TableName)", "public void releaseTableWrite(org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.protobuf.generated.RPCProtos$UserInformation", "org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.toProtoUserInfo(org.apache.hadoop.security.UserGroupInformation)", "public static org.apache.hadoop.hbase.protobuf.generated.RPCProtos$UserInformation toProtoUserInfo(org.apache.hadoop.security.UserGroupInformation)"], ["org.apache.hadoop.security.UserGroupInformation", "org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.toUserInfo(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$UserInformation)", "public static org.apache.hadoop.security.UserGroupInformation toUserInfo(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$UserInformation)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure()", "public org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure()"], ["org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor)", "public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["void", "org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch$CompatibilityLatch.await()", "public void await() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch$NoopLatch.await()", "public void await() throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch", "org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch()", "public org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch()"], ["org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch", "org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.createLatch()", "public static org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch createLatch()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.hasProcedureSupport()", "public static boolean hasProcedureSupport()"], ["java.lang.Boolean", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$1.evaluate()", "public java.lang.Boolean evaluate() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$1.evaluate()", "public java.lang.Object evaluate() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$2.evaluate()", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> evaluate() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$2.evaluate()", "public java.lang.Object evaluate() throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$3.evaluate()", "public java.lang.Boolean evaluate() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$3.evaluate()", "public java.lang.Object evaluate() throws java.io.IOException"], ["org.apache.hadoop.hbase.quotas.MasterQuotaManager", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$4.evaluate()", "public org.apache.hadoop.hbase.quotas.MasterQuotaManager evaluate() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$4.evaluate()", "public java.lang.Object evaluate() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.submitAndWaitProcedure(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure)", "public static byte[] submitAndWaitProcedure(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.waitForProcedureToComplete(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, long)", "public static byte[] waitForProcedureToComplete(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, long) throws java.io.IOException"], ["<T> T", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>)", "public static <T> T waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>) throws java.io.IOException"], ["<T> T", "org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait.waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, long, long, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>)", "public static <T> T waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, long, long, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType[]", "org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType.values()", "public static org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType[] values()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType valueOf(java.lang.String)"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure$2.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure$2.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure()", "public org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure()"], ["org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean)", "public org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.getTableOperationType()", "public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType()"], ["boolean", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)", "public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv)"], ["void", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.toStringClassDetails(java.lang.StringBuilder)", "public void toStringClassDetails(java.lang.StringBuilder)"], ["void", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.serializeStateData(java.io.OutputStream)", "public void serializeStateData(java.io.OutputStream) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.deserializeStateData(java.io.InputStream)", "public void deserializeStateData(java.io.InputStream) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.abort(java.lang.Object)", "public boolean abort(java.lang.Object)"], ["org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)"], ["org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler prepare() throws org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException, org.apache.hadoop.hbase.TableExistsException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.isFinished()", "public boolean isFinished()"], ["long", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.getCompletionTimestamp()", "public long getCompletionTimestamp()"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.getSnapshot()", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription getSnapshot()"], ["void", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.cancel(java.lang.String)", "public void cancel(java.lang.String)"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.getExceptionIfFailed()", "public org.apache.hadoop.hbase.errorhandling.ForeignException getExceptionIfFailed()"], ["void", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.rethrowExceptionIfFailed()", "public void rethrowExceptionIfFailed() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["org.apache.hadoop.hbase.master.handler.CreateTableHandler", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.CreateTableHandler prepare() throws org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException, org.apache.hadoop.hbase.TableExistsException, java.io.IOException"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler$1.editRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void editRegion(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler prepare() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.snapshotRegions(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>>)", "public void snapshotRegions(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName>>) throws java.io.IOException, org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.snapshot.SnapshotManager)", "public org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.snapshot.SnapshotManager)"], ["org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier", "org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifySnapshot(org.apache.hadoop.fs.Path, java.util.Set<java.lang.String>)", "public void verifySnapshot(org.apache.hadoop.fs.Path, java.util.Set<java.lang.String>) throws org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException, java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler prepare() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.isFinished()", "public boolean isFinished()"], ["long", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.getCompletionTimestamp()", "public long getCompletionTimestamp()"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.getSnapshot()", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription getSnapshot()"], ["void", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.cancel(java.lang.String)", "public void cancel(java.lang.String)"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.getExceptionIfFailed()", "public org.apache.hadoop.hbase.errorhandling.ForeignException getExceptionIfFailed()"], ["void", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.rethrowExceptionIfFailed()", "public void rethrowExceptionIfFailed() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["org.apache.hadoop.hbase.master.handler.TableEventHandler", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.handler.TableEventHandler prepare() throws java.io.IOException"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask(org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache)", "public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask(org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask.run()", "public void run()"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo(long, java.util.Collection<java.lang.String>)", "public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo(long, java.util.Collection<java.lang.String>)"], ["java.util.Collection<java.lang.String>", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo.getFiles()", "public java.util.Collection<java.lang.String> getFiles()"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo.hasBeenModified(long)", "public boolean hasBeenModified(long)"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.conf.Configuration, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector)", "public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.conf.Configuration, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector)", "public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.triggerCacheRefreshForTesting()", "public void triggerCacheRefreshForTesting()"], ["java.lang.Iterable<org.apache.hadoop.fs.FileStatus>", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.getUnreferencedFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)", "public synchronized java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getUnreferencedFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.isStopped()", "public boolean isStopped()"], ["java.util.Collection<java.lang.String>", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner$1.filesUnderSnapshot(org.apache.hadoop.fs.Path)", "public java.util.Collection<java.lang.String> filesUnderSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner()", "public org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner()"], ["java.lang.Iterable<org.apache.hadoop.fs.FileStatus>", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)", "public synchronized java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache", "org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.getFileCacheForTesting()", "public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache getFileCacheForTesting()"], ["java.util.Collection<java.lang.String>", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner$1.filesUnderSnapshot(org.apache.hadoop.fs.Path)", "public java.util.Collection<java.lang.String> filesUnderSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner()", "public org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner()"], ["java.lang.Iterable<org.apache.hadoop.fs.FileStatus>", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner.getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)", "public synchronized java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotManager", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager()", "public org.apache.hadoop.hbase.master.snapshot.SnapshotManager()"], ["org.apache.hadoop.hbase.master.snapshot.SnapshotManager", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster, org.apache.hadoop.hbase.procedure.ProcedureCoordinator, org.apache.hadoop.hbase.executor.ExecutorService)", "public org.apache.hadoop.hbase.master.snapshot.SnapshotManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster, org.apache.hadoop.hbase.procedure.ProcedureCoordinator, org.apache.hadoop.hbase.executor.ExecutorService) throws java.io.IOException, java.lang.UnsupportedOperationException"], ["java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription>", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.getCompletedSnapshots()", "public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription> getCompletedSnapshots() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.deleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void deleteSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws org.apache.hadoop.hbase.snapshot.SnapshotDoesNotExistException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public boolean isSnapshotDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.takeSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void takeSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.void setSnapshotHandlerForTesting(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotSentinel)", "public synchronized void setSnapshotHandlerForTesting(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotSentinel)"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.restoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void restoreSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isRestoreDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public boolean isRestoreDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.checkSnapshotSupport()", "public void checkSnapshotSupport() throws java.lang.UnsupportedOperationException"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster)", "public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException"], ["java.lang.String", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.getProcedureSignature()", "public java.lang.String getProcedureSignature()"], ["void", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public void execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isProcedureDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public boolean isProcedureDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices)"], ["org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.process()", "public void process()"], ["void", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.completeSnapshot(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem)", "public void completeSnapshot(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem) throws org.apache.hadoop.hbase.snapshot.SnapshotCreationException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.cancel(java.lang.String)", "public void cancel(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.isFinished()", "public boolean isFinished()"], ["long", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.getCompletionTimestamp()", "public long getCompletionTimestamp()"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.getSnapshot()", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription getSnapshot()"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.getExceptionIfFailed()", "public org.apache.hadoop.hbase.errorhandling.ForeignException getExceptionIfFailed()"], ["void", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowExceptionIfFailed()", "public void rethrowExceptionIfFailed() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowException()", "public void rethrowException() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["boolean", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.hasException()", "public boolean hasException()"], ["org.apache.hadoop.hbase.errorhandling.ForeignException", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.getException()", "public org.apache.hadoop.hbase.errorhandling.ForeignException getException()"], ["org.apache.hadoop.hbase.executor.EventHandler", "org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.prepare()", "public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception"], ["boolean", "org.apache.hadoop.hbase.migration.NamespaceUpgrade$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["int", "org.apache.hadoop.hbase.migration.NamespaceUpgrade$2.compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus)", "public int compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus)"], ["int", "org.apache.hadoop.hbase.migration.NamespaceUpgrade$2.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.migration.NamespaceUpgrade", "org.apache.hadoop.hbase.migration.NamespaceUpgrade()", "public org.apache.hadoop.hbase.migration.NamespaceUpgrade() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.init()", "public void init() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.upgradeTableDirs()", "public void upgradeTableDirs() throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.deleteRoot()", "public void deleteRoot() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateDotDirs()", "public void migrateDotDirs() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.makeNamespaceDirs()", "public void makeNamespaceDirs() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateTables()", "public void migrateTables() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateSnapshots()", "public void migrateSnapshots() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateMeta()", "public void migrateMeta() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateACL()", "public void migrateACL() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.verifyNSUpgrade(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean verifyNSUpgrade(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.migration.NamespaceUpgrade.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.migration.UpgradeTo96$1.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.migration.UpgradeTo96$1.isAborted()", "public boolean isAborted()"], ["int", "org.apache.hadoop.hbase.migration.UpgradeTo96.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.migration.UpgradeTo96.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.monitoring.LogMonitoring", "org.apache.hadoop.hbase.monitoring.LogMonitoring()", "public org.apache.hadoop.hbase.monitoring.LogMonitoring()"], ["java.util.Set<java.io.File>", "org.apache.hadoop.hbase.monitoring.LogMonitoring.getActiveLogFiles()", "public static java.util.Set<java.io.File> getActiveLogFiles() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.monitoring.LogMonitoring.dumpTailOfLogs(java.io.PrintWriter, long)", "public static void dumpTailOfLogs(java.io.PrintWriter, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage", "org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage(java.lang.String, long)", "public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage(java.lang.String, long)"], ["long", "org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage.estimateHeapUsage()", "public long estimateHeapUsage()"], ["org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer", "org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer(long)", "public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer(long)"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer.void add(java.lang.String)", "public synchronized void add(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer.void dumpTo(java.io.PrintWriter)", "public synchronized void dumpTo(java.io.PrintWriter)"], ["org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl()", "public org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl clone()", "public synchronized org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl clone()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getStatus()", "public java.lang.String getStatus()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getRPCQueueTime()", "public long getRPCQueueTime()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getRPCStartTime()", "public long getRPCStartTime()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getRPC()", "public java.lang.String getRPC()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java.lang.String getRPC(boolean)", "public synchronized java.lang.String getRPC(boolean)"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getRPCPacketLength()", "public long getRPCPacketLength()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getClient()", "public java.lang.String getClient()"], ["boolean", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.isRPCRunning()", "public boolean isRPCRunning()"], ["boolean", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.isOperationRunning()", "public boolean isOperationRunning()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.void setRPC(java.lang.String, java.lang.Object[], long)", "public synchronized void setRPC(java.lang.String, java.lang.Object[], long)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.setRPCPacket(com.google.protobuf.Message)", "public void setRPCPacket(com.google.protobuf.Message)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.setConnection(java.lang.String, int)", "public void setConnection(java.lang.String, int)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.markComplete(java.lang.String)", "public void markComplete(java.lang.String)"], ["java.util.Map<java.lang.String, java.lang.Object>", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.toMap()", "public synchronized java.util.Map<java.lang.String, java.lang.Object> toMap()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.toJSON()", "public java.lang.String toJSON() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.expireNow()", "public void expireNow()"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.cleanup()", "public void cleanup()"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.setDescription(java.lang.String)", "public void setDescription(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.setStatus(java.lang.String)", "public void setStatus(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.abort(java.lang.String)", "public void abort(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.resume(java.lang.String)", "public void resume(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.pause(java.lang.String)", "public void pause(java.lang.String)"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getCompletionTimestamp()", "public long getCompletionTimestamp()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getStateTime()", "public long getStateTime()"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask$State", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getState()", "public org.apache.hadoop.hbase.monitoring.MonitoredTask$State getState()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getStatusTime()", "public long getStatusTime()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getDescription()", "public java.lang.String getDescription()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.getStartTime()", "public long getStartTime()"], ["org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.clone()", "public org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl clone()"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.clone()", "public org.apache.hadoop.hbase.monitoring.MonitoredTask clone()"], ["java.lang.Object", "org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.clone()", "public java.lang.Object clone() throws java.lang.CloneNotSupportedException"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask$State[]", "org.apache.hadoop.hbase.monitoring.MonitoredTask$State.values()", "public static org.apache.hadoop.hbase.monitoring.MonitoredTask$State[] values()"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask$State", "org.apache.hadoop.hbase.monitoring.MonitoredTask$State.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.monitoring.MonitoredTask$State valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl()", "public org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl clone()", "public synchronized org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl clone()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getStartTime()", "public long getStartTime()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getDescription()", "public java.lang.String getDescription()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getStatus()", "public java.lang.String getStatus()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getStatusTime()", "public long getStatusTime()"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask$State", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getState()", "public org.apache.hadoop.hbase.monitoring.MonitoredTask$State getState()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getStateTime()", "public long getStateTime()"], ["long", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.getCompletionTimestamp()", "public long getCompletionTimestamp()"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.markComplete(java.lang.String)", "public void markComplete(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.pause(java.lang.String)", "public void pause(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.resume(java.lang.String)", "public void resume(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.abort(java.lang.String)", "public void abort(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.setStatus(java.lang.String)", "public void setStatus(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.setDescription(java.lang.String)", "public void setDescription(java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.cleanup()", "public void cleanup()"], ["void", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.expireNow()", "public void expireNow()"], ["java.util.Map<java.lang.String, java.lang.Object>", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.toMap()", "public java.util.Map<java.lang.String, java.lang.Object> toMap()"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.toJSON()", "public java.lang.String toJSON() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.toString()", "public java.lang.String toString()"], ["java.lang.Object", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.clone()", "public java.lang.Object clone() throws java.lang.CloneNotSupportedException"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask", "org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.clone()", "public org.apache.hadoop.hbase.monitoring.MonitoredTask clone()"], ["org.apache.hadoop.hbase.monitoring.StateDumpServlet", "org.apache.hadoop.hbase.monitoring.StateDumpServlet()", "public org.apache.hadoop.hbase.monitoring.StateDumpServlet()"], ["org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler", "org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler(T)", "public org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler(T)"], ["java.lang.Object", "org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler.invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[])", "public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable"], ["org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair", "org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair(org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair(org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.monitoring.MonitoredTask)"], ["org.apache.hadoop.hbase.monitoring.MonitoredTask", "org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair.get()", "public org.apache.hadoop.hbase.monitoring.MonitoredTask get()"], ["boolean", "org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair.isDead()", "public boolean isDead()"], ["org.apache.hadoop.hbase.monitoring.TaskMonitor", "org.apache.hadoop.hbase.monitoring.TaskMonitor()", "public org.apache.hadoop.hbase.monitoring.TaskMonitor()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.TaskMonitor.org.apache.hadoop.hbase.monitoring.TaskMonitor get()", "public static synchronized org.apache.hadoop.hbase.monitoring.TaskMonitor get()"], ["synchronized", "org.apache.hadoop.hbase.monitoring.TaskMonitor.org.apache.hadoop.hbase.monitoring.MonitoredTask createStatus(java.lang.String)", "public synchronized org.apache.hadoop.hbase.monitoring.MonitoredTask createStatus(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.monitoring.TaskMonitor.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler createRPCStatus(java.lang.String)", "public synchronized org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler createRPCStatus(java.lang.String)"], ["java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask>", "org.apache.hadoop.hbase.monitoring.TaskMonitor.getTasks()", "public synchronized java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask> getTasks()"], ["void", "org.apache.hadoop.hbase.monitoring.TaskMonitor.dumpAsText(java.io.PrintWriter)", "public void dumpAsText(java.io.PrintWriter)"], ["org.apache.hadoop.hbase.monitoring.ThreadMonitoring", "org.apache.hadoop.hbase.monitoring.ThreadMonitoring()", "public org.apache.hadoop.hbase.monitoring.ThreadMonitoring()"], ["java.lang.management.ThreadInfo", "org.apache.hadoop.hbase.monitoring.ThreadMonitoring.getThreadInfo(java.lang.Thread)", "public static java.lang.management.ThreadInfo getThreadInfo(java.lang.Thread)"], ["java.lang.String", "org.apache.hadoop.hbase.monitoring.ThreadMonitoring.formatThreadInfo(java.lang.management.ThreadInfo, java.lang.String)", "public static java.lang.String formatThreadInfo(java.lang.management.ThreadInfo, java.lang.String)"], ["void", "org.apache.hadoop.hbase.monitoring.ThreadMonitoring.appendThreadInfo(java.lang.StringBuilder, java.lang.management.ThreadInfo, java.lang.String)", "public static void appendThreadInfo(java.lang.StringBuilder, java.lang.management.ThreadInfo, java.lang.String)"], ["org.apache.hadoop.hbase.namespace.NamespaceAuditor", "org.apache.hadoop.hbase.namespace.NamespaceAuditor(org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.namespace.NamespaceAuditor(org.apache.hadoop.hbase.master.MasterServices)"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.start()", "public void start() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.checkQuotaToCreateTable(org.apache.hadoop.hbase.TableName, int)", "public void checkQuotaToCreateTable(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.checkQuotaToUpdateRegion(org.apache.hadoop.hbase.TableName, int)", "public void checkQuotaToUpdateRegion(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.checkQuotaToSplitRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void checkQuotaToSplitRegion(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.updateQuotaForRegionMerge(org.apache.hadoop.hbase.HRegionInfo)", "public void updateQuotaForRegionMerge(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.addNamespace(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void addNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.deleteNamespace(java.lang.String)", "public void deleteNamespace(java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.removeFromNamespaceUsage(org.apache.hadoop.hbase.TableName)", "public void removeFromNamespaceUsage(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.removeRegionFromNamespaceUsage(org.apache.hadoop.hbase.HRegionInfo)", "public void removeRegionFromNamespaceUsage(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.namespace.NamespaceAuditor.isInitialized()", "public boolean isInitialized()"], ["org.apache.hadoop.hbase.namespace.NamespaceStateManager", "org.apache.hadoop.hbase.namespace.NamespaceStateManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.namespace.NamespaceStateManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceStateManager.start()", "public void start() throws java.io.IOException"], ["org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo", "org.apache.hadoop.hbase.namespace.NamespaceStateManager.getState(java.lang.String)", "public org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo getState(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.namespace.NamespaceStateManager.void removeRegionFromTable(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized void removeRegionFromTable(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceStateManager.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.namespace.NamespaceStateManager.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo", "org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo(java.lang.String)", "public org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.procedure.MasterProcedureManager", "org.apache.hadoop.hbase.procedure.MasterProcedureManager()", "public org.apache.hadoop.hbase.procedure.MasterProcedureManager()"], ["void", "org.apache.hadoop.hbase.procedure.MasterProcedureManager.execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public void execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.procedure.MasterProcedureManager.execProcedureWithRet(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public byte[] execProcedureWithRet(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost()", "public org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost()"], ["void", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost.loadProcedures(org.apache.hadoop.conf.Configuration)", "public void loadProcedures(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost.initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster)", "public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException"], ["void", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost.stop(java.lang.String)", "public void stop(java.lang.String)"], ["org.apache.hadoop.hbase.procedure.MasterProcedureManager", "org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost.getProcedureManager(java.lang.String)", "public org.apache.hadoop.hbase.procedure.MasterProcedureManager getProcedureManager(java.lang.String)"], ["org.apache.hadoop.hbase.procedure.Procedure", "org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.lang.String, byte[], java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.lang.String, byte[], java.util.List<java.lang.String>)"], ["org.apache.hadoop.hbase.procedure.Procedure", "org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, long, long, java.lang.String, byte[], java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, long, long, java.lang.String, byte[], java.util.List<java.lang.String>)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.Procedure.getName()", "public java.lang.String getName()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.Procedure.getStatus()", "public java.lang.String getStatus()"], ["org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher", "org.apache.hadoop.hbase.procedure.Procedure.getErrorMonitor()", "public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher getErrorMonitor()"], ["java.lang.Void", "org.apache.hadoop.hbase.procedure.Procedure.call()", "public final java.lang.Void call()"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.sendGlobalBarrierStart()", "public void sendGlobalBarrierStart() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.sendGlobalBarrierReached()", "public void sendGlobalBarrierReached() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.sendGlobalBarrierComplete()", "public void sendGlobalBarrierComplete()"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.barrierAcquiredByMember(java.lang.String)", "public void barrierAcquiredByMember(java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.barrierReleasedByMember(java.lang.String, byte[])", "public void barrierReleasedByMember(java.lang.String, byte[])"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.waitForCompleted()", "public void waitForCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException"], ["java.util.HashMap<java.lang.String, byte[]>", "org.apache.hadoop.hbase.procedure.Procedure.waitForCompletedWithRet()", "public java.util.HashMap<java.lang.String, byte[]> waitForCompletedWithRet() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.procedure.Procedure.isCompleted()", "public boolean isCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.receive(org.apache.hadoop.hbase.errorhandling.ForeignException)", "public void receive(org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["void", "org.apache.hadoop.hbase.procedure.Procedure.waitForLatch(java.util.concurrent.CountDownLatch, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare, long, java.lang.String)", "public static void waitForLatch(java.util.concurrent.CountDownLatch, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare, long, java.lang.String) throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.procedure.Procedure.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.procedure.ProcedureCoordinator", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor)", "public org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor)"], ["org.apache.hadoop.hbase.procedure.ProcedureCoordinator", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor, long, long)", "public org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor, long, long)"], ["java.util.concurrent.ThreadPoolExecutor", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.defaultPool(java.lang.String, int)", "public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int)"], ["java.util.concurrent.ThreadPoolExecutor", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.defaultPool(java.lang.String, int, long)", "public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int, long)"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.abortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException)", "public void abortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["org.apache.hadoop.hbase.procedure.Procedure", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.startProcedure(org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, java.lang.String, byte[], java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.procedure.Procedure startProcedure(org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, java.lang.String, byte[], java.util.List<java.lang.String>)"], ["org.apache.hadoop.hbase.procedure.Procedure", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.getProcedure(java.lang.String)", "public org.apache.hadoop.hbase.procedure.Procedure getProcedure(java.lang.String)"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.procedure.ProcedureCoordinator.getProcedureNames()", "public java.util.Set<java.lang.String> getProcedureNames()"], ["org.apache.hadoop.hbase.procedure.ProcedureManager", "org.apache.hadoop.hbase.procedure.ProcedureManager()", "public org.apache.hadoop.hbase.procedure.ProcedureManager()"], ["boolean", "org.apache.hadoop.hbase.procedure.ProcedureManager.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.procedure.ProcedureManager.hashCode()", "public int hashCode()"], ["org.apache.hadoop.hbase.procedure.ProcedureManagerHost", "org.apache.hadoop.hbase.procedure.ProcedureManagerHost()", "public org.apache.hadoop.hbase.procedure.ProcedureManagerHost()"], ["E", "org.apache.hadoop.hbase.procedure.ProcedureManagerHost.loadInstance(java.lang.Class<?>)", "public E loadInstance(java.lang.Class<?>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureManagerHost.register(E)", "public void register(E)"], ["java.util.Set<E>", "org.apache.hadoop.hbase.procedure.ProcedureManagerHost.getProcedureManagers()", "public java.util.Set<E> getProcedureManagers()"], ["org.apache.hadoop.hbase.procedure.ProcedureMember", "org.apache.hadoop.hbase.procedure.ProcedureMember(org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs, java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.hbase.procedure.SubprocedureFactory)", "public org.apache.hadoop.hbase.procedure.ProcedureMember(org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs, java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.hbase.procedure.SubprocedureFactory)"], ["java.util.concurrent.ThreadPoolExecutor", "org.apache.hadoop.hbase.procedure.ProcedureMember.defaultPool(java.lang.String, int)", "public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int)"], ["java.util.concurrent.ThreadPoolExecutor", "org.apache.hadoop.hbase.procedure.ProcedureMember.defaultPool(java.lang.String, int, long)", "public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int, long)"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.procedure.ProcedureMember.createSubprocedure(java.lang.String, byte[])", "public org.apache.hadoop.hbase.procedure.Subprocedure createSubprocedure(java.lang.String, byte[])"], ["boolean", "org.apache.hadoop.hbase.procedure.ProcedureMember.submitSubprocedure(org.apache.hadoop.hbase.procedure.Subprocedure)", "public boolean submitSubprocedure(org.apache.hadoop.hbase.procedure.Subprocedure)"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureMember.receivedReachedGlobalBarrier(java.lang.String)", "public void receivedReachedGlobalBarrier(java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureMember.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureMember.controllerConnectionFailure(java.lang.String, java.lang.Throwable, java.lang.String)", "public void controllerConnectionFailure(java.lang.String, java.lang.Throwable, java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.ProcedureMember.receiveAbortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException)", "public void receiveAbortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["org.apache.hadoop.hbase.procedure.RegionServerProcedureManager", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManager()", "public org.apache.hadoop.hbase.procedure.RegionServerProcedureManager()"], ["org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost()", "public org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost()"], ["void", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost.initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost.stop(boolean)", "public void stop(boolean)"], ["void", "org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost.loadProcedures(org.apache.hadoop.conf.Configuration)", "public void loadProcedures(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure$1.receive(org.apache.hadoop.hbase.errorhandling.ForeignException)", "public void receive(org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl", "org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long)", "public org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long)"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl.acquireBarrier()", "public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["byte[]", "org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl.insideBarrier()", "public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl.cleanup(java.lang.Exception)", "public void cleanup(java.lang.Exception)"], ["java.lang.Object", "org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.procedure.Subprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long)", "public org.apache.hadoop.hbase.procedure.Subprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.Subprocedure.getName()", "public java.lang.String getName()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.Subprocedure.getMemberName()", "public java.lang.String getMemberName()"], ["java.lang.Void", "org.apache.hadoop.hbase.procedure.Subprocedure.call()", "public final java.lang.Void call()"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure.cancel(java.lang.String, java.lang.Throwable)", "public void cancel(java.lang.String, java.lang.Throwable)"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure.receiveReachedGlobalBarrier()", "public void receiveReachedGlobalBarrier()"], ["void", "org.apache.hadoop.hbase.procedure.Subprocedure.waitForLocallyCompleted()", "public void waitForLocallyCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.procedure.Subprocedure.call()", "public java.lang.Object call() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs$1.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.lang.String) throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.sendGlobalBarrierAcquire(org.apache.hadoop.hbase.procedure.Procedure, byte[], java.util.List<java.lang.String>)", "public final void sendGlobalBarrierAcquire(org.apache.hadoop.hbase.procedure.Procedure, byte[], java.util.List<java.lang.String>) throws java.io.IOException, java.lang.IllegalArgumentException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.sendGlobalBarrierReached(org.apache.hadoop.hbase.procedure.Procedure, java.util.List<java.lang.String>)", "public void sendGlobalBarrierReached(org.apache.hadoop.hbase.procedure.Procedure, java.util.List<java.lang.String>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.resetMembers(org.apache.hadoop.hbase.procedure.Procedure)", "public final void resetMembers(org.apache.hadoop.hbase.procedure.Procedure) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.start(org.apache.hadoop.hbase.procedure.ProcedureCoordinator)", "public final boolean start(org.apache.hadoop.hbase.procedure.ProcedureCoordinator)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.sendAbortToMembers(org.apache.hadoop.hbase.procedure.Procedure, org.apache.hadoop.hbase.errorhandling.ForeignException)", "public final void sendAbortToMembers(org.apache.hadoop.hbase.procedure.Procedure, org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.close()", "public final void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs$1.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs$1.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String) throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.procedure.ZKProcedureUtil", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.getZkController()", "public org.apache.hadoop.hbase.procedure.ZKProcedureUtil getZkController()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.getMemberName()", "public java.lang.String getMemberName()"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.sendMemberAcquired(org.apache.hadoop.hbase.procedure.Subprocedure)", "public void sendMemberAcquired(org.apache.hadoop.hbase.procedure.Subprocedure) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.sendMemberCompleted(org.apache.hadoop.hbase.procedure.Subprocedure, byte[])", "public void sendMemberCompleted(org.apache.hadoop.hbase.procedure.Subprocedure, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.sendMemberAborted(org.apache.hadoop.hbase.procedure.Subprocedure, org.apache.hadoop.hbase.errorhandling.ForeignException)", "public void sendMemberAborted(org.apache.hadoop.hbase.procedure.Subprocedure, org.apache.hadoop.hbase.errorhandling.ForeignException)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.start(java.lang.String, org.apache.hadoop.hbase.procedure.ProcedureMember)", "public void start(java.lang.String, org.apache.hadoop.hbase.procedure.ProcedureMember)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.procedure.ZKProcedureUtil", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public org.apache.hadoop.hbase.procedure.ZKProcedureUtil(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String) throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.close()", "public void close() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAcquiredBarrierNode(java.lang.String)", "public java.lang.String getAcquiredBarrierNode(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getReachedBarrierNode(java.lang.String)", "public java.lang.String getReachedBarrierNode(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAbortZNode(java.lang.String)", "public java.lang.String getAbortZNode(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAbortZnode()", "public java.lang.String getAbortZnode()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getBaseZnode()", "public java.lang.String getBaseZnode()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAcquiredBarrier()", "public java.lang.String getAcquiredBarrier()"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAcquireBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)", "public static java.lang.String getAcquireBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getReachedBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)", "public static java.lang.String getReachedBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getAbortNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)", "public static java.lang.String getAbortNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String)"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.getWatcher()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher getWatcher()"], ["boolean", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.isAbortPathNode(java.lang.String)", "public boolean isAbortPathNode(java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.clearChildZNodes()", "public void clearChildZNodes() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.procedure.ZKProcedureUtil.clearZNodes(java.lang.String)", "public void clearZNodes(java.lang.String) throws org.apache.zookeeper.KeeperException"], ["java.lang.Void", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure$RegionFlushTask.call()", "public java.lang.Void call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure$RegionFlushTask.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region>, java.lang.String, org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedurePool)", "public org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region>, java.lang.String, org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedurePool)"], ["void", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.acquireBarrier()", "public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["byte[]", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.insideBarrier()", "public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.cleanup(java.lang.Exception)", "public void cleanup(java.lang.Exception)"], ["void", "org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.releaseBarrier()", "public void releaseBarrier()"], ["org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager()", "public org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager()"], ["void", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster)", "public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.getProcedureSignature()", "public java.lang.String getProcedureSignature()"], ["void", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public void execProcedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager.boolean isProcedureDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription)", "public synchronized boolean isProcedureDone(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException"], ["org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder(org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager)", "public org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder(org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager)"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder.buildSubprocedure(java.lang.String, byte[])", "public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, byte[])"], ["org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager()", "public org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager()"], ["void", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.stop(boolean)", "public void stop(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.buildSubprocedure(java.lang.String)", "public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String)"], ["void", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException"], ["java.lang.String", "org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.getProcedureSignature()", "public java.lang.String getProcedureSignature()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil$1.current()", "public org.apache.hadoop.hbase.Cell current()"], ["boolean", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil$1.advance()", "public boolean advance()"], ["long", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil$1.heapSize()", "public long heapSize()"], ["org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil()", "public org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil()"], ["void", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil.replicateWALEntry(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface, org.apache.hadoop.hbase.wal.WAL$Entry[])", "public static void replicateWALEntry(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface, org.apache.hadoop.hbase.wal.WAL$Entry[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner>", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil.buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[])", "public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner> buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[])"], ["org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner>", "org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil.buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[], byte[])", "public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner> buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[], byte[])"], ["org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter()", "public org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter()"], ["long", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.refill(long)", "public long refill(long)"], ["long", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.getWaitInterval(long, long, long)", "public long getWaitInterval(long, long, long)"], ["void", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.setNextRefillTime(long)", "public void setNextRefillTime(long)"], ["long", "org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.getNextRefillTime()", "public long getNextRefillTime()"], ["org.apache.hadoop.hbase.quotas.DefaultOperationQuota", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota(org.apache.hadoop.hbase.quotas.QuotaLimiter...)", "public org.apache.hadoop.hbase.quotas.DefaultOperationQuota(org.apache.hadoop.hbase.quotas.QuotaLimiter...)"], ["org.apache.hadoop.hbase.quotas.DefaultOperationQuota", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota(java.util.List<org.apache.hadoop.hbase.quotas.QuotaLimiter>)", "public org.apache.hadoop.hbase.quotas.DefaultOperationQuota(java.util.List<org.apache.hadoop.hbase.quotas.QuotaLimiter>)"], ["void", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.checkQuota(int, int, int)", "public void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.ThrottlingException"], ["void", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.close()", "public void close()"], ["long", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.getReadAvailable()", "public long getReadAvailable()"], ["long", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.getWriteAvailable()", "public long getWriteAvailable()"], ["void", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.addGetResult(org.apache.hadoop.hbase.client.Result)", "public void addGetResult(org.apache.hadoop.hbase.client.Result)"], ["void", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)", "public void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)"], ["void", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.addMutation(org.apache.hadoop.hbase.client.Mutation)", "public void addMutation(org.apache.hadoop.hbase.client.Mutation)"], ["long", "org.apache.hadoop.hbase.quotas.DefaultOperationQuota.getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter()", "public org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter()"], ["long", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.refill(long)", "public long refill(long)"], ["long", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.getWaitInterval(long, long, long)", "public long getWaitInterval(long, long, long)"], ["void", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.setNextRefillTime(long)", "public void setNextRefillTime(long)"], ["long", "org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.getNextRefillTime()", "public long getNextRefillTime()"], ["org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$1.fetch()", "public org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas fetch() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$1.update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$1.delete()", "public void delete() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$1.preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$1.postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$2.fetch()", "public org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas fetch() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$2.update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$2.delete()", "public void delete() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$2.preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$2.postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$3.fetch()", "public org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas fetch() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$3.update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$3.delete()", "public void delete() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$3.preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$3.postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.fetch()", "public org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas fetch() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.delete()", "public void delete() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$5.fetch()", "public org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas fetch() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$5.update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void update(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$5.delete()", "public void delete() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$5.preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$5.postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void postApply(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$NamedLock.lock(T)", "public void lock(T) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager$NamedLock.unlock(T)", "public void unlock(T)"], ["org.apache.hadoop.hbase.quotas.MasterQuotaManager", "org.apache.hadoop.hbase.quotas.MasterQuotaManager(org.apache.hadoop.hbase.master.MasterServices)", "public org.apache.hadoop.hbase.quotas.MasterQuotaManager(org.apache.hadoop.hbase.master.MasterServices)"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.start()", "public void start() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.stop()", "public void stop()"], ["boolean", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.isQuotaEnabled()", "public boolean isQuotaEnabled()"], ["org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaResponse", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setQuota(org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaResponse setQuota(org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public void setUserQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public void setUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public void setUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public void setTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest)", "public void setNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.setNamespaceQuota(org.apache.hadoop.hbase.NamespaceDescriptor)", "public void setNamespaceQuota(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.removeNamespaceQuota(java.lang.String)", "public void removeNamespaceQuota(java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.checkNamespaceTableAndRegionQuota(org.apache.hadoop.hbase.TableName, int)", "public void checkNamespaceTableAndRegionQuota(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.checkAndUpdateNamespaceRegionQuota(org.apache.hadoop.hbase.TableName, int)", "public void checkAndUpdateNamespaceRegionQuota(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.onRegionMerged(org.apache.hadoop.hbase.HRegionInfo)", "public void onRegionMerged(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.onRegionSplit(org.apache.hadoop.hbase.HRegionInfo)", "public void onRegionSplit(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.removeTableFromNamespaceQuota(org.apache.hadoop.hbase.TableName)", "public void removeTableFromNamespaceQuota(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.namespace.NamespaceAuditor", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.getNamespaceQuotaManager()", "public org.apache.hadoop.hbase.namespace.NamespaceAuditor getNamespaceQuotaManager()"], ["void", "org.apache.hadoop.hbase.quotas.MasterQuotaManager.onRegionSplitReverted(org.apache.hadoop.hbase.HRegionInfo)", "public void onRegionSplitReverted(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.quotas.OperationQuota", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.get()", "public static org.apache.hadoop.hbase.quotas.OperationQuota get()"], ["void", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.checkQuota(int, int, int)", "public void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.ThrottlingException"], ["void", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.close()", "public void close()"], ["void", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.addGetResult(org.apache.hadoop.hbase.client.Result)", "public void addGetResult(org.apache.hadoop.hbase.client.Result)"], ["void", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)", "public void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)"], ["void", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.addMutation(org.apache.hadoop.hbase.client.Mutation)", "public void addMutation(org.apache.hadoop.hbase.client.Mutation)"], ["long", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.getReadAvailable()", "public long getReadAvailable()"], ["long", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.getWriteAvailable()", "public long getWriteAvailable()"], ["long", "org.apache.hadoop.hbase.quotas.NoopOperationQuota.getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["void", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.checkQuota(long, long)", "public void checkQuota(long, long) throws org.apache.hadoop.hbase.quotas.ThrottlingException"], ["void", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.grabQuota(long, long)", "public void grabQuota(long, long)"], ["void", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.consumeWrite(long)", "public void consumeWrite(long)"], ["void", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.consumeRead(long)", "public void consumeRead(long)"], ["boolean", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.isBypass()", "public boolean isBypass()"], ["long", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.getWriteAvailable()", "public long getWriteAvailable()"], ["long", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.getReadAvailable()", "public long getReadAvailable()"], ["void", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)", "public void addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)"], ["long", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["java.lang.String", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.NoopQuotaLimiter.get()", "public static org.apache.hadoop.hbase.quotas.QuotaLimiter get()"], ["org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize()", "public org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize()"], ["void", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)", "public void addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)"], ["long", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["long", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.getOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["void", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.addGetResult(org.apache.hadoop.hbase.client.Result)", "public void addGetResult(org.apache.hadoop.hbase.client.Result)"], ["void", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)", "public void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>)"], ["void", "org.apache.hadoop.hbase.quotas.OperationQuota$AvgOperationSize.addMutation(org.apache.hadoop.hbase.client.Mutation)", "public void addMutation(org.apache.hadoop.hbase.client.Mutation)"], ["org.apache.hadoop.hbase.quotas.OperationQuota$OperationType[]", "org.apache.hadoop.hbase.quotas.OperationQuota$OperationType.values()", "public static org.apache.hadoop.hbase.quotas.OperationQuota$OperationType[] values()"], ["org.apache.hadoop.hbase.quotas.OperationQuota$OperationType", "org.apache.hadoop.hbase.quotas.OperationQuota$OperationType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.quotas.OperationQuota$OperationType valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.client.Get", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$1.makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>)", "public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>)"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$1.fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Get", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$2.makeGet(java.util.Map$Entry<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState>)", "public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState>)"], ["java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState>", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$2.fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Get", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$3.makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState>)", "public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState>)"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState>", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$3.fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>)", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore", "org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore(org.apache.hadoop.hbase.quotas.QuotaCache, int, org.apache.hadoop.hbase.Stoppable)", "public org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore(org.apache.hadoop.hbase.quotas.QuotaCache, int, org.apache.hadoop.hbase.Stoppable)"], ["org.apache.hadoop.hbase.quotas.QuotaCache", "org.apache.hadoop.hbase.quotas.QuotaCache(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.quotas.QuotaCache(org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["void", "org.apache.hadoop.hbase.quotas.QuotaCache.start()", "public void start() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaCache.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.quotas.QuotaCache.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.QuotaCache.getUserLimiter(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.quotas.QuotaLimiter getUserLimiter(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.quotas.UserQuotaState", "org.apache.hadoop.hbase.quotas.QuotaCache.getUserQuotaState(org.apache.hadoop.security.UserGroupInformation)", "public org.apache.hadoop.hbase.quotas.UserQuotaState getUserQuotaState(org.apache.hadoop.security.UserGroupInformation)"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.QuotaCache.getTableLimiter(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.quotas.QuotaLimiter getTableLimiter(org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.QuotaCache.getNamespaceLimiter(java.lang.String)", "public org.apache.hadoop.hbase.quotas.QuotaLimiter getNamespaceLimiter(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.quotas.QuotaCache.isTEST_FORCE_REFRESH()", "public static boolean isTEST_FORCE_REFRESH()"], ["void", "org.apache.hadoop.hbase.quotas.QuotaCache.setTEST_FORCE_REFRESH(boolean)", "public static void setTEST_FORCE_REFRESH(boolean)"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.QuotaLimiterFactory.fromThrottle(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Throttle)", "public static org.apache.hadoop.hbase.quotas.QuotaLimiter fromThrottle(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Throttle)"], ["org.apache.hadoop.hbase.quotas.QuotaLimiter", "org.apache.hadoop.hbase.quotas.QuotaLimiterFactory.update(org.apache.hadoop.hbase.quotas.QuotaLimiter, org.apache.hadoop.hbase.quotas.QuotaLimiter)", "public static org.apache.hadoop.hbase.quotas.QuotaLimiter update(org.apache.hadoop.hbase.quotas.QuotaLimiter, org.apache.hadoop.hbase.quotas.QuotaLimiter)"], ["org.apache.hadoop.hbase.quotas.QuotaState", "org.apache.hadoop.hbase.quotas.QuotaState()", "public org.apache.hadoop.hbase.quotas.QuotaState()"], ["org.apache.hadoop.hbase.quotas.QuotaState", "org.apache.hadoop.hbase.quotas.QuotaState(long)", "public org.apache.hadoop.hbase.quotas.QuotaState(long)"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.long getLastUpdate()", "public synchronized long getLastUpdate()"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.long getLastQuery()", "public synchronized long getLastQuery()"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.boolean isBypass()", "public synchronized boolean isBypass()"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.void setQuotas(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public synchronized void setQuotas(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.void update(org.apache.hadoop.hbase.quotas.QuotaState)", "public synchronized void update(org.apache.hadoop.hbase.quotas.QuotaState)"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.org.apache.hadoop.hbase.quotas.QuotaLimiter getGlobalLimiter()", "public synchronized org.apache.hadoop.hbase.quotas.QuotaLimiter getGlobalLimiter()"], ["synchronized", "org.apache.hadoop.hbase.quotas.QuotaState.void setLastQuery(long)", "public synchronized void setLastQuery(long)"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil$1.visitUserQuotas(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void visitUserQuotas(java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil$1.visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil$1.visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.quotas.QuotaUtil$2.getKeyFromRow(byte[])", "public org.apache.hadoop.hbase.TableName getKeyFromRow(byte[])"], ["java.lang.Object", "org.apache.hadoop.hbase.quotas.QuotaUtil$2.getKeyFromRow(byte[])", "public java.lang.Object getKeyFromRow(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.quotas.QuotaUtil$3.getKeyFromRow(byte[])", "public java.lang.String getKeyFromRow(byte[])"], ["java.lang.Object", "org.apache.hadoop.hbase.quotas.QuotaUtil$3.getKeyFromRow(byte[])", "public java.lang.Object getKeyFromRow(byte[])"], ["org.apache.hadoop.hbase.quotas.QuotaUtil", "org.apache.hadoop.hbase.quotas.QuotaUtil()", "public org.apache.hadoop.hbase.quotas.QuotaUtil()"], ["boolean", "org.apache.hadoop.hbase.quotas.QuotaUtil.isQuotaEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isQuotaEnabled(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.addTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public static void addTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.deleteTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName)", "public static void deleteTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.addNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public static void addNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.deleteNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String)", "public static void deleteNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String)", "public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName)", "public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.QuotaUtil.deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String)", "public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState>", "org.apache.hadoop.hbase.quotas.QuotaUtil.fetchUserQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>)", "public static java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState> fetchUserQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState>", "org.apache.hadoop.hbase.quotas.QuotaUtil.fetchTableQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>)", "public static java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState> fetchTableQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>", "org.apache.hadoop.hbase.quotas.QuotaUtil.fetchNamespaceQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>)", "public static java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchNamespaceQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException"], ["<K> java.util.Map<K, org.apache.hadoop.hbase.quotas.QuotaState>", "org.apache.hadoop.hbase.quotas.QuotaUtil.fetchGlobalQuotas(java.lang.String, org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<K>)", "public static <K> java.util.Map<K, org.apache.hadoop.hbase.quotas.QuotaState> fetchGlobalQuotas(java.lang.String, org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<K>) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.quotas.QuotaUtil.calculateMutationSize(org.apache.hadoop.hbase.client.Mutation)", "public static long calculateMutationSize(org.apache.hadoop.hbase.client.Mutation)"], ["long", "org.apache.hadoop.hbase.quotas.QuotaUtil.calculateResultSize(org.apache.hadoop.hbase.client.Result)", "public static long calculateResultSize(org.apache.hadoop.hbase.client.Result)"], ["long", "org.apache.hadoop.hbase.quotas.QuotaUtil.calculateResultSize(java.util.List<org.apache.hadoop.hbase.client.Result>)", "public static long calculateResultSize(java.util.List<org.apache.hadoop.hbase.client.Result>)"], ["org.apache.hadoop.hbase.quotas.RateLimiter", "org.apache.hadoop.hbase.quotas.RateLimiter()", "public org.apache.hadoop.hbase.quotas.RateLimiter()"], ["void", "org.apache.hadoop.hbase.quotas.RateLimiter.set(long, java.util.concurrent.TimeUnit)", "public void set(long, java.util.concurrent.TimeUnit)"], ["java.lang.String", "org.apache.hadoop.hbase.quotas.RateLimiter.toString()", "public java.lang.String toString()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.void update(org.apache.hadoop.hbase.quotas.RateLimiter)", "public synchronized void update(org.apache.hadoop.hbase.quotas.RateLimiter)"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.boolean isBypass()", "public synchronized boolean isBypass()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.long getLimit()", "public synchronized long getLimit()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.long getAvailable()", "public synchronized long getAvailable()"], ["boolean", "org.apache.hadoop.hbase.quotas.RateLimiter.canExecute()", "public boolean canExecute()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.boolean canExecute(long)", "public synchronized boolean canExecute(long)"], ["void", "org.apache.hadoop.hbase.quotas.RateLimiter.consume()", "public void consume()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.void consume(long)", "public synchronized void consume(long)"], ["long", "org.apache.hadoop.hbase.quotas.RateLimiter.waitInterval()", "public long waitInterval()"], ["synchronized", "org.apache.hadoop.hbase.quotas.RateLimiter.long waitInterval(long)", "public synchronized long waitInterval(long)"], ["void", "org.apache.hadoop.hbase.quotas.RateLimiter.setNextRefillTime(long)", "public void setNextRefillTime(long)"], ["long", "org.apache.hadoop.hbase.quotas.RateLimiter.getNextRefillTime()", "public long getNextRefillTime()"], ["org.apache.hadoop.hbase.quotas.RegionServerQuotaManager", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.quotas.RegionServerQuotaManager(org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["void", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.start(org.apache.hadoop.hbase.ipc.RpcScheduler)", "public void start(org.apache.hadoop.hbase.ipc.RpcScheduler) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.stop()", "public void stop()"], ["boolean", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.isQuotaEnabled()", "public boolean isQuotaEnabled()"], ["org.apache.hadoop.hbase.quotas.OperationQuota", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.getQuota(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.quotas.OperationQuota getQuota(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.quotas.OperationQuota", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.checkQuota(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public org.apache.hadoop.hbase.quotas.OperationQuota checkQuota(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.quotas.OperationQuota$OperationType) throws java.io.IOException, org.apache.hadoop.hbase.quotas.ThrottlingException"], ["org.apache.hadoop.hbase.quotas.OperationQuota", "org.apache.hadoop.hbase.quotas.RegionServerQuotaManager.checkQuota(org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Action>)", "public org.apache.hadoop.hbase.quotas.OperationQuota checkQuota(org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClientProtos$Action>) throws java.io.IOException, org.apache.hadoop.hbase.quotas.ThrottlingException"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.update(org.apache.hadoop.hbase.quotas.TimeBasedLimiter)", "public void update(org.apache.hadoop.hbase.quotas.TimeBasedLimiter)"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.checkQuota(long, long)", "public void checkQuota(long, long) throws org.apache.hadoop.hbase.quotas.ThrottlingException"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.grabQuota(long, long)", "public void grabQuota(long, long)"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.consumeWrite(long)", "public void consumeWrite(long)"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.consumeRead(long)", "public void consumeRead(long)"], ["boolean", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.isBypass()", "public boolean isBypass()"], ["long", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.getWriteAvailable()", "public long getWriteAvailable()"], ["long", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.getReadAvailable()", "public long getReadAvailable()"], ["void", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)", "public void addOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType, long)"], ["long", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)", "public long getAvgOperationSize(org.apache.hadoop.hbase.quotas.OperationQuota$OperationType)"], ["java.lang.String", "org.apache.hadoop.hbase.quotas.TimeBasedLimiter.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.quotas.UserQuotaState", "org.apache.hadoop.hbase.quotas.UserQuotaState()", "public org.apache.hadoop.hbase.quotas.UserQuotaState()"], ["org.apache.hadoop.hbase.quotas.UserQuotaState", "org.apache.hadoop.hbase.quotas.UserQuotaState(long)", "public org.apache.hadoop.hbase.quotas.UserQuotaState(long)"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.boolean isBypass()", "public synchronized boolean isBypass()"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.boolean hasBypassGlobals()", "public synchronized boolean hasBypassGlobals()"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.void setQuotas(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public synchronized void setQuotas(org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.void setQuotas(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public synchronized void setQuotas(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.void setQuotas(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public synchronized void setQuotas(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.void update(org.apache.hadoop.hbase.quotas.QuotaState)", "public synchronized void update(org.apache.hadoop.hbase.quotas.QuotaState)"], ["synchronized", "org.apache.hadoop.hbase.quotas.UserQuotaState.org.apache.hadoop.hbase.quotas.QuotaLimiter getTableLimiter(org.apache.hadoop.hbase.TableName)", "public synchronized org.apache.hadoop.hbase.quotas.QuotaLimiter getTableLimiter(org.apache.hadoop.hbase.TableName)"], ["int", "org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.getPriority(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)", "public int getPriority(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)"], ["long", "org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.getDeadline(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)", "public long getDeadline(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)"], ["org.apache.hadoop.hbase.regionserver.BaseRowProcessor", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor()", "public org.apache.hadoop.hbase.regionserver.BaseRowProcessor()"], ["void", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.preProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.preBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.postBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion)", "public void postBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.postProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit, boolean)", "public void postProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit, boolean) throws java.io.IOException"], ["java.util.List<java.util.UUID>", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.getClusterIds()", "public java.util.List<java.util.UUID> getClusterIds()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.getName()", "public java.lang.String getName()"], ["org.apache.hadoop.hbase.client.Durability", "org.apache.hadoop.hbase.regionserver.BaseRowProcessor.useDurability()", "public org.apache.hadoop.hbase.client.Durability useDurability()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.ceiling(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell ceiling(org.apache.hadoop.hbase.Cell)"], ["java.util.Iterator<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.descendingIterator()", "public java.util.Iterator<org.apache.hadoop.hbase.Cell> descendingIterator()"], ["java.util.NavigableSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.descendingSet()", "public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> descendingSet()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.floor(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell floor(org.apache.hadoop.hbase.Cell)"], ["java.util.SortedSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.headSet(org.apache.hadoop.hbase.Cell)", "public java.util.SortedSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell)"], ["java.util.NavigableSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.headSet(org.apache.hadoop.hbase.Cell, boolean)", "public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell, boolean)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.higher(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell higher(org.apache.hadoop.hbase.Cell)"], ["java.util.Iterator<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.Cell> iterator()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.lower(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell lower(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.pollFirst()", "public org.apache.hadoop.hbase.Cell pollFirst()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.pollLast()", "public org.apache.hadoop.hbase.Cell pollLast()"], ["java.util.SortedSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.subSet(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public java.util.SortedSet<org.apache.hadoop.hbase.Cell> subSet(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["java.util.NavigableSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.subSet(org.apache.hadoop.hbase.Cell, boolean, org.apache.hadoop.hbase.Cell, boolean)", "public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> subSet(org.apache.hadoop.hbase.Cell, boolean, org.apache.hadoop.hbase.Cell, boolean)"], ["java.util.SortedSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.tailSet(org.apache.hadoop.hbase.Cell)", "public java.util.SortedSet<org.apache.hadoop.hbase.Cell> tailSet(org.apache.hadoop.hbase.Cell)"], ["java.util.NavigableSet<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.tailSet(org.apache.hadoop.hbase.Cell, boolean)", "public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> tailSet(org.apache.hadoop.hbase.Cell, boolean)"], ["java.util.Comparator<? super org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.comparator()", "public java.util.Comparator<? super org.apache.hadoop.hbase.Cell> comparator()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.first()", "public org.apache.hadoop.hbase.Cell first()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.last()", "public org.apache.hadoop.hbase.Cell last()"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.add(org.apache.hadoop.hbase.Cell)", "public boolean add(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.addAll(java.util.Collection<? extends org.apache.hadoop.hbase.Cell>)", "public boolean addAll(java.util.Collection<? extends org.apache.hadoop.hbase.Cell>)"], ["void", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.clear()", "public void clear()"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.contains(java.lang.Object)", "public boolean contains(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.containsAll(java.util.Collection<?>)", "public boolean containsAll(java.util.Collection<?>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.isEmpty()", "public boolean isEmpty()"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.remove(java.lang.Object)", "public boolean remove(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.removeAll(java.util.Collection<?>)", "public boolean removeAll(java.util.Collection<?>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.retainAll(java.util.Collection<?>)", "public boolean retainAll(java.util.Collection<?>)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.get(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell get(org.apache.hadoop.hbase.Cell)"], ["int", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.size()", "public int size()"], ["java.lang.Object[]", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.toArray()", "public java.lang.Object[] toArray()"], ["<T> T[]", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.toArray(T[])", "public <T> T[] toArray(T[])"], ["java.util.SortedSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.tailSet(java.lang.Object)", "public java.util.SortedSet tailSet(java.lang.Object)"], ["java.util.SortedSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.headSet(java.lang.Object)", "public java.util.SortedSet headSet(java.lang.Object)"], ["java.util.SortedSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.subSet(java.lang.Object, java.lang.Object)", "public java.util.SortedSet subSet(java.lang.Object, java.lang.Object)"], ["java.util.NavigableSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.tailSet(java.lang.Object, boolean)", "public java.util.NavigableSet tailSet(java.lang.Object, boolean)"], ["java.util.NavigableSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.headSet(java.lang.Object, boolean)", "public java.util.NavigableSet headSet(java.lang.Object, boolean)"], ["java.util.NavigableSet", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.subSet(java.lang.Object, boolean, java.lang.Object, boolean)", "public java.util.NavigableSet subSet(java.lang.Object, boolean, java.lang.Object, boolean)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.pollLast()", "public java.lang.Object pollLast()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.pollFirst()", "public java.lang.Object pollFirst()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.higher(java.lang.Object)", "public java.lang.Object higher(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.ceiling(java.lang.Object)", "public java.lang.Object ceiling(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.floor(java.lang.Object)", "public java.lang.Object floor(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.lower(java.lang.Object)", "public java.lang.Object lower(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.last()", "public java.lang.Object last()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.first()", "public java.lang.Object first()"], ["boolean", "org.apache.hadoop.hbase.regionserver.CellSkipListSet.add(java.lang.Object)", "public boolean add(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.ColumnCount", "org.apache.hadoop.hbase.regionserver.ColumnCount(byte[])", "public org.apache.hadoop.hbase.regionserver.ColumnCount(byte[])"], ["org.apache.hadoop.hbase.regionserver.ColumnCount", "org.apache.hadoop.hbase.regionserver.ColumnCount(byte[], int)", "public org.apache.hadoop.hbase.regionserver.ColumnCount(byte[], int)"], ["org.apache.hadoop.hbase.regionserver.ColumnCount", "org.apache.hadoop.hbase.regionserver.ColumnCount(byte[], int, int, int)", "public org.apache.hadoop.hbase.regionserver.ColumnCount(byte[], int, int, int)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.ColumnCount.getBuffer()", "public byte[] getBuffer()"], ["int", "org.apache.hadoop.hbase.regionserver.ColumnCount.getOffset()", "public int getOffset()"], ["int", "org.apache.hadoop.hbase.regionserver.ColumnCount.getLength()", "public int getLength()"], ["int", "org.apache.hadoop.hbase.regionserver.ColumnCount.decrement()", "public int decrement()"], ["int", "org.apache.hadoop.hbase.regionserver.ColumnCount.increment()", "public int increment()"], ["void", "org.apache.hadoop.hbase.regionserver.ColumnCount.setCount(int)", "public void setCount(int)"], ["java.lang.Thread", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$1.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["java.lang.Thread", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$2.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["java.lang.Thread", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$3.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["java.lang.Thread", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$4.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner(org.apache.hadoop.hbase.regionserver.CompactSplitThread, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, java.util.concurrent.ThreadPoolExecutor)", "public org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner(org.apache.hadoop.hbase.regionserver.CompactSplitThread, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, java.util.concurrent.ThreadPoolExecutor)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run()", "public void run()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.compareTo(org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner)", "public int compareTo(org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner)"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread$Rejection.rejectedExecution(java.lang.Runnable, java.util.concurrent.ThreadPoolExecutor)", "public void rejectedExecution(java.lang.Runnable, java.util.concurrent.ThreadPoolExecutor)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.dumpQueue()", "public java.lang.String dumpQueue()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.void requestRegionsMerge(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean, long)", "public synchronized void requestRegionsMerge(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean, long)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.boolean requestSplit(org.apache.hadoop.hbase.regionserver.Region)", "public synchronized boolean requestSplit(org.apache.hadoop.hbase.regionserver.Region)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.void requestSplit(org.apache.hadoop.hbase.regionserver.Region, byte[])", "public synchronized void requestSplit(org.apache.hadoop.hbase.regionserver.Region, byte[])"], ["java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest>", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String)", "public synchronized java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest> requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest>", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.Store>>)", "public synchronized java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest> requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.Store>>) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest requestCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public synchronized org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest requestCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest>", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String, int, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.Store>>)", "public synchronized java.util.List<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest> requestCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String, int, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.Store>>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest requestCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String)", "public synchronized void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.Region, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestSystemCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String)", "public void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Store, java.lang.String) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getCompactionQueueSize()", "public int getCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getLargeCompactionQueueSize()", "public int getLargeCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getSmallCompactionQueueSize()", "public int getSmallCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getSplitQueueSize()", "public int getSplitQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getRegionSplitLimit()", "public int getRegionSplitLimit()"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.onConfigurationChange(org.apache.hadoop.conf.Configuration)", "public void onConfigurationChange(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getLargeCompactionThreadNum()", "public int getLargeCompactionThreadNum()"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["void", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController", "org.apache.hadoop.hbase.regionserver.CompactSplitThread.getCompactionThroughputController()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController getCompactionThroughputController()"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionInputFormat.createInputFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.util.Set<org.apache.hadoop.fs.Path>)", "public static void createInputFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.util.Set<org.apache.hadoop.fs.Path>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionMapper.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context)"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionMapper.map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context)", "public void map(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context) throws java.lang.InterruptedException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker$1.getTempDir()", "public org.apache.hadoop.fs.Path getTempDir()"], ["org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker.compact(org.apache.hadoop.fs.Path, boolean, boolean)", "public void compact(org.apache.hadoop.fs.Path, boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.CompactionTool", "org.apache.hadoop.hbase.regionserver.CompactionTool()", "public org.apache.hadoop.hbase.regionserver.CompactionTool()"], ["int", "org.apache.hadoop.hbase.regionserver.CompactionTool.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.regionserver.CompactionTool.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult", "org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext)", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.boolean seek(org.apache.hadoop.hbase.Cell)", "public synchronized boolean seek(org.apache.hadoop.hbase.Cell)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.boolean reseek(org.apache.hadoop.hbase.Cell)", "public synchronized boolean reseek(org.apache.hadoop.hbase.Cell)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.org.apache.hadoop.hbase.Cell peek()", "public synchronized org.apache.hadoop.hbase.Cell peek()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.org.apache.hadoop.hbase.Cell next()", "public synchronized org.apache.hadoop.hbase.Cell next()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.void close()", "public synchronized void close()"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.getSequenceID()", "public long getSequenceID()"], ["boolean", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)", "public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.boolean backwardSeek(org.apache.hadoop.hbase.Cell)", "public synchronized boolean backwardSeek(org.apache.hadoop.hbase.Cell)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell)", "public synchronized boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.DefaultMemStore$MemStoreScanner.boolean seekToLastRow()", "public synchronized boolean seekToLastRow()"], ["org.apache.hadoop.hbase.regionserver.DefaultMemStore", "org.apache.hadoop.hbase.regionserver.DefaultMemStore()", "public org.apache.hadoop.hbase.regionserver.DefaultMemStore()"], ["org.apache.hadoop.hbase.regionserver.DefaultMemStore", "org.apache.hadoop.hbase.regionserver.DefaultMemStore(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.DefaultMemStore(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.regionserver.MemStoreSnapshot", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.snapshot()", "public org.apache.hadoop.hbase.regionserver.MemStoreSnapshot snapshot()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.clearSnapshot(long)", "public void clearSnapshot(long) throws org.apache.hadoop.hbase.regionserver.UnexpectedStateException"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.getFlushableSize()", "public long getFlushableSize()"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.getSnapshotSize()", "public long getSnapshotSize()"], ["org.apache.hadoop.hbase.util.Pair<java.lang.Long, org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.add(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.util.Pair<java.lang.Long, org.apache.hadoop.hbase.Cell> add(org.apache.hadoop.hbase.Cell)"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.timeOfOldestEdit()", "public long timeOfOldestEdit()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.rollback(org.apache.hadoop.hbase.Cell)", "public void rollback(org.apache.hadoop.hbase.Cell)"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.delete(org.apache.hadoop.hbase.Cell)", "public long delete(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.getRowKeyAtOrBefore(org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker)", "public void getRowKeyAtOrBefore(org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker)"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.updateColumnValue(byte[], byte[], byte[], long, long)", "public long updateColumnValue(byte[], byte[], byte[], long, long)"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long)", "public long upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.getScanners(long)", "public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.shouldSeek(org.apache.hadoop.hbase.client.Scan, long)", "public boolean shouldSeek(org.apache.hadoop.hbase.client.Scan, long)"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.heapSize()", "public long heapSize()"], ["long", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.size()", "public long size()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultMemStore.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["boolean", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.select(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean)", "public boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.DefaultStoreEngine", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine()", "public org.apache.hadoop.hbase.regionserver.DefaultStoreEngine()"], ["boolean", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine.needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionContext", "org.apache.hadoop.hbase.regionserver.DefaultStoreEngine.createCompaction()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction()"], ["org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration)", "public org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration)"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getStorefiles()", "public final java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getStorefiles()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.clearFiles()", "public com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile> clearFiles()"], ["int", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getStorefileCount()", "public final int getStorefileCount()"], ["void", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue)", "public final java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile> getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue)"], ["java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell)", "public java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile> updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getSplitPoint()", "public final byte[] getSplitPoint() throws java.io.IOException"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getFilesForScanOrGet(boolean, byte[], byte[])", "public final java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getFilesForScanOrGet(boolean, byte[], byte[])"], ["int", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getStoreCompactionPriority()", "public int getStoreCompactionPriority()"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["double", "org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager.getCompactionPressure()", "public double getCompactionPressure()"], ["org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher", "org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)", "public org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare[]", "org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare.values()", "public static org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare[] values()"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare", "org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteCompare valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult[]", "org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult.values()", "public static org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult[] values()"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult", "org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker(java.util.NavigableSet<byte[]>, int, int, long)", "public org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker(java.util.NavigableSet<byte[]>, int, int, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.done()", "public boolean done()"], ["org.apache.hadoop.hbase.regionserver.ColumnCount", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.getColumnHint()", "public org.apache.hadoop.hbase.regionserver.ColumnCount getColumnHint()"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(byte[], int, int, byte)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode checkColumn(byte[], int, int, byte)"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkVersions(byte[], int, int, long, byte, boolean)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode checkVersions(byte[], int, int, long, byte, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.reset()", "public void reset()"], ["void", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.doneWithColumn(byte[], int, int)", "public void doneWithColumn(byte[], int, int)"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.getNextRowOrNextColumn(byte[], int, int)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.isDone(long)", "public boolean isDone(long)"], ["org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy", "org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy()", "public org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy()"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.Store>", "org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy.selectStoresToFlush()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.Store> selectStoresToFlush()"], ["org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy", "org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy()", "public org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy()"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.Store>", "org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy.selectStoresToFlush()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.Store> selectStoresToFlush()"], ["org.apache.hadoop.hbase.regionserver.FlushPolicy", "org.apache.hadoop.hbase.regionserver.FlushPolicy()", "public org.apache.hadoop.hbase.regionserver.FlushPolicy()"], ["org.apache.hadoop.hbase.regionserver.FlushPolicyFactory", "org.apache.hadoop.hbase.regionserver.FlushPolicyFactory()", "public org.apache.hadoop.hbase.regionserver.FlushPolicyFactory()"], ["org.apache.hadoop.hbase.regionserver.FlushPolicy", "org.apache.hadoop.hbase.regionserver.FlushPolicyFactory.create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.FlushPolicy create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.FlushPolicy>", "org.apache.hadoop.hbase.regionserver.FlushPolicyFactory.getFlushPolicyClass(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration)", "public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.FlushPolicy> getFlushPolicyClass(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.FlushType[]", "org.apache.hadoop.hbase.regionserver.FlushType.values()", "public static org.apache.hadoop.hbase.regionserver.FlushType[] values()"], ["org.apache.hadoop.hbase.regionserver.FlushType", "org.apache.hadoop.hbase.regionserver.FlushType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.FlushType valueOf(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.isDeleted(org.apache.hadoop.hbase.Cell, java.util.NavigableSet<org.apache.hadoop.hbase.KeyValue>)", "public boolean isDeleted(org.apache.hadoop.hbase.Cell, java.util.NavigableSet<org.apache.hadoop.hbase.KeyValue>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.isExpired(org.apache.hadoop.hbase.Cell)", "public boolean isExpired(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.hasCandidate()", "public boolean hasCandidate()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.getCandidate()", "public org.apache.hadoop.hbase.Cell getCandidate()"], ["org.apache.hadoop.hbase.KeyValue", "org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.getTargetKey()", "public org.apache.hadoop.hbase.KeyValue getTargetKey()"], ["org.apache.hadoop.hbase.regionserver.HStore", "org.apache.hadoop.hbase.regionserver.HRegion$1.call()", "public org.apache.hadoop.hbase.regionserver.HStore call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HRegion$1.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.Pair<byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>>", "org.apache.hadoop.hbase.regionserver.HRegion$2.call()", "public org.apache.hadoop.hbase.util.Pair<byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>> call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HRegion$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Thread", "org.apache.hadoop.hbase.regionserver.HRegion$3.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["java.lang.Void", "org.apache.hadoop.hbase.regionserver.HRegion$4.call()", "public java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HRegion$4.call()", "public java.lang.Object call() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$5.run(com.google.protobuf.Message)", "public void run(com.google.protobuf.Message)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$5.run(java.lang.Object)", "public void run(java.lang.Object)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$6.add(int, org.apache.hadoop.hbase.Cell)", "public void add(int, org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$6.addAll(int, java.util.Collection<? extends org.apache.hadoop.hbase.Cell>)", "public boolean addAll(int, java.util.Collection<? extends org.apache.hadoop.hbase.Cell>)"], ["org.apache.hadoop.hbase.KeyValue", "org.apache.hadoop.hbase.regionserver.HRegion$6.get(int)", "public org.apache.hadoop.hbase.KeyValue get(int)"], ["int", "org.apache.hadoop.hbase.regionserver.HRegion$6.size()", "public int size()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$6.add(int, java.lang.Object)", "public void add(int, java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HRegion$6.get(int)", "public java.lang.Object get(int)"], ["org.apache.hadoop.hbase.regionserver.HRegion$BatchOperationInProgress", "org.apache.hadoop.hbase.regionserver.HRegion$BatchOperationInProgress(T[])", "public org.apache.hadoop.hbase.regionserver.HRegion$BatchOperationInProgress(T[])"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$BatchOperationInProgress.isDone()", "public boolean isDone()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl.isFlushSucceeded()", "public boolean isFlushSucceeded()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl.isCompactionNeeded()", "public boolean isCompactionNeeded()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result", "org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl.getResult()", "public org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result getResult()"], ["org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch(org.apache.hadoop.hbase.client.Mutation[], long, long)", "public org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch(org.apache.hadoop.hbase.client.Mutation[], long, long)"], ["org.apache.hadoop.hbase.client.Mutation", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.getMutation(int)", "public org.apache.hadoop.hbase.client.Mutation getMutation(int)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.getNonceGroup(int)", "public long getNonceGroup(int)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.getNonce(int)", "public long getNonce(int)"], ["org.apache.hadoop.hbase.client.Mutation[]", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.getMutationsForCoprocs()", "public org.apache.hadoop.hbase.client.Mutation[] getMutationsForCoprocs()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.isInReplay()", "public boolean isInReplay()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$MutationBatch.getReplaySequenceId()", "public long getReplaySequenceId()"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult", "org.apache.hadoop.hbase.regionserver.HRegion$PrepareFlushResult.getResult()", "public org.apache.hadoop.hbase.regionserver.Region$FlushResult getResult()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.getMaxResultSize()", "public long getMaxResultSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.getMvccReadPoint()", "public long getMvccReadPoint()"], ["int", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.getBatch()", "public int getBatch()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(java.util.List<org.apache.hadoop.hbase.Cell>)", "public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext)", "public synchronized boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>)", "public boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext)", "public boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.boolean isFilterDone()", "public synchronized boolean isFilterDone() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.void close()", "public synchronized void close()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.boolean reseek(byte[])", "public synchronized boolean reseek(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch(org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay[], long)", "public org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch(org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay[], long)"], ["org.apache.hadoop.hbase.client.Mutation", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.getMutation(int)", "public org.apache.hadoop.hbase.client.Mutation getMutation(int)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.getNonceGroup(int)", "public long getNonceGroup(int)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.getNonce(int)", "public long getNonce(int)"], ["org.apache.hadoop.hbase.client.Mutation[]", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.getMutationsForCoprocs()", "public org.apache.hadoop.hbase.client.Mutation[] getMutationsForCoprocs()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.isInReplay()", "public boolean isInReplay()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatch.getReplaySequenceId()", "public long getReplaySequenceId()"], ["org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl", "org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl()", "public org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl()"], ["org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext", "org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl.getContext()", "public org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext getContext()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl.setContext(org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext)", "public void setContext(org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl.release()", "public void release()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getSmallestReadPoint()", "public long getSmallestReadPoint()"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.initialize()", "public long initialize() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.hasReferences()", "public boolean hasReferences()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.HRegion.getHDFSBlocksDistribution()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlocksDistribution()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.HRegion.computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo)", "public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.HRegion.computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.addAndGetGlobalMemstoreSize(long)", "public long addAndGetGlobalMemstoreSize(long)"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegion.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getReadRequestsCount()", "public long getReadRequestsCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.updateReadRequestsCount(long)", "public void updateReadRequestsCount(long)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getWriteRequestsCount()", "public long getWriteRequestsCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.updateWriteRequestsCount(long)", "public void updateWriteRequestsCount(long)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getMemstoreSize()", "public long getMemstoreSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getNumMutationsWithoutWAL()", "public long getNumMutationsWithoutWAL()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getDataInMemoryWithoutWAL()", "public long getDataInMemoryWithoutWAL()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getBlockedRequestsCount()", "public long getBlockedRequestsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getCheckAndMutateChecksPassed()", "public long getCheckAndMutateChecksPassed()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getCheckAndMutateChecksFailed()", "public long getCheckAndMutateChecksFailed()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegion", "org.apache.hadoop.hbase.regionserver.HRegion.getMetrics()", "public org.apache.hadoop.hbase.regionserver.MetricsRegion getMetrics()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isClosed()", "public boolean isClosed()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isClosing()", "public boolean isClosing()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isReadOnly()", "public boolean isReadOnly()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.setRecovering(boolean)", "public void setRecovering(boolean)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isRecovering()", "public boolean isRecovering()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isAvailable()", "public boolean isAvailable()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isSplittable()", "public boolean isSplittable()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isMergeable()", "public boolean isMergeable()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.areWritesEnabled()", "public boolean areWritesEnabled()"], ["org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl", "org.apache.hadoop.hbase.regionserver.HRegion.getMVCC()", "public org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl getMVCC()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getMaxFlushedSeqId()", "public long getMaxFlushedSeqId()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getReadpoint(org.apache.hadoop.hbase.client.IsolationLevel)", "public long getReadpoint(org.apache.hadoop.hbase.client.IsolationLevel)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.isLoadingCfsOnDemandDefault()", "public boolean isLoadingCfsOnDemandDefault()"], ["java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>>", "org.apache.hadoop.hbase.regionserver.HRegion.close()", "public java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>> close() throws java.io.IOException"], ["java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>>", "org.apache.hadoop.hbase.regionserver.HRegion.close(boolean)", "public java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>> close(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.setClosing(boolean)", "public void setClosing(boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.waitForFlushesAndCompactions()", "public void waitForFlushesAndCompactions()"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.regionserver.HRegion.getTableDesc()", "public org.apache.hadoop.hbase.HTableDescriptor getTableDesc()"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.regionserver.HRegion.getWAL()", "public org.apache.hadoop.hbase.wal.WAL getWAL()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.regionserver.HRegion.getFilesystem()", "public org.apache.hadoop.fs.FileSystem getFilesystem()"], ["org.apache.hadoop.hbase.regionserver.HRegionFileSystem", "org.apache.hadoop.hbase.regionserver.HRegion.getRegionFileSystem()", "public org.apache.hadoop.hbase.regionserver.HRegionFileSystem getRegionFileSystem()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getEarliestFlushTimeForAllStores()", "public long getEarliestFlushTimeForAllStores()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getOldestHfileTs(boolean)", "public long getOldestHfileTs(boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getLargestHStoreSize()", "public long getLargestHStoreSize()"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.regionserver.HRegion.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.triggerMajorCompaction()", "public void triggerMajorCompaction() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.compact(boolean)", "public void compact(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.compactStores()", "public void compactStores() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public boolean compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult", "org.apache.hadoop.hbase.regionserver.HRegion.flush(boolean)", "public org.apache.hadoop.hbase.regionserver.Region$FlushResult flush(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult", "org.apache.hadoop.hbase.regionserver.HRegion.flushcache(boolean, boolean)", "public org.apache.hadoop.hbase.regionserver.Region$FlushResult flushcache(boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(byte[], byte[])", "public org.apache.hadoop.hbase.client.Result getClosestRowBefore(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.regionserver.HRegion.getScanner(org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.regionserver.RegionScanner getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.prepareDelete(org.apache.hadoop.hbase.client.Delete)", "public void prepareDelete(org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.delete(org.apache.hadoop.hbase.client.Delete)", "public void delete(org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.prepareDeleteTimestamps(org.apache.hadoop.hbase.client.Mutation, java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>, byte[])", "public void prepareDeleteTimestamps(org.apache.hadoop.hbase.client.Mutation, java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.put(org.apache.hadoop.hbase.client.Put)", "public void put(org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(org.apache.hadoop.hbase.client.Mutation[], long, long)", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] batchMutate(org.apache.hadoop.hbase.client.Mutation[], long, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(org.apache.hadoop.hbase.client.Mutation[])", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] batchMutate(org.apache.hadoop.hbase.client.Mutation[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.regionserver.HRegion.batchReplay(org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay[], long)", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] batchReplay(org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay[], long) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Mutation, boolean)", "public boolean checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Mutation, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.checkAndRowMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.RowMutations, boolean)", "public boolean checkAndRowMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.RowMutations, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.addRegionToSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare)", "public void addRegionToSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.updateCellTimestamps(java.lang.Iterable<java.util.List<org.apache.hadoop.hbase.Cell>>, byte[])", "public void updateCellTimestamps(java.lang.Iterable<java.util.List<org.apache.hadoop.hbase.Cell>>, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.setReadsEnabled(boolean)", "public void setReadsEnabled(boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.checkFamilies(java.util.Collection<byte[]>)", "public void checkFamilies(java.util.Collection<byte[]>) throws org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.checkTimestamps(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>, long)", "public void checkTimestamps(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>, long) throws org.apache.hadoop.hbase.exceptions.FailedSanityCheckException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.refreshStoreFiles()", "public boolean refreshStoreFiles() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.Store", "org.apache.hadoop.hbase.regionserver.HRegion.getStore(byte[])", "public org.apache.hadoop.hbase.regionserver.Store getStore(byte[])"], ["java.util.List<org.apache.hadoop.hbase.regionserver.Store>", "org.apache.hadoop.hbase.regionserver.HRegion.getStores()", "public java.util.List<org.apache.hadoop.hbase.regionserver.Store> getStores()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.regionserver.HRegion.getStoreFileList(byte[][])", "public java.util.List<java.lang.String> getStoreFileList(byte[][]) throws java.lang.IllegalArgumentException"], ["org.apache.hadoop.hbase.regionserver.Region$RowLock", "org.apache.hadoop.hbase.regionserver.HRegion.getRowLock(byte[], boolean)", "public org.apache.hadoop.hbase.regionserver.Region$RowLock getRowLock(byte[], boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.Region$RowLock", "org.apache.hadoop.hbase.regionserver.HRegion.getRowLock(byte[])", "public org.apache.hadoop.hbase.regionserver.Region$RowLock getRowLock(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.releaseRowLocks(java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>)", "public void releaseRowLocks(java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(java.util.Collection<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean, org.apache.hadoop.hbase.regionserver.Region$BulkLoadListener)", "public boolean bulkLoadHFiles(java.util.Collection<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean, org.apache.hadoop.hbase.regionserver.Region$BulkLoadListener) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.HRegion.hashCode()", "public int hashCode()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HRegion.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.closeHRegion(org.apache.hadoop.hbase.regionserver.HRegion)", "public static void closeHRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean, boolean)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean, boolean)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static org.apache.hadoop.hbase.regionserver.Region openHRegion(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.warmupHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable)", "public static void warmupHRegion(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.addRegionToMETA(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public static void addRegionToMETA(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegion.getRegionDir(org.apache.hadoop.fs.Path, java.lang.String)", "public static org.apache.hadoop.fs.Path getRegionDir(org.apache.hadoop.fs.Path, java.lang.String)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegion.getRegionDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo)", "public static org.apache.hadoop.fs.Path getRegionDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.rowIsInRange(org.apache.hadoop.hbase.HRegionInfo, byte[])", "public static boolean rowIsInRange(org.apache.hadoop.hbase.HRegionInfo, byte[])"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.mergeAdjacent(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public static org.apache.hadoop.hbase.regionserver.HRegion mergeAdjacent(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HRegion.merge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public static org.apache.hadoop.hbase.regionserver.HRegion merge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.get(org.apache.hadoop.hbase.client.Get)", "public org.apache.hadoop.hbase.client.Result get(org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.HRegion.get(org.apache.hadoop.hbase.client.Get, boolean)", "public java.util.List<org.apache.hadoop.hbase.Cell> get(org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.mutateRow(org.apache.hadoop.hbase.client.RowMutations)", "public void mutateRow(org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>)", "public void mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>, long, long)", "public void mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>, long, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$RegionLoadStats", "org.apache.hadoop.hbase.regionserver.HRegion.getRegionStats()", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$RegionLoadStats getRegionStats()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>)", "public void processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>, long, long)", "public void processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>, long, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>, long, long, long)", "public void processRowsWithLocks(org.apache.hadoop.hbase.regionserver.RowProcessor<?, ?>, long, long, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.append(org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.append(org.apache.hadoop.hbase.client.Append, long, long)", "public org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append, long, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.increment(org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.HRegion.increment(org.apache.hadoop.hbase.client.Increment, long, long)", "public org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment, long, long) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.heapSize()", "public long heapSize()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegion.registerService(com.google.protobuf.Service)", "public boolean registerService(com.google.protobuf.Service)"], ["com.google.protobuf.Message", "org.apache.hadoop.hbase.regionserver.HRegion.execService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceCall)", "public com.google.protobuf.Message execService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceCall) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.regionserver.HRegion.checkSplit()", "public byte[] checkSplit()"], ["int", "org.apache.hadoop.hbase.regionserver.HRegion.getCompactPriority()", "public int getCompactPriority()"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost", "org.apache.hadoop.hbase.regionserver.HRegion.getCoprocessorHost()", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost getCoprocessorHost()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.setCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)", "public void setCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.startRegionOperation()", "public void startRegionOperation() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.startRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void startRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.closeRegionOperation()", "public void closeRegionOperation() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.closeRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void closeRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getOpenSeqNum()", "public long getOpenSeqNum()"], ["java.util.Map<byte[], java.lang.Long>", "org.apache.hadoop.hbase.regionserver.HRegion.getMaxStoreSeqId()", "public java.util.Map<byte[], java.lang.Long> getMaxStoreSeqId()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegion.getOldestSeqIdOfStore(byte[])", "public long getOldestSeqIdOfStore(byte[])"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoResponse$CompactionState", "org.apache.hadoop.hbase.regionserver.HRegion.getCompactionState()", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoResponse$CompactionState getCompactionState()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.reportCompactionRequestStart(boolean)", "public void reportCompactionRequestStart(boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.reportCompactionRequestEnd(boolean, int, long)", "public void reportCompactionRequestEnd(boolean, int, long)"], ["java.util.concurrent.atomic.AtomicLong", "org.apache.hadoop.hbase.regionserver.HRegion.getSequenceId()", "public java.util.concurrent.atomic.AtomicLong getSequenceId()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.onConfigurationChange(org.apache.hadoop.conf.Configuration)", "public void onConfigurationChange(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegion.deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["org.apache.hadoop.hbase.regionserver.RegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.HRegion.getSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.RegionSplitPolicy getSplitPolicy()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getRegionInfoForFS()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfoForFS()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getTableDir()", "public org.apache.hadoop.fs.Path getTableDir()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getRegionDir()", "public org.apache.hadoop.fs.Path getRegionDir()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getStoreDir(java.lang.String)", "public org.apache.hadoop.fs.Path getStoreDir(java.lang.String)"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getStoreFiles(byte[])", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo> getStoreFiles(byte[]) throws java.io.IOException"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getStoreFiles(java.lang.String)", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo> getStoreFiles(java.lang.String) throws java.io.IOException"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getStoreFiles(java.lang.String, boolean)", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo> getStoreFiles(java.lang.String, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.hasReferences(java.lang.String)", "public boolean hasReferences(java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.hasReferences(org.apache.hadoop.hbase.HTableDescriptor)", "public boolean hasReferences(org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["java.util.Collection<java.lang.String>", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.getFamilies()", "public java.util.Collection<java.lang.String> getFamilies() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.deleteFamily(java.lang.String)", "public void deleteFamily(java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.createTempName()", "public org.apache.hadoop.fs.Path createTempName()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.createTempName(java.lang.String)", "public org.apache.hadoop.fs.Path createTempName(java.lang.String)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.commitStoreFile(java.lang.String, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.fs.Path commitStoreFile(java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.removeStoreFile(java.lang.String, org.apache.hadoop.fs.Path)", "public void removeStoreFile(java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.removeStoreFiles(java.lang.String, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void removeStoreFiles(java.lang.String, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.loadRegionInfoFileContent(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.HRegionInfo loadRegionInfoFileContent(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegionFileSystem", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.createRegionOnFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo)", "public static org.apache.hadoop.hbase.regionserver.HRegionFileSystem createRegionOnFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegionFileSystem", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.openRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public static org.apache.hadoop.hbase.regionserver.HRegionFileSystem openRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionFileSystem.deleteRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo)", "public static void deleteRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer$1.uncaughtException(java.lang.Thread, java.lang.Throwable)", "public void uncaughtException(java.lang.Thread, java.lang.Throwable)"], ["int", "org.apache.hadoop.hbase.regionserver.HRegionServer$2.compare(java.lang.Long, java.lang.Long)", "public int compare(java.lang.Long, java.lang.Long)"], ["int", "org.apache.hadoop.hbase.regionserver.HRegionServer$2.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer$3.run(com.google.protobuf.Message)", "public void run(com.google.protobuf.Message)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer$3.run(java.lang.Object)", "public void run(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo(org.apache.hadoop.hbase.ServerName, long)", "public org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo(org.apache.hadoop.hbase.ServerName, long)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo.getSeqNum()", "public long getSeqNum()"], ["long", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo.getMoveTime()", "public long getMoveTime()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionsCleaner$1.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionsCleaner$1.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionsCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionsCleaner.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.regionserver.HRegionServer$PeriodicMemstoreFlusher", "org.apache.hadoop.hbase.regionserver.HRegionServer$PeriodicMemstoreFlusher(int, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.regionserver.HRegionServer$PeriodicMemstoreFlusher(int, org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.regionserver.HRegionServer(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.HRegionServer(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.regionserver.HRegionServer(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)", "public org.apache.hadoop.hbase.regionserver.HRegionServer(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager) throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.registerService(com.google.protobuf.Service)", "public boolean registerService(com.google.protobuf.Service)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HRegionServer.getClusterId()", "public java.lang.String getClusterId()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.RegionServerAccounting", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionServerAccounting()", "public org.apache.hadoop.hbase.regionserver.RegionServerAccounting getRegionServerAccounting()"], ["org.apache.hadoop.hbase.master.TableLockManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getTableLockManager()", "public org.apache.hadoop.hbase.master.TableLockManager getTableLockManager()"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionLoad", "org.apache.hadoop.hbase.regionserver.HRegionServer.createRegionLoad(java.lang.String)", "public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionLoad createRegionLoad(java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.isOnline()", "public boolean isOnline()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServer", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionServerMetrics()", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServer getRegionServerMetrics()"], ["org.apache.hadoop.hbase.zookeeper.MasterAddressTracker", "org.apache.hadoop.hbase.regionserver.HRegionServer.getMasterAddressTracker()", "public org.apache.hadoop.hbase.zookeeper.MasterAddressTracker getMasterAddressTracker()"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.regionserver.HRegionServer.getWAL(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.ClusterConnection", "org.apache.hadoop.hbase.regionserver.HRegionServer.getConnection()", "public org.apache.hadoop.hbase.client.ClusterConnection getConnection()"], ["org.apache.hadoop.hbase.zookeeper.MetaTableLocator", "org.apache.hadoop.hbase.regionserver.HRegionServer.getMetaTableLocator()", "public org.apache.hadoop.hbase.zookeeper.MetaTableLocator getMetaTableLocator()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.stop(java.lang.String)", "public void stop(java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.waitForServerOnline()", "public void waitForServerOnline()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.Region)", "public void postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.Region) throws org.apache.zookeeper.KeeperException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext)", "public void postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext) throws org.apache.zookeeper.KeeperException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.reportRegionStateTransition(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, org.apache.hadoop.hbase.HRegionInfo...)", "public boolean reportRegionStateTransition(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, org.apache.hadoop.hbase.HRegionInfo...)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.reportRegionStateTransition(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, org.apache.hadoop.hbase.HRegionInfo...)", "public boolean reportRegionStateTransition(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, org.apache.hadoop.hbase.HRegionInfo...)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.reportRegionStateTransition(org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext)", "public boolean reportRegionStateTransition(org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext)"], ["org.apache.hadoop.hbase.ipc.RpcServerInterface", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRpcServer()", "public org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer()"], ["org.apache.hadoop.hbase.regionserver.RSRpcServices", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRSRpcServices()", "public org.apache.hadoop.hbase.regionserver.RSRpcServices getRSRpcServices()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.abort(java.lang.String)", "public void abort(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.isAborted()", "public boolean isAborted()"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds", "org.apache.hadoop.hbase.regionserver.HRegionServer.getLastSequenceId(byte[])", "public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getLastSequenceId(byte[])"], ["org.apache.hadoop.hbase.http.InfoServer", "org.apache.hadoop.hbase.regionserver.HRegionServer.getInfoServer()", "public org.apache.hadoop.hbase.http.InfoServer getInfoServer()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.isStopped()", "public boolean isStopped()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.isStopping()", "public boolean isStopping()"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRecoveringRegions()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.regionserver.Region> getRecoveringRegions()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.HRegionServer.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["int", "org.apache.hadoop.hbase.regionserver.HRegionServer.getNumberOfOnlineRegions()", "public int getNumberOfOnlineRegions()"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegionsLocalContext()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.Region> getOnlineRegionsLocalContext()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.addToOnlineRegions(org.apache.hadoop.hbase.regionserver.Region)", "public void addToOnlineRegions(org.apache.hadoop.hbase.regionserver.Region)"], ["long", "org.apache.hadoop.hbase.regionserver.HRegionServer.getStartcode()", "public long getStartcode()"], ["org.apache.hadoop.hbase.regionserver.FlushRequester", "org.apache.hadoop.hbase.regionserver.HRegionServer.getFlushRequester()", "public org.apache.hadoop.hbase.regionserver.FlushRequester getFlushRequester()"], ["org.apache.hadoop.hbase.regionserver.Leases", "org.apache.hadoop.hbase.regionserver.HRegionServer.getLeases()", "public org.apache.hadoop.hbase.regionserver.Leases getLeases()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.regionserver.HRegionServer.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HRegionServer.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.regionserver.HRegionServer.getThreadWakeFrequency()", "public int getThreadWakeFrequency()"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher", "org.apache.hadoop.hbase.regionserver.HRegionServer.getZooKeeper()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher getZooKeeper()"], ["org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCoordinatedStateManager()", "public org.apache.hadoop.hbase.coordination.BaseCoordinatedStateManager getCoordinatedStateManager()"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.regionserver.HRegionServer.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["org.apache.hadoop.hbase.regionserver.CompactionRequestor", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCompactionRequester()", "public org.apache.hadoop.hbase.regionserver.CompactionRequestor getCompactionRequester()"], ["org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionServerCoprocessorHost()", "public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost getRegionServerCoprocessorHost()"], ["java.util.concurrent.ConcurrentMap<byte[], java.lang.Boolean>", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionsInTransitionInRS()", "public java.util.concurrent.ConcurrentMap<byte[], java.lang.Boolean> getRegionsInTransitionInRS()"], ["org.apache.hadoop.hbase.executor.ExecutorService", "org.apache.hadoop.hbase.regionserver.HRegionServer.getExecutorService()", "public org.apache.hadoop.hbase.executor.ExecutorService getExecutorService()"], ["org.apache.hadoop.hbase.ChoreService", "org.apache.hadoop.hbase.regionserver.HRegionServer.getChoreService()", "public org.apache.hadoop.hbase.ChoreService getChoreService()"], ["org.apache.hadoop.hbase.quotas.RegionServerQuotaManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionServerQuotaManager()", "public org.apache.hadoop.hbase.quotas.RegionServerQuotaManager getRegionServerQuotaManager()"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)", "public static org.apache.hadoop.hbase.regionserver.HRegionServer constructRegionServer(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager)"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegions(org.apache.hadoop.hbase.TableName)", "public java.util.List<org.apache.hadoop.hbase.regionserver.Region> getOnlineRegions(org.apache.hadoop.hbase.TableName)"], ["java.util.Set<org.apache.hadoop.hbase.TableName>", "org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineTables()", "public java.util.Set<org.apache.hadoop.hbase.TableName> getOnlineTables()"], ["java.lang.String[]", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionServerCoprocessors()", "public java.lang.String[] getRegionServerCoprocessors()"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegion(byte[])", "public org.apache.hadoop.hbase.regionserver.Region getOnlineRegion(byte[])"], ["java.net.InetSocketAddress[]", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionBlockLocations(java.lang.String)", "public java.net.InetSocketAddress[] getRegionBlockLocations(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.HRegionServer.getFromOnlineRegions(java.lang.String)", "public org.apache.hadoop.hbase.regionserver.Region getFromOnlineRegions(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.removeFromOnlineRegions(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.ServerName)", "public boolean removeFromOnlineRegions(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(java.lang.String)", "public org.apache.hadoop.hbase.regionserver.Region getRegionByEncodedName(java.lang.String) throws org.apache.hadoop.hbase.NotServingRegionException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HRegionServer.checkFileSystem()", "public boolean checkFileSystem()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.updateRegionFavoredNodesMapping(java.lang.String, java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ServerName>)", "public void updateRegionFavoredNodesMapping(java.lang.String, java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$ServerName>)"], ["java.net.InetSocketAddress[]", "org.apache.hadoop.hbase.regionserver.HRegionServer.getFavoredNodesForRegion(java.lang.String)", "public java.net.InetSocketAddress[] getFavoredNodesForRegion(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.ServerNonceManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getNonceManager()", "public org.apache.hadoop.hbase.regionserver.ServerNonceManager getNonceManager()"], ["org.apache.hadoop.hbase.regionserver.CompactSplitThread", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCompactSplitThread()", "public org.apache.hadoop.hbase.regionserver.CompactSplitThread getCompactSplitThread()"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse", "org.apache.hadoop.hbase.regionserver.HRegionServer.execRegionServerService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse execRegionServerService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCacheConfig()", "public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig()"], ["org.apache.hadoop.hbase.TableDescriptors", "org.apache.hadoop.hbase.regionserver.HRegionServer.getTableDescriptors()", "public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors()"], ["void", "org.apache.hadoop.hbase.regionserver.HRegionServer.updateConfiguration()", "public void updateConfiguration()"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getHeapMemoryManager()", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager getHeapMemoryManager()"], ["double", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCompactionPressure()", "public double getCompactionPressure()"], ["org.apache.hadoop.hbase.CoordinatedStateManager", "org.apache.hadoop.hbase.regionserver.HRegionServer.getCoordinatedStateManager()", "public org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager()"], ["org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine", "org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>)", "public org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>)"], ["int", "org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.regionserver.StoreFile", "org.apache.hadoop.hbase.regionserver.HStore$1.call()", "public org.apache.hadoop.hbase.regionserver.StoreFile call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HStore$1.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.regionserver.HStore$2.call()", "public java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.HStore$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.prepare()", "public void prepare()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public void flushCache(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public boolean commit(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.getCommittedFiles()", "public java.util.List<org.apache.hadoop.fs.Path> getCommittedFiles()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.replayFlush(java.util.List<java.lang.String>, boolean)", "public void replayFlush(java.util.List<java.lang.String>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.abort()", "public void abort() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HStore.getColumnFamilyName()", "public java.lang.String getColumnFamilyName()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.regionserver.HStore.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.regionserver.HStore.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["org.apache.hadoop.hbase.regionserver.HRegionFileSystem", "org.apache.hadoop.hbase.regionserver.HStore.getRegionFileSystem()", "public org.apache.hadoop.hbase.regionserver.HRegionFileSystem getRegionFileSystem()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getStoreFileTtl()", "public long getStoreFileTtl()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMemstoreFlushSize()", "public long getMemstoreFlushSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getFlushableSize()", "public long getFlushableSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getSnapshotSize()", "public long getSnapshotSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getCompactionCheckMultiplier()", "public long getCompactionCheckMultiplier()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getBlockingFileCount()", "public long getBlockingFileCount()"], ["int", "org.apache.hadoop.hbase.regionserver.HStore.getBytesPerChecksum(org.apache.hadoop.conf.Configuration)", "public static int getBytesPerChecksum(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.util.ChecksumType", "org.apache.hadoop.hbase.regionserver.HStore.getChecksumType(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.util.ChecksumType getChecksumType(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.regionserver.HStore.getCloseCheckInterval()", "public static int getCloseCheckInterval()"], ["org.apache.hadoop.hbase.HColumnDescriptor", "org.apache.hadoop.hbase.regionserver.HStore.getFamily()", "public org.apache.hadoop.hbase.HColumnDescriptor getFamily()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMaxSequenceId()", "public long getMaxSequenceId()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMaxMemstoreTS()", "public long getMaxMemstoreTS()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HStore.getStoreHomedir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, byte[])", "public static org.apache.hadoop.fs.Path getStoreHomedir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo, byte[])"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HStore.getStoreHomedir(org.apache.hadoop.fs.Path, java.lang.String, byte[])", "public static org.apache.hadoop.fs.Path getStoreHomedir(org.apache.hadoop.fs.Path, java.lang.String, byte[])"], ["org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder", "org.apache.hadoop.hbase.regionserver.HStore.getDataBlockEncoder()", "public org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder getDataBlockEncoder()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.refreshStoreFiles()", "public void refreshStoreFiles() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.refreshStoreFiles(java.util.Collection<java.lang.String>)", "public void refreshStoreFiles(java.util.Collection<java.lang.String>) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.Pair<java.lang.Long, org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.HStore.add(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.util.Pair<java.lang.Long, org.apache.hadoop.hbase.Cell> add(org.apache.hadoop.hbase.Cell)"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.timeOfOldestEdit()", "public long timeOfOldestEdit()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.rollback(org.apache.hadoop.hbase.Cell)", "public void rollback(org.apache.hadoop.hbase.Cell)"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.HStore.getStorefiles()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getStorefiles()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.assertBulkLoadHFileOk(org.apache.hadoop.fs.Path)", "public void assertBulkLoadHFileOk(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.HStore.bulkLoadHFile(java.lang.String, long)", "public org.apache.hadoop.fs.Path bulkLoadHFile(java.lang.String, long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.bulkLoadHFile(org.apache.hadoop.hbase.regionserver.StoreFileInfo)", "public void bulkLoadHFile(org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException"], ["com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.HStore.close()", "public com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile> close() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Writer", "org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Writer createWriterInTmp(long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, boolean, boolean, boolean) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>", "org.apache.hadoop.hbase.regionserver.HStore.getScanners(boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.ScanQueryMatcher, byte[], byte[], long)", "public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.ScanQueryMatcher, byte[], byte[], long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.addChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver)", "public void addChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver)"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.deleteChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver)", "public void deleteChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.HStore.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.replayCompactionMarker(org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor, boolean, boolean)", "public void replayCompactionMarker(org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.compactRecentForTestingAssumingDefaultPolicy(int)", "public void compactRecentForTestingAssumingDefaultPolicy(int) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.hasReferences()", "public boolean hasReferences()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress", "org.apache.hadoop.hbase.regionserver.HStore.getCompactionProgress()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress getCompactionProgress()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.isMajorCompaction()", "public boolean isMajorCompaction() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionContext", "org.apache.hadoop.hbase.regionserver.HStore.requestCompaction()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext requestCompaction() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionContext", "org.apache.hadoop.hbase.regionserver.HStore.requestCompaction(int, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext requestCompaction(int, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.cancelRequestedCompaction(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext)", "public void cancelRequestedCompaction(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(byte[])", "public org.apache.hadoop.hbase.Cell getRowKeyAtOrBefore(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.canSplit()", "public boolean canSplit()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.HStore.getSplitPoint()", "public byte[] getSplitPoint()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getLastCompactSize()", "public long getLastCompactSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getSize()", "public long getSize()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.triggerMajorCompaction()", "public void triggerMajorCompaction()"], ["org.apache.hadoop.hbase.regionserver.KeyValueScanner", "org.apache.hadoop.hbase.regionserver.HStore.getScanner(org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long)", "public org.apache.hadoop.hbase.regionserver.KeyValueScanner getScanner(org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HStore.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.regionserver.HStore.getStorefilesCount()", "public int getStorefilesCount()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getStoreSizeUncompressed()", "public long getStoreSizeUncompressed()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getStorefilesSize()", "public long getStorefilesSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getStorefilesIndexSize()", "public long getStorefilesIndexSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getTotalStaticIndexSize()", "public long getTotalStaticIndexSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getTotalStaticBloomSize()", "public long getTotalStaticBloomSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMemStoreSize()", "public long getMemStoreSize()"], ["int", "org.apache.hadoop.hbase.regionserver.HStore.getCompactPriority()", "public int getCompactPriority()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.throttleCompaction(long)", "public boolean throttleCompaction(long)"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.HStore.getHRegion()", "public org.apache.hadoop.hbase.regionserver.HRegion getHRegion()"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost", "org.apache.hadoop.hbase.regionserver.HStore.getCoprocessorHost()", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost getCoprocessorHost()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.HStore.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.areWritesEnabled()", "public boolean areWritesEnabled()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getSmallestReadPoint()", "public long getSmallestReadPoint()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.updateColumnValue(byte[], byte[], byte[], long)", "public long updateColumnValue(byte[], byte[], byte[], long) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long)", "public long upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFlushContext", "org.apache.hadoop.hbase.regionserver.HStore.createFlushContext(long)", "public org.apache.hadoop.hbase.regionserver.StoreFlushContext createFlushContext(long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.needsCompaction()", "public boolean needsCompaction()"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.regionserver.HStore.getCacheConfig()", "public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.heapSize()", "public long heapSize()"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.regionserver.HStore.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["org.apache.hadoop.hbase.regionserver.ScanInfo", "org.apache.hadoop.hbase.regionserver.HStore.getScanInfo()", "public org.apache.hadoop.hbase.regionserver.ScanInfo getScanInfo()"], ["boolean", "org.apache.hadoop.hbase.regionserver.HStore.hasTooManyStoreFiles()", "public boolean hasTooManyStoreFiles()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getFlushedCellsCount()", "public long getFlushedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getFlushedCellsSize()", "public long getFlushedCellsSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getCompactedCellsCount()", "public long getCompactedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getCompactedCellsSize()", "public long getCompactedCellsSize()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMajorCompactedCellsCount()", "public long getMajorCompactedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.HStore.getMajorCompactedCellsSize()", "public long getMajorCompactedCellsSize()"], ["org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?>", "org.apache.hadoop.hbase.regionserver.HStore.getStoreEngine()", "public org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?> getStoreEngine()"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.onConfigurationChange(org.apache.hadoop.conf.Configuration)", "public void onConfigurationChange(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["void", "org.apache.hadoop.hbase.regionserver.HStore.deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)", "public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager)"], ["double", "org.apache.hadoop.hbase.regionserver.HStore.getCompactionPressure()", "public double getCompactionPressure()"], ["java.util.Collection", "org.apache.hadoop.hbase.regionserver.HStore.close()", "public java.util.Collection close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB$Chunk.init()", "public void init()"], ["int", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB$Chunk.alloc(int)", "public int alloc(int)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB$Chunk.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB()", "public org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB()"], ["org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.util.ByteRange", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB.allocateBytes(int)", "public org.apache.hadoop.hbase.util.ByteRange allocateBytes(int)"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB.close()", "public void close()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB.incScannerCount()", "public void incScannerCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB.decScannerCount()", "public void decScannerCount()"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore(org.apache.hadoop.hbase.regionserver.HeapMemoryManager)", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore(org.apache.hadoop.hbase.regionserver.HeapMemoryManager)"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore.flushRequested(org.apache.hadoop.hbase.regionserver.FlushType, org.apache.hadoop.hbase.regionserver.Region)", "public void flushRequested(org.apache.hadoop.hbase.regionserver.FlushType, org.apache.hadoop.hbase.regionserver.Region)"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext()", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext()"], ["long", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.getBlockedFlushCount()", "public long getBlockedFlushCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.setBlockedFlushCount(long)", "public void setBlockedFlushCount(long)"], ["long", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.getUnblockedFlushCount()", "public long getUnblockedFlushCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.setUnblockedFlushCount(long)", "public void setUnblockedFlushCount(long)"], ["long", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.getEvictCount()", "public long getEvictCount()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.setEvictCount(long)", "public void setEvictCount(long)"], ["float", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.getCurMemStoreSize()", "public float getCurMemStoreSize()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.setCurMemStoreSize(float)", "public void setCurMemStoreSize(float)"], ["float", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.getCurBlockCacheSize()", "public float getCurBlockCacheSize()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext.setCurBlockCacheSize(float)", "public void setCurBlockCacheSize(float)"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult(boolean)", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult(boolean)"], ["float", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult.getMemstoreSize()", "public float getMemstoreSize()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult.setMemstoreSize(float)", "public void setMemstoreSize(float)"], ["float", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult.getBlockCacheSize()", "public float getBlockCacheSize()"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult.setBlockCacheSize(float)", "public void setBlockCacheSize(float)"], ["boolean", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult.needsTuning()", "public boolean needsTuning()"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.FlushRequester, org.apache.hadoop.hbase.Server)", "public static org.apache.hadoop.hbase.regionserver.HeapMemoryManager create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.FlushRequester, org.apache.hadoop.hbase.Server)"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager.start(org.apache.hadoop.hbase.ChoreService)", "public void start(org.apache.hadoop.hbase.ChoreService)"], ["void", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager.stop()", "public void stop()"], ["float", "org.apache.hadoop.hbase.regionserver.HeapMemoryManager.getHeapOccupancyPercent()", "public float getHeapOccupancyPercent()"], ["org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.InternalScan", "org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Get)", "public org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Get)"], ["org.apache.hadoop.hbase.regionserver.InternalScan", "org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.InternalScan.checkOnlyMemStore()", "public void checkOnlyMemStore()"], ["void", "org.apache.hadoop.hbase.regionserver.InternalScan.checkOnlyStoreFiles()", "public void checkOnlyStoreFiles()"], ["boolean", "org.apache.hadoop.hbase.regionserver.InternalScan.isCheckOnlyMemStore()", "public boolean isCheckOnlyMemStore()"], ["boolean", "org.apache.hadoop.hbase.regionserver.InternalScan.isCheckOnlyStoreFiles()", "public boolean isCheckOnlyStoreFiles()"], ["org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator", "org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["int", "org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner)", "public int compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner)"], ["int", "org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["int", "org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.KeyValueHeap", "org.apache.hadoop.hbase.regionserver.KeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.KeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.KeyValue$KVComparator) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.peek()", "public org.apache.hadoop.hbase.Cell peek()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.next()", "public org.apache.hadoop.hbase.Cell next() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(java.util.List<org.apache.hadoop.hbase.Cell>)", "public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext)", "public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean)", "public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException"], ["java.util.PriorityQueue<org.apache.hadoop.hbase.regionserver.KeyValueScanner>", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.getHeap()", "public java.util.PriorityQueue<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getHeap()"], ["long", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.getSequenceID()", "public long getSequenceID()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.KeyValueHeap.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.Leases$Lease.getLeaseName()", "public java.lang.String getLeaseName()"], ["org.apache.hadoop.hbase.regionserver.LeaseListener", "org.apache.hadoop.hbase.regionserver.Leases$Lease.getListener()", "public org.apache.hadoop.hbase.regionserver.LeaseListener getListener()"], ["boolean", "org.apache.hadoop.hbase.regionserver.Leases$Lease.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.Leases$Lease.hashCode()", "public int hashCode()"], ["long", "org.apache.hadoop.hbase.regionserver.Leases$Lease.getDelay(java.util.concurrent.TimeUnit)", "public long getDelay(java.util.concurrent.TimeUnit)"], ["int", "org.apache.hadoop.hbase.regionserver.Leases$Lease.compareTo(java.util.concurrent.Delayed)", "public int compareTo(java.util.concurrent.Delayed)"], ["void", "org.apache.hadoop.hbase.regionserver.Leases$Lease.resetExpirationTime()", "public void resetExpirationTime()"], ["int", "org.apache.hadoop.hbase.regionserver.Leases$Lease.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException", "org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException(java.lang.String)", "public org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException.getName()", "public java.lang.String getName()"], ["org.apache.hadoop.hbase.regionserver.Leases", "org.apache.hadoop.hbase.regionserver.Leases(int)", "public org.apache.hadoop.hbase.regionserver.Leases(int)"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.closeAfterLeasesExpire()", "public void closeAfterLeasesExpire()"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.close()", "public void close()"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.createLease(java.lang.String, int, org.apache.hadoop.hbase.regionserver.LeaseListener)", "public void createLease(java.lang.String, int, org.apache.hadoop.hbase.regionserver.LeaseListener) throws org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.addLease(org.apache.hadoop.hbase.regionserver.Leases$Lease)", "public void addLease(org.apache.hadoop.hbase.regionserver.Leases$Lease) throws org.apache.hadoop.hbase.regionserver.Leases$LeaseStillHeldException"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.renewLease(java.lang.String)", "public void renewLease(java.lang.String) throws org.apache.hadoop.hbase.regionserver.LeaseException"], ["void", "org.apache.hadoop.hbase.regionserver.Leases.cancelLease(java.lang.String)", "public void cancelLease(java.lang.String) throws org.apache.hadoop.hbase.regionserver.LeaseException"], ["void", "org.apache.hadoop.hbase.regionserver.LogRoller$1.logRollRequested(boolean)", "public void logRollRequested(boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.LogRoller.addWAL(org.apache.hadoop.hbase.wal.WAL)", "public void addWAL(org.apache.hadoop.hbase.wal.WAL)"], ["void", "org.apache.hadoop.hbase.regionserver.LogRoller.requestRollAll()", "public void requestRollAll()"], ["org.apache.hadoop.hbase.regionserver.LogRoller", "org.apache.hadoop.hbase.regionserver.LogRoller(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.LogRoller(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["void", "org.apache.hadoop.hbase.regionserver.LogRoller.run()", "public void run()"], ["K", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.getKey()", "public K getKey()"], ["V", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.getValue()", "public V getValue()"], ["V", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.setValue(V)", "public V setValue(V)"], ["boolean", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.hashCode()", "public int hashCode()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.heapSize()", "public long heapSize()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.setValue(java.lang.Object)", "public java.lang.Object setValue(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.getValue()", "public java.lang.Object getValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap$Entry.getKey()", "public java.lang.Object getKey()"], ["org.apache.hadoop.hbase.regionserver.LruHashMap", "org.apache.hadoop.hbase.regionserver.LruHashMap(int, float, long)", "public org.apache.hadoop.hbase.regionserver.LruHashMap(int, float, long)"], ["org.apache.hadoop.hbase.regionserver.LruHashMap", "org.apache.hadoop.hbase.regionserver.LruHashMap(int, float)", "public org.apache.hadoop.hbase.regionserver.LruHashMap(int, float)"], ["org.apache.hadoop.hbase.regionserver.LruHashMap", "org.apache.hadoop.hbase.regionserver.LruHashMap(int)", "public org.apache.hadoop.hbase.regionserver.LruHashMap(int)"], ["org.apache.hadoop.hbase.regionserver.LruHashMap", "org.apache.hadoop.hbase.regionserver.LruHashMap(long)", "public org.apache.hadoop.hbase.regionserver.LruHashMap(long)"], ["org.apache.hadoop.hbase.regionserver.LruHashMap", "org.apache.hadoop.hbase.regionserver.LruHashMap()", "public org.apache.hadoop.hbase.regionserver.LruHashMap()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.getMemFree()", "public long getMemFree()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.getMemMax()", "public long getMemMax()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.getMemUsed()", "public long getMemUsed()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.getHitCount()", "public long getHitCount()"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.getMissCount()", "public long getMissCount()"], ["double", "org.apache.hadoop.hbase.regionserver.LruHashMap.getHitRatio()", "public double getHitRatio()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.long freeMemory(long)", "public synchronized long freeMemory(long) throws java.lang.Exception"], ["long", "org.apache.hadoop.hbase.regionserver.LruHashMap.heapSize()", "public long heapSize()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.V get(java.lang.Object)", "public synchronized V get(java.lang.Object)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.V put(K, V)", "public synchronized V put(K, V)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.V remove(java.lang.Object)", "public synchronized V remove(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.LruHashMap.size()", "public int size()"], ["boolean", "org.apache.hadoop.hbase.regionserver.LruHashMap.isEmpty()", "public boolean isEmpty()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.void clear()", "public synchronized void clear()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.boolean containsKey(java.lang.Object)", "public synchronized boolean containsKey(java.lang.Object)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.LruHashMap.boolean containsValue(java.lang.Object)", "public synchronized boolean containsValue(java.lang.Object)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.LruHashMap$Entry<K, V>>", "org.apache.hadoop.hbase.regionserver.LruHashMap.entryLruList()", "public java.util.List<org.apache.hadoop.hbase.regionserver.LruHashMap$Entry<K, V>> entryLruList()"], ["java.util.Set<org.apache.hadoop.hbase.regionserver.LruHashMap$Entry<K, V>>", "org.apache.hadoop.hbase.regionserver.LruHashMap.entryTableSet()", "public java.util.Set<org.apache.hadoop.hbase.regionserver.LruHashMap$Entry<K, V>> entryTableSet()"], ["org.apache.hadoop.hbase.regionserver.LruHashMap$Entry", "org.apache.hadoop.hbase.regionserver.LruHashMap.getHeadPtr()", "public org.apache.hadoop.hbase.regionserver.LruHashMap$Entry getHeadPtr()"], ["org.apache.hadoop.hbase.regionserver.LruHashMap$Entry", "org.apache.hadoop.hbase.regionserver.LruHashMap.getTailPtr()", "public org.apache.hadoop.hbase.regionserver.LruHashMap$Entry getTailPtr()"], ["java.util.Set<java.util.Map$Entry<K, V>>", "org.apache.hadoop.hbase.regionserver.LruHashMap.entrySet()", "public java.util.Set<java.util.Map$Entry<K, V>> entrySet()"], ["boolean", "org.apache.hadoop.hbase.regionserver.LruHashMap.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.LruHashMap.hashCode()", "public int hashCode()"], ["java.util.Set<K>", "org.apache.hadoop.hbase.regionserver.LruHashMap.keySet()", "public java.util.Set<K> keySet()"], ["void", "org.apache.hadoop.hbase.regionserver.LruHashMap.putAll(java.util.Map<? extends K, ? extends V>)", "public void putAll(java.util.Map<? extends K, ? extends V>)"], ["java.util.Collection<V>", "org.apache.hadoop.hbase.regionserver.LruHashMap.values()", "public java.util.Collection<V> values()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap.remove(java.lang.Object)", "public java.lang.Object remove(java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap.put(java.lang.Object, java.lang.Object)", "public java.lang.Object put(java.lang.Object, java.lang.Object)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.LruHashMap.get(java.lang.Object)", "public java.lang.Object get(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.MemStoreChunkPool$StatisticsThread", "org.apache.hadoop.hbase.regionserver.MemStoreChunkPool$StatisticsThread(org.apache.hadoop.hbase.regionserver.MemStoreChunkPool)", "public org.apache.hadoop.hbase.regionserver.MemStoreChunkPool$StatisticsThread(org.apache.hadoop.hbase.regionserver.MemStoreChunkPool)"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreChunkPool$StatisticsThread.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run()", "public void run()"], ["boolean", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.isMaximumWait(long)", "public boolean isMaximumWait(long)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.getRequeueCount()", "public int getRequeueCount()"], ["boolean", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.isForceFlushAllStores()", "public boolean isForceFlushAllStores()"], ["org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.requeue(long)", "public org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry requeue(long)"], ["long", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.getDelay(java.util.concurrent.TimeUnit)", "public long getDelay(java.util.concurrent.TimeUnit)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.compareTo(java.util.concurrent.Delayed)", "public int compareTo(java.util.concurrent.Delayed)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["long", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$WakeupFlushThread.getDelay(java.util.concurrent.TimeUnit)", "public long getDelay(java.util.concurrent.TimeUnit)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$WakeupFlushThread.compareTo(java.util.concurrent.Delayed)", "public int compareTo(java.util.concurrent.Delayed)"], ["boolean", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$WakeupFlushThread.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher$WakeupFlushThread.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.MemStoreFlusher", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.regionserver.MemStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["org.apache.hadoop.hbase.util.Counter", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.getUpdatesBlockedMsHighWater()", "public org.apache.hadoop.hbase.util.Counter getUpdatesBlockedMsHighWater()"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.requestFlush(org.apache.hadoop.hbase.regionserver.Region, boolean)", "public void requestFlush(org.apache.hadoop.hbase.regionserver.Region, boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.requestDelayedFlush(org.apache.hadoop.hbase.regionserver.Region, long, boolean)", "public void requestDelayedFlush(org.apache.hadoop.hbase.regionserver.Region, long, boolean)"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.getFlushQueueSize()", "public int getFlushQueueSize()"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory()", "public void reclaimMemStoreMemory()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.dumpQueue()", "public java.lang.String dumpQueue()"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.registerFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener)", "public void registerFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener)"], ["boolean", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.unregisterFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener)", "public boolean unregisterFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener)"], ["void", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.setGlobalMemstoreLimit(long)", "public void setGlobalMemstoreLimit(long)"], ["long", "org.apache.hadoop.hbase.regionserver.MemStoreFlusher.getMemoryLimit()", "public long getMemoryLimit()"], ["org.apache.hadoop.hbase.regionserver.MemStoreSnapshot", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot(long, int, long, org.apache.hadoop.hbase.regionserver.TimeRangeTracker, org.apache.hadoop.hbase.regionserver.KeyValueScanner)", "public org.apache.hadoop.hbase.regionserver.MemStoreSnapshot(long, int, long, org.apache.hadoop.hbase.regionserver.TimeRangeTracker, org.apache.hadoop.hbase.regionserver.KeyValueScanner)"], ["long", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot.getId()", "public long getId()"], ["int", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot.getCellsCount()", "public int getCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot.getSize()", "public long getSize()"], ["org.apache.hadoop.hbase.regionserver.TimeRangeTracker", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot.getTimeRangeTracker()", "public org.apache.hadoop.hbase.regionserver.TimeRangeTracker getTimeRangeTracker()"], ["org.apache.hadoop.hbase.regionserver.KeyValueScanner", "org.apache.hadoop.hbase.regionserver.MemStoreSnapshot.getScanner()", "public org.apache.hadoop.hbase.regionserver.KeyValueScanner getScanner()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegion", "org.apache.hadoop.hbase.regionserver.MetricsRegion(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper)", "public org.apache.hadoop.hbase.regionserver.MetricsRegion(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.close()", "public void close()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updatePut()", "public void updatePut()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updateDelete()", "public void updateDelete()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updateGet(long)", "public void updateGet(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updateScanNext(long)", "public void updateScanNext(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updateAppend()", "public void updateAppend()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegion.updateIncrement()", "public void updateIncrement()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper", "org.apache.hadoop.hbase.regionserver.MetricsRegion.getRegionWrapper()", "public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper getRegionWrapper()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServer", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.getMetricsSource()", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource getMetricsSource()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.getRegionServerWrapper()", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper getRegionServerWrapper()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updatePut(long)", "public void updatePut(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateDelete(long)", "public void updateDelete(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateGet(long)", "public void updateGet(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateIncrement(long)", "public void updateIncrement(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateAppend(long)", "public void updateAppend(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateReplay(long)", "public void updateReplay(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateScannerNext(long)", "public void updateScannerNext(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateSplitTime(long)", "public void updateSplitTime(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.incrSplitRequest()", "public void incrSplitRequest()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.incrSplitSuccess()", "public void incrSplitSuccess()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServer.updateFlushTime(long)", "public void updateFlushTime(long)"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl)", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable.void run()", "public synchronized void run()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getClusterId()", "public java.lang.String getClusterId()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getStartCode()", "public long getStartCode()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getZookeeperQuorum()", "public java.lang.String getZookeeperQuorum()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCoprocessors()", "public java.lang.String getCoprocessors()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getServerName()", "public java.lang.String getServerName()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getNumOnlineRegions()", "public long getNumOnlineRegions()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getTotalRequestCount()", "public long getTotalRequestCount()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getSplitQueueSize()", "public int getSplitQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCompactionQueueSize()", "public int getCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getSmallCompactionQueueSize()", "public int getSmallCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getLargeCompactionQueueSize()", "public int getLargeCompactionQueueSize()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getFlushQueueSize()", "public int getFlushQueueSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheCount()", "public long getBlockCacheCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheSize()", "public long getBlockCacheSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheFreeSize()", "public long getBlockCacheFreeSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheHitCount()", "public long getBlockCacheHitCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheMissCount()", "public long getBlockCacheMissCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheEvictedCount()", "public long getBlockCacheEvictedCount()"], ["double", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheHitPercent()", "public double getBlockCacheHitPercent()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockCacheHitCachingPercent()", "public int getBlockCacheHitCachingPercent()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.forceRecompute()", "public void forceRecompute()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getNumStores()", "public long getNumStores()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getNumWALFiles()", "public long getNumWALFiles()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getWALFileSize()", "public long getWALFileSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getNumStoreFiles()", "public long getNumStoreFiles()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getMemstoreSize()", "public long getMemstoreSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getStoreFileSize()", "public long getStoreFileSize()"], ["double", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getRequestsPerSecond()", "public double getRequestsPerSecond()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getReadRequestsCount()", "public long getReadRequestsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getWriteRequestsCount()", "public long getWriteRequestsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCheckAndMutateChecksFailed()", "public long getCheckAndMutateChecksFailed()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCheckAndMutateChecksPassed()", "public long getCheckAndMutateChecksPassed()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getStoreFileIndexSize()", "public long getStoreFileIndexSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getTotalStaticIndexSize()", "public long getTotalStaticIndexSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getTotalStaticBloomSize()", "public long getTotalStaticBloomSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getNumMutationsWithoutWAL()", "public long getNumMutationsWithoutWAL()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getDataInMemoryWithoutWAL()", "public long getDataInMemoryWithoutWAL()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getPercentFileLocal()", "public int getPercentFileLocal()"], ["int", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getPercentFileLocalSecondaryRegions()", "public int getPercentFileLocalSecondaryRegions()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getUpdatesBlockedTime()", "public long getUpdatesBlockedTime()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getFlushedCellsCount()", "public long getFlushedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCompactedCellsCount()", "public long getCompactedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getMajorCompactedCellsCount()", "public long getMajorCompactedCellsCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getFlushedCellsSize()", "public long getFlushedCellsSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getCompactedCellsSize()", "public long getCompactedCellsSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getMajorCompactedCellsSize()", "public long getMajorCompactedCellsSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.getBlockedRequestsCount()", "public long getBlockedRequestsCount()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl)", "public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl)"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegion)", "public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegion)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getTableName()", "public java.lang.String getTableName()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNamespace()", "public java.lang.String getNamespace()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getRegionName()", "public java.lang.String getRegionName()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNumStores()", "public long getNumStores()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNumStoreFiles()", "public long getNumStoreFiles()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getMemstoreSize()", "public long getMemstoreSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getStoreFileSize()", "public long getStoreFileSize()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getReadRequestCount()", "public long getReadRequestCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getWriteRequestCount()", "public long getWriteRequestCount()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNumFilesCompacted()", "public long getNumFilesCompacted()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNumBytesCompacted()", "public long getNumBytesCompacted()"], ["long", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getNumCompactionsCompleted()", "public long getNumCompactionsCompleted()"], ["void", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.close()", "public void close() throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.commons.math.stat.descriptive.DescriptiveStatistics>", "org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.getCoprocessorExecutionStatistics()", "public java.util.Map<java.lang.String, org.apache.commons.math.stat.descriptive.DescriptiveStatistics> getCoprocessorExecutionStatistics()"], ["org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress(T[], org.apache.hadoop.hbase.regionserver.OperationStatus[], org.apache.hadoop.hbase.regionserver.wal.WALEdit[], int, int)", "public org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress(T[], org.apache.hadoop.hbase.regionserver.OperationStatus[], org.apache.hadoop.hbase.regionserver.wal.WALEdit[], int, int)"], ["int", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.size()", "public int size()"], ["T", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.getOperation(int)", "public T getOperation(int)"], ["void", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.setOperationStatus(int, org.apache.hadoop.hbase.regionserver.OperationStatus)", "public void setOperationStatus(int, org.apache.hadoop.hbase.regionserver.OperationStatus)"], ["org.apache.hadoop.hbase.regionserver.OperationStatus", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.getOperationStatus(int)", "public org.apache.hadoop.hbase.regionserver.OperationStatus getOperationStatus(int)"], ["void", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.setWalEdit(int, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void setWalEdit(int, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.getWalEdit(int)", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit getWalEdit(int)"], ["java.util.Collection<byte[]>", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.getRowsToLock()", "public java.util.Collection<byte[]> getRowsToLock()"], ["boolean", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.readOnly()", "public boolean readOnly()"], ["org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorResponse", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.getResult()", "public org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorResponse getResult()"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.process(long, org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void process(long, org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.preProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.preBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void preBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.postBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion)", "public void postBatchMutate(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.postProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit, boolean)", "public void postProcess(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.wal.WALEdit, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorRequest", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.getRequestData()", "public org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorRequest getRequestData()"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.initialize(org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorRequest)", "public void initialize(org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos$MultiRowMutationProcessorRequest)"], ["org.apache.hadoop.hbase.client.Durability", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.useDurability()", "public org.apache.hadoop.hbase.client.Durability useDurability()"], ["void", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.initialize(com.google.protobuf.Message)", "public void initialize(com.google.protobuf.Message) throws java.io.IOException"], ["com.google.protobuf.Message", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.getRequestData()", "public com.google.protobuf.Message getRequestData() throws java.io.IOException"], ["com.google.protobuf.Message", "org.apache.hadoop.hbase.regionserver.MultiRowMutationProcessor.getResult()", "public com.google.protobuf.Message getResult()"], ["org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl()", "public org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl()"], ["void", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.initialize(long)", "public void initialize(long)"], ["long", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.getPreAssignedWriteNumber(java.util.concurrent.atomic.AtomicLong)", "public static long getPreAssignedWriteNumber(java.util.concurrent.atomic.AtomicLong)"], ["org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.beginMemstoreInsertWithSeqNum(long)", "public org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry beginMemstoreInsertWithSeqNum(long)"], ["void", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.completeMemstoreInsertWithSeqNum(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry, org.apache.hadoop.hbase.regionserver.SequenceId)", "public void completeMemstoreInsertWithSeqNum(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry, org.apache.hadoop.hbase.regionserver.SequenceId) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.completeMemstoreInsert(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry)", "public void completeMemstoreInsert(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry)"], ["void", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.waitForPreviousTransactionsComplete()", "public void waitForPreviousTransactionsComplete()"], ["void", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.waitForPreviousTransactionsComplete(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry)", "public void waitForPreviousTransactionsComplete(org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl$WriteEntry)"], ["long", "org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.memstoreReadPoint()", "public long memstoreReadPoint()"], ["org.apache.hadoop.hbase.regionserver.NoLimitScannerContext", "org.apache.hadoop.hbase.regionserver.NoLimitScannerContext()", "public org.apache.hadoop.hbase.regionserver.NoLimitScannerContext()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext", "org.apache.hadoop.hbase.regionserver.NoLimitScannerContext.getInstance()", "public static final org.apache.hadoop.hbase.regionserver.ScannerContext getInstance()"], ["org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner", "org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner()", "public org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult", "org.apache.hadoop.hbase.regionserver.NoOpHeapMemoryTuner.tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext)", "public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext)"], ["org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner()", "public org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner()"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean)", "public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.realSeekDone()", "public boolean realSeekDone()"], ["void", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.enforceSeek()", "public void enforceSeek() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.doRealSeek(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, boolean)", "public static boolean doRealSeek(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)", "public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.isFileScanner()", "public boolean isFileScanner()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner", "org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner()", "public org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner()"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner.backwardSeek(org.apache.hadoop.hbase.Cell)", "public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner.seekToPreviousRow(org.apache.hadoop.hbase.Cell)", "public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner.seekToLastRow()", "public boolean seekToLastRow() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus", "org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode)", "public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode)"], ["org.apache.hadoop.hbase.regionserver.OperationStatus", "org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.String)", "public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.OperationStatus", "org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.Exception)", "public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.Exception)"], ["org.apache.hadoop.hbase.HConstants$OperationStatusCode", "org.apache.hadoop.hbase.regionserver.OperationStatus.getOperationStatusCode()", "public org.apache.hadoop.hbase.HConstants$OperationStatusCode getOperationStatusCode()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.OperationStatus.getExceptionMsg()", "public java.lang.String getExceptionMsg()"], ["org.apache.hadoop.hbase.regionserver.RSDumpServlet", "org.apache.hadoop.hbase.regionserver.RSDumpServlet()", "public org.apache.hadoop.hbase.regionserver.RSDumpServlet()"], ["void", "org.apache.hadoop.hbase.regionserver.RSDumpServlet.doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse)", "public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RSDumpServlet.dumpQueue(org.apache.hadoop.hbase.regionserver.HRegionServer, java.io.PrintWriter)", "public static void dumpQueue(org.apache.hadoop.hbase.regionserver.HRegionServer, java.io.PrintWriter) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerHolder", "org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerHolder(org.apache.hadoop.hbase.regionserver.RegionScanner, org.apache.hadoop.hbase.regionserver.Region)", "public org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerHolder(org.apache.hadoop.hbase.regionserver.RegionScanner, org.apache.hadoop.hbase.regionserver.Region)"], ["void", "org.apache.hadoop.hbase.regionserver.RSRpcServices$ScannerListener.leaseExpired()", "public void leaseExpired()"], ["org.apache.hadoop.hbase.regionserver.RSRpcServices", "org.apache.hadoop.hbase.regionserver.RSRpcServices(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.regionserver.RSRpcServices(org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getHostname(org.apache.hadoop.conf.Configuration, boolean)", "public static java.lang.String getHostname(org.apache.hadoop.conf.Configuration, boolean) throws java.net.UnknownHostException"], ["org.apache.hadoop.hbase.ipc.PriorityFunction", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getPriority()", "public org.apache.hadoop.hbase.ipc.PriorityFunction getPriority()"], ["java.net.InetSocketAddress", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getSocketAddress()", "public java.net.InetSocketAddress getSocketAddress()"], ["int", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getPriority(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)", "public int getPriority(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)"], ["long", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getDeadline(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)", "public long getDeadline(org.apache.hadoop.hbase.protobuf.generated.RPCProtos$RequestHeader, com.google.protobuf.Message)"], ["boolean", "org.apache.hadoop.hbase.regionserver.RSRpcServices.checkOOME(java.lang.Throwable)", "public boolean checkOOME(java.lang.Throwable)"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.closeRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionResponse closeRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CloseRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CompactRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.compactRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CompactRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CompactRegionResponse compactRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$CompactRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$FlushRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.flushRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$FlushRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$FlushRegionResponse flushRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$FlushRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetOnlineRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getOnlineRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetOnlineRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetOnlineRegionResponse getOnlineRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetOnlineRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoResponse getRegionInfo(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetRegionInfoRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetServerInfoResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getServerInfo(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetServerInfoRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetServerInfoResponse getServerInfo(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetServerInfoRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetStoreFileResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.getStoreFile(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetStoreFileRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetStoreFileResponse getStoreFile(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$GetStoreFileRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$MergeRegionsResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.mergeRegions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$MergeRegionsRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$MergeRegionsResponse mergeRegions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$MergeRegionsRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.openRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionResponse openRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$OpenRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WarmupRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.warmupRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WarmupRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WarmupRegionResponse warmupRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WarmupRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.replay(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replay(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.replicateWALEntry(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replicateWALEntry(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$RollWALWriterResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.rollWALWriter(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$RollWALWriterRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$RollWALWriterResponse rollWALWriter(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$RollWALWriterRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$SplitRegionResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.splitRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$SplitRegionRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$SplitRegionResponse splitRegion(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$SplitRegionRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$StopServerResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.stopServer(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$StopServerRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$StopServerResponse stopServer(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$StopServerRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.updateFavoredNodes(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse updateFavoredNodes(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$BulkLoadHFileResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.bulkLoadHFile(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$BulkLoadHFileRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$BulkLoadHFileResponse bulkLoadHFile(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$BulkLoadHFileRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse execService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.get(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetResponse get(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiResponse multi(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutateResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.mutate(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutateRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutateResponse mutate(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutateRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanResponse scan(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.execRegionServerService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest)", "public org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceResponse execRegionServerService(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateConfigurationResponse", "org.apache.hadoop.hbase.regionserver.RSRpcServices.updateConfiguration(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateConfigurationRequest)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateConfigurationResponse updateConfiguration(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$UpdateConfigurationRequest) throws com.google.protobuf.ServiceException"], ["org.apache.hadoop.hbase.regionserver.RSStatusServlet", "org.apache.hadoop.hbase.regionserver.RSStatusServlet()", "public org.apache.hadoop.hbase.regionserver.RSStatusServlet()"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result[]", "org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result.values()", "public static org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result[] values()"], ["org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result", "org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.Region$FlushResult$Result valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.Region$Operation[]", "org.apache.hadoop.hbase.regionserver.Region$Operation.values()", "public static org.apache.hadoop.hbase.regionserver.Region$Operation[] values()"], ["org.apache.hadoop.hbase.regionserver.Region$Operation", "org.apache.hadoop.hbase.regionserver.Region$Operation.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.Region$Operation valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$1.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$10.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$11.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$12.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$13.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$14.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$15.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$16.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$17.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$18.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$2.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$20.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$21.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$22.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$23.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$24.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$25.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$26.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$27.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$28.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$29.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$3.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$30.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$31.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$32.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$33.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$34.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$35.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$36.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$37.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$38.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$39.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$4.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$40.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$41.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$42.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$43.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$44.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$45.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$46.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$47.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$48.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$49.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$5.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$5.postEnvCall(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$50.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$51.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$52.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$53.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$54.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$55.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$56.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$57.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$58.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$59.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$6.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$60.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$61.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$62.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$63.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$64.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$65.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$66.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$67.call(org.apache.hadoop.hbase.coprocessor.EndpointObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.EndpointObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$68.call(org.apache.hadoop.hbase.coprocessor.EndpointObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.EndpointObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$69.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$7.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$8.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$9.call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$CoprocessorOperation.postEnvCall(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment)"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$EndpointOperation.hasCall(org.apache.hadoop.hbase.Coprocessor)", "public boolean hasCall(org.apache.hadoop.hbase.Coprocessor)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$EndpointOperation.call(org.apache.hadoop.hbase.Coprocessor, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.Coprocessor, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$EndpointOperationWithResult.setResult(T)", "public void setResult(T)"], ["T", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$EndpointOperationWithResult.getResult()", "public T getResult()"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object>)", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object>)"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.getRegion()", "public org.apache.hadoop.hbase.regionserver.Region getRegion()"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.getRegionServerServices()", "public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.shutdown()", "public void shutdown()"], ["java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object>", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.getSharedData()", "public java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object> getSharedData()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.offerExecutionLatency(long)", "public void offerExecutionLatency(long)"], ["java.util.Collection<java.lang.Long>", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.getExecutionLatenciesNanos()", "public java.util.Collection<java.lang.Long> getExecutionLatenciesNanos()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionOperation.hasCall(org.apache.hadoop.hbase.Coprocessor)", "public boolean hasCall(org.apache.hadoop.hbase.Coprocessor)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionOperation.call(org.apache.hadoop.hbase.Coprocessor, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.Coprocessor, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionOperationWithResult.setResult(T)", "public void setResult(T)"], ["T", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionOperationWithResult.getResult()", "public T getResult()"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute.getClassName()", "public java.lang.String getClassName()"], ["int", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute.getPriority()", "public int getPriority()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.testTableCoprocessorAttrs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor)", "public static void testTableCoprocessorAttrs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preOpen()", "public void preOpen() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postOpen()", "public void postOpen()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postLogReplay()", "public void postLogReplay()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preClose(boolean)", "public void preClose(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postClose(boolean)", "public void postClose(boolean)"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCompactScannerOpen(org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompactScannerOpen(org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCompactSelection(org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public boolean preCompactSelection(org.apache.hadoop.hbase.regionserver.Store, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCompactSelection(org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void postCompactSelection(org.apache.hadoop.hbase.regionserver.Store, com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCompact(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCompact(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void postCompact(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preFlush(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preFlush(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preFlush()", "public void preFlush() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preFlushScannerOpen(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.KeyValueScanner)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preFlushScannerOpen(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.KeyValueScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postFlush()", "public void postFlush() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postFlush(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile)", "public void postFlush(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preSplit()", "public void preSplit() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preSplit(byte[])", "public void preSplit(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postSplit(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postSplit(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preSplitBeforePONR(byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>)", "public boolean preSplitBeforePONR(byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preSplitAfterPONR()", "public void preSplitAfterPONR() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preRollBackSplit()", "public void preRollBackSplit() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postRollBackSplit()", "public void postRollBackSplit() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCompleteSplit()", "public void postCompleteSplit() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGetClosestRowBefore(byte[], byte[], org.apache.hadoop.hbase.client.Result)", "public boolean preGetClosestRowBefore(byte[], byte[], org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postGetClosestRowBefore(byte[], byte[], org.apache.hadoop.hbase.client.Result)", "public void postGetClosestRowBefore(byte[], byte[], org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public boolean preGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public void postGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preExists(org.apache.hadoop.hbase.client.Get)", "public java.lang.Boolean preExists(org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postExists(org.apache.hadoop.hbase.client.Get, boolean)", "public boolean postExists(org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.prePut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public boolean prePut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get)", "public boolean prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public boolean preDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public boolean preBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void postBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postBatchMutateIndispensably(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean)", "public void postBatchMutateIndispensably(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCheckAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put)", "public java.lang.Boolean preCheckAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCheckAndPutAfterRowLock(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put)", "public java.lang.Boolean preCheckAndPutAfterRowLock(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCheckAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean postCheckAndPut(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCheckAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete)", "public java.lang.Boolean preCheckAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCheckAndDeleteAfterRowLock(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete)", "public java.lang.Boolean preCheckAndDeleteAfterRowLock(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCheckAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean postCheckAndDelete(byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preAppend(org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preAppendAfterRowLock(org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppendAfterRowLock(org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preIncrement(org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preIncrementAfterRowLock(org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrementAfterRowLock(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postAppend(org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result)", "public void postAppend(org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postIncrement(org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result)", "public org.apache.hadoop.hbase.client.Result postIncrement(org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preScannerOpen(org.apache.hadoop.hbase.client.Scan)", "public org.apache.hadoop.hbase.regionserver.RegionScanner preScannerOpen(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.KeyValueScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preStoreScannerOpen(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>)", "public org.apache.hadoop.hbase.regionserver.KeyValueScanner preStoreScannerOpen(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postScannerOpen(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["java.lang.Boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int)", "public java.lang.Boolean preScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean)", "public boolean postScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postScannerFilterRow(org.apache.hadoop.hbase.regionserver.InternalScanner, byte[], int, short)", "public boolean postScannerFilterRow(org.apache.hadoop.hbase.regionserver.InternalScanner, byte[], int, short) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner)", "public boolean preScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void postScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public boolean preWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public boolean preWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALRestore(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.regionserver.wal.HLogKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>)", "public boolean preBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean)", "public boolean postBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postStartRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void postStartRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postCloseRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation)", "public void postCloseRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader preStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader postStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFile$Reader) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["com.google.protobuf.Message", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preEndpointInvocation(com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message)", "public com.google.protobuf.Message preEndpointInvocation(com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postEndpointInvocation(com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message, com.google.protobuf.Message$Builder)", "public void postEndpointInvocation(com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message, com.google.protobuf.Message$Builder) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.postInstantiateDeleteTracker(org.apache.hadoop.hbase.regionserver.DeleteTracker)", "public org.apache.hadoop.hbase.regionserver.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.regionserver.DeleteTracker) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.commons.math.stat.descriptive.DescriptiveStatistics>", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.getCoprocessorExecutionStatistics()", "public java.util.Map<java.lang.String, org.apache.commons.math.stat.descriptive.DescriptiveStatistics> getCoprocessorExecutionStatistics()"], ["org.apache.hadoop.hbase.CoprocessorEnvironment", "org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.RegionMergeRequest.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionMergeRequest.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase[]", "org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase.values()", "public static org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase[] values()"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase", "org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionFactory.create(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl create(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean)"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase)"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase, long)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase, long)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl.getPhase()", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$RegionMergeTransactionPhase getPhase()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl$JournalEntryImpl.getTimeStamp()", "public long getTimeStamp()"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean)"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean, long)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, boolean, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.prepare(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public boolean prepare(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.HRegion execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.stepsAfterPONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.HRegion)", "public org.apache.hadoop.hbase.regionserver.HRegion stepsAfterPONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.prepareMutationsForMerge(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.Mutation>, int)", "public void prepareMutationsForMerge(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.Mutation>, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Put", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long)", "public org.apache.hadoop.hbase.client.Put addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long)"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.stepsBeforePONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, boolean)", "public org.apache.hadoop.hbase.regionserver.HRegion stepsBeforePONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.getMergedRegionInfo(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public static org.apache.hadoop.hbase.HRegionInfo getMergedRegionInfo(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.rollback(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public boolean rollback(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.getMergedRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getMergedRegionInfo()"], ["java.util.List<org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$JournalEntry>", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.getJournal()", "public java.util.List<org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$JournalEntry> getJournal()"], ["org.apache.hadoop.hbase.regionserver.RegionMergeTransaction", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.registerTransactionListener(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$TransactionListener)", "public org.apache.hadoop.hbase.regionserver.RegionMergeTransaction registerTransactionListener(org.apache.hadoop.hbase.regionserver.RegionMergeTransaction$TransactionListener)"], ["org.apache.hadoop.hbase.Server", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.getServer()", "public org.apache.hadoop.hbase.Server getServer()"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.getRegionServerServices()", "public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices()"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.RegionMergeTransactionImpl.execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.Region execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionServerAccounting", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting()", "public org.apache.hadoop.hbase.regionserver.RegionServerAccounting()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting.getGlobalMemstoreSize()", "public long getGlobalMemstoreSize()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting.addAndGetGlobalMemstoreSize(long)", "public long addAndGetGlobalMemstoreSize(long)"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting.addAndGetRegionReplayEditsSize(byte[], long)", "public long addAndGetRegionReplayEditsSize(byte[], long)"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting.rollbackRegionReplayEditsSize(byte[])", "public long rollbackRegionReplayEditsSize(byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerAccounting.clearRegionReplayEditsSize(byte[])", "public void clearRegionReplayEditsSize(byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$1.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$1.postEnvCall(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$10.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$11.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$12.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$2.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$3.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$4.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$5.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$6.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$7.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$8.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$9.call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessOperationWithResult.setResult(T)", "public void setResult(T)"], ["T", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessOperationWithResult.getResult()", "public T getResult()"], ["org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessorOperation", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessorOperation()", "public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessorOperation()"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$CoprocessorOperation.postEnvCall(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment)", "public void postEnvCall(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment)"], ["int", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$EnvironmentPriorityComparator.compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment)", "public int compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["int", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$EnvironmentPriorityComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment.getRegionServerServices()", "public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices()"], ["org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preStop(java.lang.String)", "public void preStop(java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public boolean preMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public void postMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preMergeCommit(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<org.apache.hadoop.hbase.client.Mutation>)", "public boolean preMergeCommit(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postMergeCommit(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public void postMergeCommit(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preRollBackMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public void preRollBackMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postRollBackMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion)", "public void postRollBackMerge(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preRollWALWriterRequest()", "public void preRollWALWriterRequest() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postRollWALWriterRequest()", "public void postRollWALWriterRequest() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.preReplicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void preReplicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postReplicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void postReplicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.postCreateReplicationEndPoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint) throws java.io.IOException"], ["org.apache.hadoop.hbase.CoprocessorEnvironment", "org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext", "org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext(org.apache.hadoop.hbase.regionserver.Region, long)", "public org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext(org.apache.hadoop.hbase.regionserver.Region, long)"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext.getRegion()", "public org.apache.hadoop.hbase.regionserver.Region getRegion()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext.getMasterSystemTime()", "public long getMasterSystemTime()"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext", "org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, long, org.apache.hadoop.hbase.HRegionInfo...)", "public org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext(org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, long, org.apache.hadoop.hbase.HRegionInfo...)"], ["org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode", "org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext.getCode()", "public org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode getCode()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext.getOpenSeqNum()", "public long getOpenSeqNum()"], ["long", "org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext.getMasterSystemTime()", "public long getMasterSystemTime()"], ["org.apache.hadoop.hbase.HRegionInfo[]", "org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext.getHris()", "public org.apache.hadoop.hbase.HRegionInfo[] getHris()"], ["org.apache.hadoop.hbase.regionserver.RegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.RegionSplitPolicy()", "public org.apache.hadoop.hbase.regionserver.RegionSplitPolicy()"], ["org.apache.hadoop.hbase.regionserver.RegionSplitPolicy", "org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.RegionSplitPolicy create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.RegionSplitPolicy>", "org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.getSplitPolicyClass(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration)", "public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.RegionSplitPolicy> getSplitPolicyClass(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["int", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator.compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner)", "public int compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner)"], ["int", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator.compareRows(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public int compareRows(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["int", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.KeyValue$KVComparator) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean)", "public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.seekToPreviousRow(org.apache.hadoop.hbase.Cell)", "public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.backwardSeek(org.apache.hadoop.hbase.Cell)", "public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.next()", "public org.apache.hadoop.hbase.Cell next() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap.seekToLastRow()", "public boolean seekToLastRow() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedStoreScanner.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedStoreScanner.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedStoreScanner.seekToPreviousRow(org.apache.hadoop.hbase.Cell)", "public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ReversedStoreScanner.backwardSeek(org.apache.hadoop.hbase.Cell)", "public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RowTooBigException", "org.apache.hadoop.hbase.regionserver.RowTooBigException(java.lang.String)", "public org.apache.hadoop.hbase.regionserver.RowTooBigException(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.ScanDeleteTracker", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker()", "public org.apache.hadoop.hbase.regionserver.ScanDeleteTracker()"], ["void", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.add(org.apache.hadoop.hbase.Cell)", "public void add(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.isDeleted(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.isEmpty()", "public boolean isEmpty()"], ["void", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.reset()", "public void reset()"], ["void", "org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.update()", "public void update()"], ["org.apache.hadoop.hbase.regionserver.ScanInfo", "org.apache.hadoop.hbase.regionserver.ScanInfo(org.apache.hadoop.hbase.HColumnDescriptor, long, long, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.ScanInfo(org.apache.hadoop.hbase.HColumnDescriptor, long, long, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.regionserver.ScanInfo", "org.apache.hadoop.hbase.regionserver.ScanInfo(byte[], int, int, long, org.apache.hadoop.hbase.KeepDeletedCells, long, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.ScanInfo(byte[], int, int, long, org.apache.hadoop.hbase.KeepDeletedCells, long, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.ScanInfo.getFamily()", "public byte[] getFamily()"], ["int", "org.apache.hadoop.hbase.regionserver.ScanInfo.getMinVersions()", "public int getMinVersions()"], ["int", "org.apache.hadoop.hbase.regionserver.ScanInfo.getMaxVersions()", "public int getMaxVersions()"], ["long", "org.apache.hadoop.hbase.regionserver.ScanInfo.getTtl()", "public long getTtl()"], ["org.apache.hadoop.hbase.KeepDeletedCells", "org.apache.hadoop.hbase.regionserver.ScanInfo.getKeepDeletedCells()", "public org.apache.hadoop.hbase.KeepDeletedCells getKeepDeletedCells()"], ["long", "org.apache.hadoop.hbase.regionserver.ScanInfo.getTimeToPurgeDeletes()", "public long getTimeToPurgeDeletes()"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.regionserver.ScanInfo.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode[]", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode.values()", "public static org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode[] values()"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.regionserver.ScanType, long, long, long, long, org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.regionserver.ScanType, long, long, long, long, org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, long, long, long, long, byte[], byte[], org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, long, long, long, long, byte[], byte[], org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.hasNullColumnInQuery()", "public boolean hasNullColumnInQuery()"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.moreRowsMayExistAfter(org.apache.hadoop.hbase.Cell)", "public boolean moreRowsMayExistAfter(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.setRow(byte[], int, short)", "public void setRow(byte[], int, short)"], ["void", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.reset()", "public void reset()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.getStartKey()", "public org.apache.hadoop.hbase.Cell getStartKey()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.getNextKeyHint(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell getNextKeyHint(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.getKeyForNextColumn(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell getKeyForNextColumn(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.getKeyForNextRow(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell getKeyForNextRow(org.apache.hadoop.hbase.Cell)"], ["int", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.compareKeyForNextRow(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public int compareKeyForNextRow(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["int", "org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.compareKeyForNextColumn(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public int compareKeyForNextColumn(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.regionserver.ScanType[]", "org.apache.hadoop.hbase.regionserver.ScanType.values()", "public static org.apache.hadoop.hbase.regionserver.ScanType[] values()"], ["org.apache.hadoop.hbase.regionserver.ScanType", "org.apache.hadoop.hbase.regionserver.ScanType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.ScanType valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker(int, int, long)", "public org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker(int, int, long)"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkColumn(byte[], int, int, byte)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode checkColumn(byte[], int, int, byte) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkVersions(byte[], int, int, long, byte, boolean)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode checkVersions(byte[], int, int, long, byte, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.reset()", "public void reset()"], ["org.apache.hadoop.hbase.regionserver.ColumnCount", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.getColumnHint()", "public org.apache.hadoop.hbase.regionserver.ColumnCount getColumnHint()"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.done()", "public boolean done()"], ["org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.getNextRowOrNextColumn(byte[], int, int)", "public org.apache.hadoop.hbase.regionserver.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.isDone(long)", "public boolean isDone(long)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext$Builder.setKeepProgress(boolean)", "public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setKeepProgress(boolean)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext$Builder.setSizeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long)", "public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setSizeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext$Builder.setTimeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long)", "public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setTimeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext$Builder.setBatchLimit(int)", "public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setBatchLimit(int)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext", "org.apache.hadoop.hbase.regionserver.ScannerContext$Builder.build()", "public org.apache.hadoop.hbase.regionserver.ScannerContext build()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.ScannerContext$LimitFields.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope[]", "org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope.values()", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope[] values()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope", "org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$NextState[]", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.values()", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$NextState[] values()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$NextState", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$NextState valueOf(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.hasMoreValues()", "public boolean hasMoreValues()"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.limitReached()", "public boolean limitReached()"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.isValidState(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState)", "public static boolean isValidState(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ScannerContext$NextState.hasMoreValues(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState)", "public static boolean hasMoreValues(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.ScannerContext.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext.newBuilder()", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$Builder newBuilder()"], ["org.apache.hadoop.hbase.regionserver.ScannerContext$Builder", "org.apache.hadoop.hbase.regionserver.ScannerContext.newBuilder(boolean)", "public static org.apache.hadoop.hbase.regionserver.ScannerContext$Builder newBuilder(boolean)"], ["org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey(long, long)", "public org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey(long, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey.hashCode()", "public int hashCode()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$NonceKey.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext()", "public org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext()"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.setState(int)", "public void setState(int)"], ["int", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.getState()", "public int getState()"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.setHasWait()", "public void setHasWait()"], ["boolean", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.hasWait()", "public boolean hasWait()"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.reportActivity()", "public void reportActivity()"], ["boolean", "org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext.isExpired(long)", "public boolean isExpired(long)"], ["org.apache.hadoop.hbase.regionserver.ServerNonceManager", "org.apache.hadoop.hbase.regionserver.ServerNonceManager(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.ServerNonceManager(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager.setConflictWaitIterationMs(int)", "public void setConflictWaitIterationMs(int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ServerNonceManager.startOperation(long, long, org.apache.hadoop.hbase.Stoppable)", "public boolean startOperation(long, long, org.apache.hadoop.hbase.Stoppable) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager.endOperation(long, long, boolean)", "public void endOperation(long, long, boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.ServerNonceManager.reportOperationFromWal(long, long, long)", "public void reportOperationFromWal(long, long, long)"], ["org.apache.hadoop.hbase.ScheduledChore", "org.apache.hadoop.hbase.regionserver.ServerNonceManager.createCleanupScheduledChore(org.apache.hadoop.hbase.Stoppable)", "public org.apache.hadoop.hbase.ScheduledChore createCleanupScheduledChore(org.apache.hadoop.hbase.Stoppable)"], ["boolean", "org.apache.hadoop.hbase.regionserver.ShutdownHook$DoNothingStoppable.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.regionserver.ShutdownHook$DoNothingStoppable.stop(java.lang.String)", "public void stop(java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.ShutdownHook$DoNothingThread.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.regionserver.ShutdownHook$ShutdownHookThread.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.ShutdownHook", "org.apache.hadoop.hbase.regionserver.ShutdownHook()", "public org.apache.hadoop.hbase.regionserver.ShutdownHook()"], ["void", "org.apache.hadoop.hbase.regionserver.ShutdownHook.install(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Stoppable, java.lang.Thread)", "public static void install(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Stoppable, java.lang.Thread)"], ["void", "org.apache.hadoop.hbase.regionserver.ShutdownHook.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory", "org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory()", "public org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory()"], ["org.apache.hadoop.hbase.ipc.RpcScheduler", "org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction)", "public org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction)"], ["org.apache.hadoop.hbase.ipc.RpcScheduler", "org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable)"], ["org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status", "org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode, org.apache.hadoop.hbase.util.CancelableProgressable)", "public org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status exec(java.lang.String, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode, org.apache.hadoop.hbase.util.CancelableProgressable)"], ["org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status[]", "org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status.values()", "public static org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status[] values()"], ["org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status", "org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.SplitLogWorker", "org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor)", "public org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor)"], ["org.apache.hadoop.hbase.regionserver.SplitLogWorker", "org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.wal.WALFactory)", "public org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.wal.WALFactory)"], ["void", "org.apache.hadoop.hbase.regionserver.SplitLogWorker.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.regionserver.SplitLogWorker.stopTask()", "public void stopTask()"], ["void", "org.apache.hadoop.hbase.regionserver.SplitLogWorker.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.regionserver.SplitLogWorker.stop()", "public void stop()"], ["int", "org.apache.hadoop.hbase.regionserver.SplitLogWorker.getTaskReadySeq()", "public int getTaskReadySeq()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.SplitRequest.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.regionserver.SplitRequest.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase[]", "org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase.values()", "public static org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase[] values()"], ["org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase", "org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.SplitTransactionFactory", "org.apache.hadoop.hbase.regionserver.SplitTransactionFactory(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.SplitTransactionFactory(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.regionserver.SplitTransactionFactory.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.regionserver.SplitTransactionFactory.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.SplitTransaction", "org.apache.hadoop.hbase.regionserver.SplitTransactionFactory.create(org.apache.hadoop.hbase.regionserver.Region, byte[])", "public org.apache.hadoop.hbase.regionserver.SplitTransaction create(org.apache.hadoop.hbase.regionserver.Region, byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$DaughterOpener.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase)", "public org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase)"], ["org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase, long)", "public org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl(org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase, long)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl.getPhase()", "public org.apache.hadoop.hbase.regionserver.SplitTransaction$SplitTransactionPhase getPhase()"], ["long", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$JournalEntryImpl.getTimeStamp()", "public long getTimeStamp()"], ["boolean", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$LoggingProgressable.progress()", "public boolean progress()"], ["org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$StoreFileSplitter", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$StoreFileSplitter(org.apache.hadoop.hbase.regionserver.SplitTransactionImpl, byte[], org.apache.hadoop.hbase.regionserver.StoreFile)", "public org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$StoreFileSplitter(org.apache.hadoop.hbase.regionserver.SplitTransactionImpl, byte[], org.apache.hadoop.hbase.regionserver.StoreFile)"], ["org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$StoreFileSplitter.call()", "public org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path> call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl$StoreFileSplitter.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.regionserver.SplitTransactionImpl", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, byte[])", "public org.apache.hadoop.hbase.regionserver.SplitTransactionImpl(org.apache.hadoop.hbase.regionserver.Region, byte[])"], ["boolean", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.prepare()", "public boolean prepare() throws java.io.IOException"], ["org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.stepsBeforePONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, boolean)", "public org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region> stepsBeforePONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region> execute(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region>", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.stepsAfterPONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region>)", "public org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region> stepsAfterPONR(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.regionserver.Region>) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Put", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long)", "public org.apache.hadoop.hbase.client.Put addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.rollback(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public boolean rollback(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.SplitTransaction$JournalEntry>", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.getJournal()", "public java.util.List<org.apache.hadoop.hbase.regionserver.SplitTransaction$JournalEntry> getJournal()"], ["org.apache.hadoop.hbase.regionserver.SplitTransaction", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.registerTransactionListener(org.apache.hadoop.hbase.regionserver.SplitTransaction$TransactionListener)", "public org.apache.hadoop.hbase.regionserver.SplitTransaction registerTransactionListener(org.apache.hadoop.hbase.regionserver.SplitTransaction$TransactionListener)"], ["org.apache.hadoop.hbase.Server", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.getServer()", "public org.apache.hadoop.hbase.Server getServer()"], ["org.apache.hadoop.hbase.regionserver.RegionServerServices", "org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.getRegionServerServices()", "public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices()"], ["org.apache.hadoop.hbase.regionserver.StoreEngine", "org.apache.hadoop.hbase.regionserver.StoreEngine()", "public org.apache.hadoop.hbase.regionserver.StoreEngine()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy", "org.apache.hadoop.hbase.regionserver.StoreEngine.getCompactionPolicy()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy getCompactionPolicy()"], ["org.apache.hadoop.hbase.regionserver.compactions.Compactor", "org.apache.hadoop.hbase.regionserver.StoreEngine.getCompactor()", "public org.apache.hadoop.hbase.regionserver.compactions.Compactor getCompactor()"], ["org.apache.hadoop.hbase.regionserver.StoreFileManager", "org.apache.hadoop.hbase.regionserver.StoreEngine.getStoreFileManager()", "public org.apache.hadoop.hbase.regionserver.StoreFileManager getStoreFileManager()"], ["org.apache.hadoop.hbase.regionserver.StoreFlusher", "org.apache.hadoop.hbase.regionserver.StoreEngine.getStoreFlusher()", "public org.apache.hadoop.hbase.regionserver.StoreFlusher getStoreFlusher()"], ["org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?>", "org.apache.hadoop.hbase.regionserver.StoreEngine.create(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public static org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?> create(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.KeyValue$KVComparator) throws java.io.IOException"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetBulkTime.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public java.lang.Long apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetBulkTime.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetFileSize.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public java.lang.Long apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetFileSize.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetPathName.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public java.lang.String apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetPathName.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetSeqId.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public java.lang.Long apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators$GetSeqId.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Comparators", "org.apache.hadoop.hbase.regionserver.StoreFile$Comparators()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Comparators()"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["org.apache.hadoop.hbase.regionserver.StoreFileScanner", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getStoreFileScanner(boolean, boolean)", "public org.apache.hadoop.hbase.regionserver.StoreFileScanner getStoreFileScanner(boolean, boolean)"], ["org.apache.hadoop.hbase.regionserver.StoreFileScanner", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getStoreFileScanner(boolean, boolean, boolean, long)", "public org.apache.hadoop.hbase.regionserver.StoreFileScanner getStoreFileScanner(boolean, boolean, boolean, long)"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getScanner(boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean)"], ["org.apache.hadoop.hbase.io.hfile.HFileScanner", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getScanner(boolean, boolean, boolean)", "public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.close(boolean)", "public void close(boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesDeleteFamilyBloomFilter(byte[], int, int)", "public boolean passesDeleteFamilyBloomFilter(byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesGeneralBloomFilter(byte[], int, int, byte[], int, int)", "public boolean passesGeneralBloomFilter(byte[], int, int, byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan)", "public boolean passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan)"], ["java.util.Map<byte[], byte[]>", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.loadFileInfo()", "public java.util.Map<byte[], byte[]> loadFileInfo() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.loadBloomfilter()", "public void loadBloomfilter()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getFilterEntries()", "public long getFilterEntries()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.setGeneralBloomFilterFaulty()", "public void setGeneralBloomFilterFaulty()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.setDeleteFamilyBloomFilterFaulty()", "public void setDeleteFamilyBloomFilterFaulty()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getLastKey()", "public byte[] getLastKey()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getLastRowKey()", "public byte[] getLastRowKey()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.midkey()", "public byte[] midkey() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.length()", "public long length()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getTotalUncompressedBytes()", "public long getTotalUncompressedBytes()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getEntries()", "public long getEntries()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getDeleteFamilyCnt()", "public long getDeleteFamilyCnt()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getFirstKey()", "public byte[] getFirstKey()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.indexSize()", "public long indexSize()"], ["org.apache.hadoop.hbase.regionserver.BloomType", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getBloomFilterType()", "public org.apache.hadoop.hbase.regionserver.BloomType getBloomFilterType()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getSequenceID()", "public long getSequenceID()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.setSequenceID(long)", "public void setSequenceID(long)"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.setBulkLoaded(boolean)", "public void setBulkLoaded(boolean)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.isBulkLoaded()", "public boolean isBulkLoaded()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getTotalBloomSize()", "public long getTotalBloomSize()"], ["int", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getHFileVersion()", "public int getHFileVersion()"], ["int", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getHFileMinorVersion()", "public int getHFileMinorVersion()"], ["org.apache.hadoop.hbase.io.hfile.HFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getHFileReader()", "public org.apache.hadoop.hbase.io.hfile.HFile$Reader getHFileReader()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getMaxTimestamp()", "public long getMaxTimestamp()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.appendMetadata(long, boolean)", "public void appendMetadata(long, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.appendTrackedTimestampsToMetadata()", "public void appendTrackedTimestampsToMetadata() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.setTimeRangeTracker(org.apache.hadoop.hbase.regionserver.TimeRangeTracker)", "public void setTimeRangeTracker(org.apache.hadoop.hbase.regionserver.TimeRangeTracker)"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.trackTimestamps(org.apache.hadoop.hbase.Cell)", "public void trackTimestamps(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(org.apache.hadoop.hbase.Cell)", "public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile$Writer.appendFileInfo(byte[], byte[])", "public void appendFileInfo(byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withOutputDir(org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withOutputDir(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withFilePath(org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withFilePath(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withFavoredNodes(java.net.InetSocketAddress[])", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withFavoredNodes(java.net.InetSocketAddress[])"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withComparator(org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withBloomType(org.apache.hadoop.hbase.regionserver.BloomType)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withBloomType(org.apache.hadoop.hbase.regionserver.BloomType)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withMaxKeyCount(long)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withMaxKeyCount(long)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext)", "public org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Writer", "org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Writer build() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getMaxMemstoreTS()", "public long getMaxMemstoreTS()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile.setMaxMemstoreTS(long)", "public void setMaxMemstoreTS(long)"], ["org.apache.hadoop.hbase.regionserver.StoreFile", "org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType)", "public org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile", "org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.StoreFileInfo, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType)", "public org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.StoreFileInfo, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile", "org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.hbase.regionserver.StoreFile)", "public org.apache.hadoop.hbase.regionserver.StoreFile(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.regionserver.StoreFile.getFileInfo()", "public org.apache.hadoop.hbase.regionserver.StoreFileInfo getFileInfo()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFile.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFile.getQualifiedPath()", "public org.apache.hadoop.fs.Path getQualifiedPath()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile.isReference()", "public boolean isReference()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile.isMajorCompaction()", "public boolean isMajorCompaction()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFile.excludeFromMinorCompaction()", "public boolean excludeFromMinorCompaction()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getMaxSequenceId()", "public long getMaxSequenceId()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getModificationTimeStamp()", "public long getModificationTimeStamp() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StoreFile.getMetadataValue(byte[])", "public byte[] getMetadataValue(byte[])"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getMaxMemstoreTSInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static long getMaxMemstoreTSInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getMaxSequenceIdInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static long getMaxSequenceIdInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFile.getBulkLoadTimestamp()", "public long getBulkLoadTimestamp()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.StoreFile.getHDFSBlockDistribution()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlockDistribution()"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFile.createReader()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader createReader() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFile.getReader()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader getReader()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.StoreFile.void closeReader(boolean)", "public synchronized void closeReader(boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFile.deleteReader()", "public void deleteReader() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.StoreFile.toString()", "public java.lang.String toString()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.StoreFile.toStringDetailed()", "public java.lang.String toStringDetailed()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFile.getUniqueFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getUniqueFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.StoreFile.getMinimumTimestamp()", "public java.lang.Long getMinimumTimestamp()"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus)", "public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.HFileLink)", "public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.HFileLink) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.Reference)", "public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.Reference) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.setRegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)", "public void setRegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost)"], ["org.apache.hadoop.hbase.io.Reference", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getReference()", "public org.apache.hadoop.hbase.io.Reference getReference()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isReference()", "public boolean isReference()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isTopReference()", "public boolean isTopReference()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isLink()", "public boolean isLink()"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getHDFSBlockDistribution()", "public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlockDistribution()"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Reader", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.open(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public org.apache.hadoop.hbase.regionserver.StoreFile$Reader open(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.hfile.CacheConfig) throws java.io.IOException"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getReferencedFileStatus(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.fs.FileStatus getReferencedFileStatus(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getPath()", "public org.apache.hadoop.fs.Path getPath()"], ["org.apache.hadoop.fs.FileStatus", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getFileStatus()", "public org.apache.hadoop.fs.FileStatus getFileStatus() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getModificationTime()", "public long getModificationTime() throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isHFile(org.apache.hadoop.fs.Path)", "public static boolean isHFile(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isHFile(java.lang.String)", "public static boolean isHFile(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isReference(org.apache.hadoop.fs.Path)", "public static boolean isReference(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isReference(java.lang.String)", "public static boolean isReference(java.lang.String)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.getReferredToFile(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getReferredToFile(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.validateStoreFileName(java.lang.String)", "public static boolean validateStoreFileName(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.isValid(org.apache.hadoop.fs.FileStatus)", "public static boolean isValid(org.apache.hadoop.fs.FileStatus) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.regionserver.StoreFileInfo.hashCode()", "public int hashCode()"], ["org.apache.hadoop.hbase.regionserver.StoreFileScanner", "org.apache.hadoop.hbase.regionserver.StoreFileScanner(org.apache.hadoop.hbase.regionserver.StoreFile$Reader, org.apache.hadoop.hbase.io.hfile.HFileScanner, boolean, boolean, long)", "public org.apache.hadoop.hbase.regionserver.StoreFileScanner(org.apache.hadoop.hbase.regionserver.StoreFile$Reader, org.apache.hadoop.hbase.io.hfile.HFileScanner, boolean, boolean, long)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, long)", "public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, long) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean, long)", "public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean, long) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.ScanQueryMatcher, long)", "public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.ScanQueryMatcher, long) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.peek()", "public org.apache.hadoop.hbase.Cell peek()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.next()", "public org.apache.hadoop.hbase.Cell next() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(org.apache.hadoop.hbase.io.hfile.HFileScanner, org.apache.hadoop.hbase.Cell)", "public static boolean seekAtOrAfter(org.apache.hadoop.hbase.io.hfile.HFileScanner, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.getSequenceID()", "public long getSequenceID()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean)", "public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.realSeekDone()", "public boolean realSeekDone()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.enforceSeek()", "public void enforceSeek() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.setScanQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanQueryMatcher)", "public void setScanQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanQueryMatcher)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.isFileScanner()", "public boolean isFileScanner()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)", "public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, java.util.SortedSet<byte[]>, long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToPreviousRow(org.apache.hadoop.hbase.Cell)", "public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToLastRow()", "public boolean seekToLastRow() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.backwardSeek(org.apache.hadoop.hbase.Cell)", "public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreFileScanner.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.regionserver.StoreFlusher", "org.apache.hadoop.hbase.regionserver.StoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)", "public org.apache.hadoop.hbase.regionserver.StoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)"], ["org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace[]", "org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace.values()", "public static org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace[] values()"], ["org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace", "org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.StoreScanner$StoreScannerCompactionRace valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.StoreScanner", "org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long)", "public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreScanner", "org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, long)", "public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, long) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreScanner", "org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, long, long, byte[], byte[])", "public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, long, long, byte[], byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreScanner.peek()", "public org.apache.hadoop.hbase.Cell peek()"], ["org.apache.hadoop.hbase.KeyValue", "org.apache.hadoop.hbase.regionserver.StoreScanner.next()", "public org.apache.hadoop.hbase.KeyValue next()"], ["void", "org.apache.hadoop.hbase.regionserver.StoreScanner.close()", "public void close()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreScanner.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreScanner.next(java.util.List<org.apache.hadoop.hbase.Cell>)", "public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreScanner.next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext)", "public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders()", "public void updateReaders() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.StoreScanner.getSequenceID()", "public long getSequenceID()"], ["long", "org.apache.hadoop.hbase.regionserver.StoreScanner.getEstimatedNumberOfKvsScanned()", "public long getEstimatedNumberOfKvsScanned()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreScanner.getNextIndexedKey()", "public org.apache.hadoop.hbase.Cell getNextIndexedKey()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.regionserver.StoreScanner.next()", "public org.apache.hadoop.hbase.Cell next() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StoreUtils", "org.apache.hadoop.hbase.regionserver.StoreUtils()", "public org.apache.hadoop.hbase.regionserver.StoreUtils()"], ["java.lang.Integer", "org.apache.hadoop.hbase.regionserver.StoreUtils.getDeterministicRandomSeed(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static java.lang.Integer getDeterministicRandomSeed(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StoreUtils.hasReferences(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static boolean hasReferences(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["long", "org.apache.hadoop.hbase.regionserver.StoreUtils.getLowestTimestamp(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static long getLowestTimestamp(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StorefileRefresherChore", "org.apache.hadoop.hbase.regionserver.StorefileRefresherChore(int, boolean, org.apache.hadoop.hbase.regionserver.HRegionServer, org.apache.hadoop.hbase.Stoppable)", "public org.apache.hadoop.hbase.regionserver.StorefileRefresherChore(int, boolean, org.apache.hadoop.hbase.regionserver.HRegionServer, org.apache.hadoop.hbase.Stoppable)"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter(java.util.List<byte[]>, byte[], byte[])", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter(java.util.List<byte[]>, byte[], byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter.append(org.apache.hadoop.hbase.Cell)", "public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter(int, long, byte[], byte[])", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter(int, long, byte[], byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter.append(org.apache.hadoop.hbase.Cell)", "public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter()", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter()"], ["void", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter.init(org.apache.hadoop.hbase.regionserver.StoreScanner, org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$WriterFactory, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public void init(org.apache.hadoop.hbase.regionserver.StoreScanner, org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$WriterFactory, org.apache.hadoop.hbase.KeyValue$KVComparator) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter.setNoStripeMetadata()", "public void setNoStripeMetadata()"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter.commitWriters(long, boolean)", "public java.util.List<org.apache.hadoop.fs.Path> commitWriters(long, boolean) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter.abortWriters()", "public java.util.List<org.apache.hadoop.fs.Path> abortWriters()"], ["org.apache.hadoop.hbase.regionserver.StripeStoreConfig", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)", "public org.apache.hadoop.hbase.regionserver.StripeStoreConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)"], ["float", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getMaxSplitImbalance()", "public float getMaxSplitImbalance()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getLevel0MinFiles()", "public int getLevel0MinFiles()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getStripeCompactMinFiles()", "public int getStripeCompactMinFiles()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getStripeCompactMaxFiles()", "public int getStripeCompactMaxFiles()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.isUsingL0Flush()", "public boolean isUsingL0Flush()"], ["long", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getSplitSize()", "public long getSplitSize()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getInitialCount()", "public int getInitialCount()"], ["float", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getSplitCount()", "public float getSplitCount()"], ["long", "org.apache.hadoop.hbase.regionserver.StripeStoreConfig.getSplitPartSize()", "public long getSplitPartSize()"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine$StripeCompaction.preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine$StripeCompaction.select(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean)", "public boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine$StripeCompaction.forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine$StripeCompaction.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreEngine", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine()", "public org.apache.hadoop.hbase.regionserver.StripeStoreEngine()"], ["boolean", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine.needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionContext", "org.apache.hadoop.hbase.regionserver.StripeStoreEngine.createCompaction()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction()"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager, boolean)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager, boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy.mergeResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void mergeResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists)"], ["java.util.ArrayList<java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator.getComponents()", "public java.util.ArrayList<java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>> getComponents()"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator.removeComponents(int)", "public void removeComponents(int)"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator.remove()", "public void remove()"], ["java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists.iterator()", "public java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile> iterator()"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFileManager", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StripeStoreConfig)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StripeStoreConfig)"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStorefiles()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getStorefiles()"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.clearFiles()", "public com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.StoreFile> clearFiles()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStorefileCount()", "public int getStorefileCount()"], ["java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue)", "public java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile> getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue)"], ["java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell)", "public java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile> updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getSplitPoint()", "public byte[] getSplitPoint() throws java.io.IOException"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getFilesForScanOrGet(boolean, byte[], byte[])", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getFilesForScanOrGet(boolean, byte[], byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public void addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStoreCompactionPriority()", "public int getStoreCompactionPriority()"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStartRow(int)", "public final byte[] getStartRow(int)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getEndRow(int)", "public final byte[] getEndRow(int)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getLevel0Files()", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> getLevel0Files()"], ["java.util.List<byte[]>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStripeBoundaries()", "public java.util.List<byte[]> getStripeBoundaries()"], ["java.util.ArrayList<com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStripes()", "public java.util.ArrayList<com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.StoreFile>> getStripes()"], ["int", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getStripeCount()", "public int getStripeCount()"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["double", "org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.getCompactionPressure()", "public double getCompactionPressure()"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Writer", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$1.createWriter()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Writer createWriter() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest(java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest(java.util.List<byte[]>)"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest.createWriter()", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest(int, long)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest(int, long)"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest.createWriter()", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest()", "public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest()"], ["org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest.createWriter()", "public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFlusher", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy, org.apache.hadoop.hbase.regionserver.StripeStoreFileManager)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy, org.apache.hadoop.hbase.regionserver.StripeStoreFileManager)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.StripeStoreFlusher.flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.TimeRangeTracker", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker()", "public org.apache.hadoop.hbase.regionserver.TimeRangeTracker()"], ["org.apache.hadoop.hbase.regionserver.TimeRangeTracker", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker(org.apache.hadoop.hbase.regionserver.TimeRangeTracker)", "public org.apache.hadoop.hbase.regionserver.TimeRangeTracker(org.apache.hadoop.hbase.regionserver.TimeRangeTracker)"], ["org.apache.hadoop.hbase.regionserver.TimeRangeTracker", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker(long, long)", "public org.apache.hadoop.hbase.regionserver.TimeRangeTracker(long, long)"], ["void", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.includeTimestamp(org.apache.hadoop.hbase.Cell)", "public void includeTimestamp(org.apache.hadoop.hbase.Cell)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.boolean includesTimeRange(org.apache.hadoop.hbase.io.TimeRange)", "public synchronized boolean includesTimeRange(org.apache.hadoop.hbase.io.TimeRange)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.long getMinimumTimestamp()", "public synchronized long getMinimumTimestamp()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.long getMaximumTimestamp()", "public synchronized long getMaximumTimestamp()"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.void write(java.io.DataOutput)", "public synchronized void write(java.io.DataOutput) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.void readFields(java.io.DataInput)", "public synchronized void readFields(java.io.DataInput) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.TimeRangeTracker.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.UnexpectedStateException", "org.apache.hadoop.hbase.regionserver.UnexpectedStateException()", "public org.apache.hadoop.hbase.regionserver.UnexpectedStateException()"], ["org.apache.hadoop.hbase.regionserver.UnexpectedStateException", "org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.String)", "public org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.UnexpectedStateException", "org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.String, java.lang.Throwable)", "public org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.regionserver.UnexpectedStateException", "org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.Throwable)", "public org.apache.hadoop.hbase.regionserver.UnexpectedStateException(java.lang.Throwable)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMinCompactSize()", "public long getMinCompactSize()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMaxCompactSize()", "public long getMaxCompactSize()"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMinFilesToCompact()", "public int getMinFilesToCompact()"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMaxFilesToCompact()", "public int getMaxFilesToCompact()"], ["double", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getCompactionRatio()", "public double getCompactionRatio()"], ["double", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getCompactionRatioOffPeak()", "public double getCompactionRatioOffPeak()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getThrottlePoint()", "public long getThrottlePoint()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMajorCompactionPeriod()", "public long getMajorCompactionPeriod()"], ["float", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMajorCompactionJitter()", "public float getMajorCompactionJitter()"], ["float", "org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.getMinLocalityToForceCompact()", "public float getMinLocalityToForceCompact()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionContext", "org.apache.hadoop.hbase.regionserver.compactions.CompactionContext()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionContext.forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.CompactionContext.getRequest()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest getRequest()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionContext.hasSelection()", "public boolean hasSelection()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy", "org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration", "org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy.getConf()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration getConf()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress(long)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress(long)"], ["float", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.getProgressPct()", "public float getProgressPct()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.cancel()", "public void cancel()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.complete()", "public void complete()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.getTotalCompactingKvs()", "public long getTotalCompactingKvs()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.getCurrentCompactedKvs()", "public long getCurrentCompactedKvs()"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.getTotalCompactedSize()", "public long getTotalCompactedSize()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$1.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public boolean apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$1.apply(java.lang.Object)", "public boolean apply(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$2.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public java.lang.String apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$2.apply(java.lang.Object)", "public java.lang.Object apply(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType[]", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType.values()", "public static org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType[] values()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest$DisplayCompactionType valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.beforeExecute()", "public void beforeExecute()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.afterExecute()", "public void afterExecute()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.combineWith(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest combineWith(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.compareTo(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public int compareTo(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.getFiles()", "public java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile> getFiles()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.setDescription(java.lang.String, java.lang.String)", "public void setDescription(java.lang.String, java.lang.String)"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.getSize()", "public long getSize()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.isAllFiles()", "public boolean isAllFiles()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.isMajor()", "public boolean isMajor()"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.getPriority()", "public int getPriority()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.setPriority(int)", "public void setPriority(int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.isOffPeak()", "public boolean isOffPeak()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.setOffPeak(boolean)", "public void setOffPeak(boolean)"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.getSelectionTime()", "public long getSelectionTime()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.setIsMajor(boolean, boolean)", "public void setIsMajor(boolean, boolean)"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.toString()", "public java.lang.String toString()"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory", "org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory()"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController", "org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory.create(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController create(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration)"], ["java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController>", "org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory.getThroughputControllerClass(org.apache.hadoop.conf.Configuration)", "public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController> getThroughputControllerClass(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress", "org.apache.hadoop.hbase.regionserver.compactions.Compactor.getProgress()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress getProgress()"], ["int", "org.apache.hadoop.hbase.regionserver.compactions.CurrentHourProvider.getCurrentHour()", "public static int getCurrentHour()"], ["org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor", "org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)", "public org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compactForTesting(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean)", "public java.util.List<org.apache.hadoop.fs.Path> compactForTesting(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy", "org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)", "public org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy.applyCompactionPolicy(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, int, int)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> applyCompactionPolicy(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, int, int)"], ["org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController()", "public org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.setup(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.start(java.lang.String)", "public void start(java.lang.String)"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.control(java.lang.String, long)", "public long control(java.lang.String, long) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.finish(java.lang.String)", "public void finish(java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.isStopped()", "public boolean isStopped()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$1.isOffPeakHour()", "public boolean isOffPeakHour()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$1.isOffPeakHour(int)", "public boolean isOffPeakHour(int)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$OffPeakHoursImpl.isOffPeakHour()", "public boolean isOffPeakHour()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$OffPeakHoursImpl.isOffPeakHour(int)", "public boolean isOffPeakHour(int)"], ["org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours()", "public org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours()"], ["org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours.getInstance(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours getInstance(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours", "org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours.getInstance(int, int)", "public static org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours getInstance(int, int)"], ["org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController()", "public org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.setup(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.start(java.lang.String)", "public void start(java.lang.String)"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.control(java.lang.String, long)", "public long control(java.lang.String, long) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.finish(java.lang.String)", "public void finish(java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.isStopped()", "public boolean isStopped()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy$1.apply(org.apache.hadoop.hbase.regionserver.StoreFile)", "public boolean apply(org.apache.hadoop.hbase.regionserver.StoreFile)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy$1.apply(java.lang.Object)", "public boolean apply(java.lang.Object)"], ["org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)", "public org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.preSelectCompactionForCoprocessor(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> preSelectCompactionForCoprocessor(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.selectCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean)", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest selectCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean, boolean, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.isMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean isMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.getNextMajorCompactTime(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public long getNextMajorCompactTime(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.throttleCompaction(long)", "public boolean throttleCompaction(long)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, java.util.List<byte[]>)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, java.util.List<byte[]>)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest.execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, byte[], byte[], int, long)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, byte[], byte[], int, long)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, byte[], byte[], long)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, byte[], byte[], long)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, byte[], byte[], long)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, byte[], byte[], long)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, byte[], byte[], int, long)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>, byte[], byte[], int, long)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest.execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest.setMajorRangeFull()", "public void setMajorRangeFull()"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest.setMajorRange(byte[], byte[])", "public void setMajorRange(byte[], byte[])"], ["org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest.getRequest()", "public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest getRequest()"], ["void", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest.setRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public void setRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation, org.apache.hadoop.hbase.regionserver.StripeStoreConfig)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation, org.apache.hadoop.hbase.regionserver.StripeStoreConfig)"], ["java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.preSelectFilesForCoprocessor(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> preSelectFilesForCoprocessor(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.createEmptyRequest(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest createEmptyRequest(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)"], ["org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.selectFlush(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, int)", "public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest selectFlush(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, int)"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.selectCompaction(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest selectCompaction(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.needsCompactions(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean needsCompactions(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.isMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public boolean isMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.throttleCompaction(long)", "public boolean throttleCompaction(long)"], ["long", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.getTotalFileSize(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)", "public static long getTotalFileSize(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFile>)"], ["org.apache.hadoop.hbase.regionserver.StoreFile$Writer", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor$1.createWriter()", "public org.apache.hadoop.hbase.regionserver.StoreFile$Writer createWriter() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)", "public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Store)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, java.util.List<byte[]>, byte[], byte[], org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, java.util.List<byte[]>, byte[], byte[], org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, int, long, byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController)", "public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, int, long, byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler", "org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)", "public org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)"], ["org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler", "org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)", "public org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails)"], ["org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler", "org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails, org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails, org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler", "org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails, org.apache.hadoop.hbase.executor.EventType)", "public org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, boolean, org.apache.hadoop.hbase.coordination.CloseRegionCoordination, org.apache.hadoop.hbase.coordination.CloseRegionCoordination$CloseRegionDetails, org.apache.hadoop.hbase.executor.EventType)"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["void", "org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process()", "public void process()"], ["org.apache.hadoop.hbase.regionserver.handler.FinishRegionRecoveringHandler", "org.apache.hadoop.hbase.regionserver.handler.FinishRegionRecoveringHandler(org.apache.hadoop.hbase.regionserver.RegionServerServices, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.regionserver.handler.FinishRegionRecoveringHandler(org.apache.hadoop.hbase.regionserver.RegionServerServices, java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.regionserver.handler.FinishRegionRecoveringHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler", "org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, long, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, long, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["boolean", "org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$1.progress()", "public boolean progress()"], ["void", "org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run()", "public void run()"], ["org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler", "org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, long, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)", "public org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor, long, org.apache.hadoop.hbase.coordination.OpenRegionCoordination, org.apache.hadoop.hbase.coordination.OpenRegionCoordination$OpenRegionDetails)"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.getRegionInfo()", "public org.apache.hadoop.hbase.HRegionInfo getRegionInfo()"], ["void", "org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler", "org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, long, java.util.concurrent.CountDownLatch)", "public org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, long, java.util.concurrent.CountDownLatch)"], ["void", "org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler.process()", "public void process()"], ["java.lang.Throwable", "org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler.getErr()", "public java.lang.Throwable getErr()"], ["void", "org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler.setErr(java.lang.Throwable)", "public void setErr(java.lang.Throwable)"], ["org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler", "org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.client.RpcRetryingCallerFactory, org.apache.hadoop.hbase.ipc.RpcControllerFactory, int, org.apache.hadoop.hbase.regionserver.HRegion)", "public org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.client.RpcRetryingCallerFactory, org.apache.hadoop.hbase.ipc.RpcControllerFactory, int, org.apache.hadoop.hbase.regionserver.HRegion)"], ["void", "org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler.process()", "public void process() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler", "org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails, org.apache.hadoop.hbase.util.CancelableProgressable, java.util.concurrent.atomic.AtomicInteger, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)", "public org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails, org.apache.hadoop.hbase.util.CancelableProgressable, java.util.concurrent.atomic.AtomicInteger, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode)"], ["void", "org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler.process()", "public void process() throws java.io.IOException"], ["java.lang.Void", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure$RegionSnapshotTask.call()", "public java.lang.Void call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure$RegionSnapshotTask.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedurePool)", "public org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedurePool)"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.acquireBarrier()", "public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["byte[]", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.insideBarrier()", "public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.cleanup(java.lang.Exception)", "public void cleanup(java.lang.Exception)"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.releaseBarrier()", "public void releaseBarrier()"], ["org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder(org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager)", "public org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder(org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager)"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder.buildSubprocedure(java.lang.String, byte[])", "public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, byte[])"], ["org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager()", "public org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager()"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.stop(boolean)", "public void stop(boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.procedure.Subprocedure", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.buildSubprocedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)"], ["void", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices)", "public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.getProcedureSignature()", "public java.lang.String getProcedureSignature()"], ["org.apache.hadoop.hbase.regionserver.wal.CompressionContext", "org.apache.hadoop.hbase.regionserver.wal.CompressionContext(java.lang.Class<? extends org.apache.hadoop.hbase.io.util.Dictionary>, boolean, boolean)", "public org.apache.hadoop.hbase.regionserver.wal.CompressionContext(java.lang.Class<? extends org.apache.hadoop.hbase.io.util.Dictionary>, boolean, boolean) throws java.lang.SecurityException, java.lang.NoSuchMethodException, java.lang.InstantiationException, java.lang.IllegalAccessException, java.lang.reflect.InvocationTargetException"], ["org.apache.hadoop.hbase.regionserver.wal.Compressor", "org.apache.hadoop.hbase.regionserver.wal.Compressor()", "public org.apache.hadoop.hbase.regionserver.wal.Compressor()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.Compressor.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$1.compare(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public int compare(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["int", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$2.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck, long, boolean)", "public void onEvent(org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck, long, boolean) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onStart()", "public void onStart()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onShutdown()", "public void onShutdown()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(java.lang.Object, long, boolean)", "public void onEvent(java.lang.Object, long, boolean) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferExceptionHandler.handleEventException(java.lang.Throwable, long, java.lang.Object)", "public void handleEventException(java.lang.Throwable, long, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferExceptionHandler.handleOnStartException(java.lang.Throwable)", "public void handleOnStartException(java.lang.Throwable)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferExceptionHandler.handleOnShutdownException(java.lang.Throwable)", "public void handleOnShutdownException(java.lang.Throwable)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)", "public void registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)", "public boolean unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getCoprocessorHost()", "public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost getCoprocessorHost()"], ["org.apache.hadoop.hbase.regionserver.wal.FSHLog", "org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.FSHLog", "org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String)", "public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String) throws java.io.IOException"], ["byte[][]", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter()", "public byte[][] rollWriter() throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException"], ["byte[][]", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(boolean)", "public byte[][] rollWriter(boolean) throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getWALArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getWALArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getCurrentFileName()", "public org.apache.hadoop.fs.Path getCurrentFileName()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.shutdown()", "public void shutdown() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, java.util.concurrent.atomic.AtomicLong, boolean, java.util.List<org.apache.hadoop.hbase.Cell>)", "public long append(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, java.util.concurrent.atomic.AtomicLong, boolean, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync()", "public void sync() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(long)", "public void sync(long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.requestLogRoll()", "public void requestLogRoll()"], ["int", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getNumRolledLogFiles()", "public int getNumRolledLogFiles()"], ["int", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getNumLogFiles()", "public int getNumLogFiles()"], ["long", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getLogFileSize()", "public long getLogFileSize()"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.startCacheFlush(byte[], java.util.Set<byte[]>)", "public java.lang.Long startCacheFlush(byte[], java.util.Set<byte[]>)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.completeCacheFlush(byte[])", "public void completeCacheFlush(byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.abortCacheFlush(byte[])", "public void abortCacheFlush(byte[])"], ["long", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getEarliestMemstoreSeqNum(byte[])", "public long getEarliestMemstoreSeqNum(byte[])"], ["long", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.getEarliestMemstoreSeqNum(byte[], byte[])", "public long getEarliestMemstoreSeqNum(byte[], byte[])"], ["void", "org.apache.hadoop.hbase.regionserver.wal.FSHLog.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.wal.FSWALEntry.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey()", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey()"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID)"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long)"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)"], ["org.apache.hadoop.hbase.regionserver.wal.HLogKey", "org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, long)", "public org.apache.hadoop.hbase.regionserver.wal.HLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, long)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.HLogKey.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.HLogKey.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter", "org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter()", "public org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter()"], ["org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter", "org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter(boolean, boolean, long, java.lang.String, java.lang.String, boolean, java.io.PrintStream)", "public org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter(boolean, boolean, long, java.lang.String, java.lang.String, boolean, java.io.PrintStream)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.KeyValue", "org.apache.hadoop.hbase.regionserver.wal.KeyValueCompression.readKV(java.io.DataInput, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public static org.apache.hadoop.hbase.KeyValue readKV(java.io.DataInput, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.KeyValueCompression.writeKV(java.io.DataOutput, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public static void writeKV(java.io.DataOutput, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.MetricsWAL", "org.apache.hadoop.hbase.regionserver.wal.MetricsWAL()", "public org.apache.hadoop.hbase.regionserver.wal.MetricsWAL()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.MetricsWAL.postSync(long, int)", "public void postSync(long, int)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.MetricsWAL.postAppend(long, long)", "public void postAppend(long, long)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.MetricsWAL.logRollRequested(boolean)", "public void logRollRequested(boolean)"], ["org.apache.hadoop.hbase.regionserver.wal.MetricsWALEditsReplay", "org.apache.hadoop.hbase.regionserver.wal.MetricsWALEditsReplay()", "public org.apache.hadoop.hbase.regionserver.wal.MetricsWALEditsReplay()"], ["org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult[]", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult.values()", "public static org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult[] values()"], ["org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader()", "public org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.close()", "public void close() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.getPosition()", "public long getPosition() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.reset()", "public void reset() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream)", "public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.getWriterClsNames()", "public java.util.List<java.lang.String> getWriterClsNames()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.getCodecClsName()", "public java.lang.String getCodecClsName()"], ["org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter()", "public org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean)", "public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(org.apache.hadoop.hbase.wal.WAL$Entry)", "public void append(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync()", "public void sync() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.getLength()", "public long getLength() throws java.io.IOException"], ["org.apache.hadoop.fs.FSDataOutputStream", "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.getStream()", "public org.apache.hadoop.fs.FSDataOutputStream getStream()"], ["org.apache.hadoop.hbase.regionserver.wal.ReaderBase", "org.apache.hadoop.hbase.regionserver.wal.ReaderBase()", "public org.apache.hadoop.hbase.regionserver.wal.ReaderBase()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ReaderBase.init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream)", "public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.regionserver.wal.ReaderBase.next()", "public org.apache.hadoop.hbase.wal.WAL$Entry next() throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.regionserver.wal.ReaderBase.next(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry next(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.ReaderBase.seek(long)", "public void seek(long) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey", "org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)"], ["org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey", "org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)"], ["long", "org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey.getSequenceId()", "public long getSequenceId() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck", "org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$1.newInstance()", "public org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck newInstance()"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$1.newInstance()", "public java.lang.Object newInstance()"], ["org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader", "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader()", "public org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader.getWriterClsNames()", "public java.util.List<java.lang.String> getWriterClsNames()"], ["org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter", "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter()", "public org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter()"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream)"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.io.crypto.Decryptor)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.io.crypto.Decryptor)"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream)"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.io.crypto.Encryptor)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.io.crypto.Encryptor)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder.write(org.apache.hadoop.hbase.Cell)", "public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor)"], ["org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor)", "public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor)"], ["org.apache.hadoop.hbase.codec.Codec$Decoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec.getDecoder(java.io.InputStream)", "public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream)"], ["org.apache.hadoop.hbase.codec.Codec$Encoder", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec.getEncoder(java.io.OutputStream)", "public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec.getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec.getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor)"], ["long", "org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader$WALReaderFSDataInputStream.getPos()", "public long getPos() throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader", "org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader()", "public org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.close()", "public void close() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.getPosition()", "public long getPosition() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.reset()", "public void reset() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.regionserver.wal.SyncFuture.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.SyncFuture.cancel(boolean)", "public boolean cancel(boolean)"], ["synchronized", "org.apache.hadoop.hbase.regionserver.wal.SyncFuture.long get()", "public synchronized long get() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException"], ["java.lang.Long", "org.apache.hadoop.hbase.regionserver.wal.SyncFuture.get(long, java.util.concurrent.TimeUnit)", "public java.lang.Long get(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException, java.util.concurrent.ExecutionException"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.SyncFuture.isCancelled()", "public boolean isCancelled()"], ["org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base()", "public org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.preLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void preLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.postLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void postLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.logRollRequested(boolean)", "public void logRollRequested(boolean)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.logCloseRequested()", "public void logCloseRequested()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.postAppend(long, long)", "public void postAppend(long, long)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$Base.postSync(long, int)", "public void postSync(long, int)"], ["byte[]", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$1.uncompress(com.google.protobuf.ByteString, org.apache.hadoop.hbase.io.util.Dictionary)", "public byte[] uncompress(com.google.protobuf.ByteString, org.apache.hadoop.hbase.io.util.Dictionary) throws java.io.IOException"], ["com.google.protobuf.ByteString", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$BaosAndCompressor.toByteString()", "public com.google.protobuf.ByteString toByteString()"], ["com.google.protobuf.ByteString", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$BaosAndCompressor.compress(byte[], org.apache.hadoop.hbase.io.util.Dictionary)", "public com.google.protobuf.ByteString compress(byte[], org.apache.hadoop.hbase.io.util.Dictionary) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvDecoder", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder.write(org.apache.hadoop.hbase.Cell)", "public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder(java.io.OutputStream)", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder(java.io.OutputStream)"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder.write(org.apache.hadoop.hbase.Cell)", "public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec()", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec()"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.create(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec create(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.lang.UnsupportedOperationException"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.lang.UnsupportedOperationException"], ["org.apache.hadoop.hbase.codec.Codec$Decoder", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.getDecoder(java.io.InputStream)", "public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream)"], ["org.apache.hadoop.hbase.codec.Codec$Encoder", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.getEncoder(java.io.OutputStream)", "public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.getByteStringCompressor()", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor getByteStringCompressor()"], ["org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor", "org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.getByteStringUncompressor()", "public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor getByteStringUncompressor()"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment.getWAL()", "public org.apache.hadoop.hbase.wal.WAL getWAL()"], ["org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.wal.WAL)", "public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.wal.WAL)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment createEnvironment(java.lang.Class<?>, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.preWALWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public boolean preWALWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.postWALWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void postWALWrite(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit) throws java.io.IOException"], ["org.apache.hadoop.hbase.CoprocessorEnvironment", "org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(java.lang.Class, org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit()", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit()"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit(boolean)", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit(boolean)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.isMetaEditFamily(byte[])", "public static boolean isMetaEditFamily(byte[])"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.isMetaEditFamily(org.apache.hadoop.hbase.Cell)", "public static boolean isMetaEditFamily(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.isMetaEdit()", "public boolean isMetaEdit()"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.isReplay()", "public boolean isReplay()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public void setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.add(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit add(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.isEmpty()", "public boolean isEmpty()"], ["int", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.size()", "public int size()"], ["java.util.ArrayList<org.apache.hadoop.hbase.Cell>", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getCells()", "public java.util.ArrayList<org.apache.hadoop.hbase.Cell> getCells()"], ["java.util.NavigableMap<byte[], java.lang.Integer>", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getAndRemoveScopes()", "public java.util.NavigableMap<byte[], java.lang.Integer> getAndRemoveScopes()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.readFromCells(org.apache.hadoop.hbase.codec.Codec$Decoder, int)", "public int readFromCells(org.apache.hadoop.hbase.codec.Codec$Decoder, int) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.heapSize()", "public long heapSize()"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.createFlushWALEdit(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALEdit createFlushWALEdit(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor)"], ["org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getFlushDescriptor(org.apache.hadoop.hbase.Cell)", "public static org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor getFlushDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.createRegionEventWALEdit(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALEdit createRegionEventWALEdit(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor)"], ["org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getRegionEventDescriptor(org.apache.hadoop.hbase.Cell)", "public static org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor getRegionEventDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.createCompaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALEdit createCompaction(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor)"], ["org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getCompaction(org.apache.hadoop.hbase.Cell)", "public static org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor getCompaction(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.createBulkLoadEvent(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor)", "public static org.apache.hadoop.hbase.regionserver.wal.WALEdit createBulkLoadEvent(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor)"], ["org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor", "org.apache.hadoop.hbase.regionserver.wal.WALEdit.getBulkLoadDescriptor(org.apache.hadoop.hbase.Cell)", "public static org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor getBulkLoadDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink$ReplayServerCallable.call(int)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse call(int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink$ReplayServerCallable.prepare(boolean)", "public void prepare(boolean) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink$ReplayServerCallable.call(int)", "public java.lang.Object call(int) throws java.lang.Exception"], ["org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.HConnection)", "public org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.replayEntries(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionLocation, org.apache.hadoop.hbase.wal.WAL$Entry>>)", "public void replayEntries(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionLocation, org.apache.hadoop.hbase.wal.WAL$Entry>>) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.getStats()", "public java.lang.String getStats()"], ["org.apache.hadoop.hbase.regionserver.wal.WALUtil", "org.apache.hadoop.hbase.regionserver.wal.WALUtil()", "public org.apache.hadoop.hbase.regionserver.wal.WALUtil()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WALUtil.writeCompactionMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor, java.util.concurrent.atomic.AtomicLong)", "public static void writeCompactionMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$CompactionDescriptor, java.util.concurrent.atomic.AtomicLong) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.WALUtil.writeFlushMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor, java.util.concurrent.atomic.AtomicLong, boolean)", "public static long writeFlushMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$FlushDescriptor, java.util.concurrent.atomic.AtomicLong, boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.WALUtil.writeRegionEventMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor, java.util.concurrent.atomic.AtomicLong)", "public static long writeRegionEventMarker(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$RegionEventDescriptor, java.util.concurrent.atomic.AtomicLong) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.regionserver.wal.WALUtil.writeBulkLoadMarkerAndSync(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor, java.util.concurrent.atomic.AtomicLong)", "public static long writeBulkLoadMarkerAndSync(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.protobuf.generated.WALProtos$BulkLoadDescriptor, java.util.concurrent.atomic.AtomicLong) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.wal.WriterBase", "org.apache.hadoop.hbase.regionserver.wal.WriterBase()", "public org.apache.hadoop.hbase.regionserver.wal.WriterBase()"], ["void", "org.apache.hadoop.hbase.regionserver.wal.WriterBase.init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean)", "public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.regionserver.wal.WriterBase.initializeCompressionContext(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public boolean initializeCompressionContext(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.BaseReplicationEndpoint", "org.apache.hadoop.hbase.replication.BaseReplicationEndpoint()", "public org.apache.hadoop.hbase.replication.BaseReplicationEndpoint()"], ["void", "org.apache.hadoop.hbase.replication.BaseReplicationEndpoint.init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context)", "public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.WALEntryFilter", "org.apache.hadoop.hbase.replication.BaseReplicationEndpoint.getWALEntryfilter()", "public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter()"], ["boolean", "org.apache.hadoop.hbase.replication.BaseReplicationEndpoint.canReplicateToSameCluster()", "public boolean canReplicateToSameCluster()"], ["org.apache.hadoop.hbase.replication.BaseWALEntryFilter", "org.apache.hadoop.hbase.replication.BaseWALEntryFilter()", "public org.apache.hadoop.hbase.replication.BaseWALEntryFilter()"], ["org.apache.hadoop.hbase.replication.ChainWALEntryFilter", "org.apache.hadoop.hbase.replication.ChainWALEntryFilter(org.apache.hadoop.hbase.replication.WALEntryFilter...)", "public org.apache.hadoop.hbase.replication.ChainWALEntryFilter(org.apache.hadoop.hbase.replication.WALEntryFilter...)"], ["org.apache.hadoop.hbase.replication.ChainWALEntryFilter", "org.apache.hadoop.hbase.replication.ChainWALEntryFilter(java.util.List<org.apache.hadoop.hbase.replication.WALEntryFilter>)", "public org.apache.hadoop.hbase.replication.ChainWALEntryFilter(java.util.List<org.apache.hadoop.hbase.replication.WALEntryFilter>)"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.ChainWALEntryFilter.filter(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry)"], ["org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener(org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint)", "public org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener(org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint)"], ["synchronized", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener.void nodeChildrenChanged(java.lang.String)", "public synchronized void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint()", "public org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint()"], ["synchronized", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.java.util.UUID getPeerUUID()", "public synchronized java.util.UUID getPeerUUID()"], ["void", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.isAborted()", "public boolean isAborted()"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.getRegionServers()", "public synchronized java.util.List<org.apache.hadoop.hbase.ServerName> getRegionServers()"], ["void", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.setRegionServers(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public void setRegionServers(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["long", "org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.getLastRegionServerUpdate()", "public long getLastRegionServerUpdate()"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.replication.regionserver.MetricsSource, org.apache.hadoop.hbase.TableDescriptors)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.replication.regionserver.MetricsSource, org.apache.hadoop.hbase.TableDescriptors)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getFilesystem()", "public org.apache.hadoop.fs.FileSystem getFilesystem()"], ["java.util.UUID", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getClusterId()", "public java.util.UUID getClusterId()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getPeerId()", "public java.lang.String getPeerId()"], ["org.apache.hadoop.hbase.replication.ReplicationPeerConfig", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getPeerConfig()", "public org.apache.hadoop.hbase.replication.ReplicationPeerConfig getPeerConfig()"], ["org.apache.hadoop.hbase.replication.ReplicationPeer", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getReplicationPeer()", "public org.apache.hadoop.hbase.replication.ReplicationPeer getReplicationPeer()"], ["org.apache.hadoop.hbase.replication.regionserver.MetricsSource", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getMetrics()", "public org.apache.hadoop.hbase.replication.regionserver.MetricsSource getMetrics()"], ["org.apache.hadoop.hbase.TableDescriptors", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context.getTableDescriptors()", "public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors()"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext()", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext()"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext.setEntries(java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext setEntries(java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>)"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext.setSize(int)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext setSize(int)"], ["java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext.getEntries()", "public java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry> getEntries()"], ["int", "org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext.getSize()", "public int getSize()"], ["org.apache.hadoop.hbase.replication.ScopeWALEntryFilter", "org.apache.hadoop.hbase.replication.ScopeWALEntryFilter()", "public org.apache.hadoop.hbase.replication.ScopeWALEntryFilter()"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.ScopeWALEntryFilter.filter(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry)"], ["org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter", "org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter()", "public org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter()"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter.filter(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry)"], ["org.apache.hadoop.hbase.replication.TableCfWALEntryFilter", "org.apache.hadoop.hbase.replication.TableCfWALEntryFilter(org.apache.hadoop.hbase.replication.ReplicationPeer)", "public org.apache.hadoop.hbase.replication.TableCfWALEntryFilter(org.apache.hadoop.hbase.replication.ReplicationPeer)"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.TableCfWALEntryFilter.filter(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry)"], ["boolean", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner$1.apply(org.apache.hadoop.fs.FileStatus)", "public boolean apply(org.apache.hadoop.fs.FileStatus)"], ["boolean", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner$1.apply(java.lang.Object)", "public boolean apply(java.lang.Object)"], ["org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner()", "public org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner()"], ["java.lang.Iterable<org.apache.hadoop.fs.FileStatus>", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)", "public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>)"], ["void", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.isAborted()", "public boolean isAborted()"], ["org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint", "org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint()", "public org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context)", "public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)", "public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)"], ["org.apache.hadoop.hbase.replication.regionserver.MetricsSink", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink()", "public org.apache.hadoop.hbase.replication.regionserver.MetricsSink()"], ["long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink.setAgeOfLastAppliedOp(long)", "public long setAgeOfLastAppliedOp(long)"], ["long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink.refreshAgeOfLastAppliedOp()", "public long refreshAgeOfLastAppliedOp()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink.applyBatch(long)", "public void applyBatch(long)"], ["long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink.getAgeOfLastAppliedOp()", "public long getAgeOfLastAppliedOp()"], ["long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSink.getTimeStampOfLastAppliedOp()", "public long getTimeStampOfLastAppliedOp()"], ["org.apache.hadoop.hbase.replication.regionserver.MetricsSource", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource(java.lang.String)", "public org.apache.hadoop.hbase.replication.regionserver.MetricsSource(java.lang.String)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.setAgeOfLastShippedOp(long)", "public void setAgeOfLastShippedOp(long)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.refreshAgeOfLastShippedOp()", "public void refreshAgeOfLastShippedOp()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.setSizeOfLogQueue(int)", "public void setSizeOfLogQueue(int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.incrLogEditsRead()", "public void incrLogEditsRead()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.incrLogEditsFiltered(long)", "public void incrLogEditsFiltered(long)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.incrLogEditsFiltered()", "public void incrLogEditsFiltered()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.shipBatch(long, int)", "public void shipBatch(long, int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.incrLogReadInBytes(long)", "public void incrLogReadInBytes(long)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.clear()", "public void clear()"], ["java.lang.Long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.getAgeOfLastShippedOp()", "public java.lang.Long getAgeOfLastShippedOp()"], ["int", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.getSizeOfLogQueue()", "public int getSizeOfLogQueue()"], ["long", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.getTimeStampOfLastShippedOp()", "public long getTimeStampOfLastShippedOp()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.MetricsSource.getPeerID()", "public java.lang.String getPeerID()"], ["org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.TableDescriptors, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, org.apache.hadoop.hbase.client.ClusterConnection, java.util.concurrent.ExecutorService, int, int)", "public org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.TableDescriptors, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, org.apache.hadoop.hbase.client.ClusterConnection, java.util.concurrent.ExecutorService, int, int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink.append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer)", "public void append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink.flush()", "public boolean flush() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink.finishWritingAndClose()", "public java.util.List<org.apache.hadoop.fs.Path> finishWritingAndClose() throws java.io.IOException"], ["java.util.Map<byte[], java.lang.Long>", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink.getOutputCounts()", "public java.util.Map<byte[], java.lang.Long> getOutputCounts()"], ["int", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink.getNumberOfRecoveredRegions()", "public int getNumberOfRecoveredRegions()"], ["org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaReplayCallable", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaReplayCallable(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.ipc.RpcControllerFactory, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HRegionLocation, org.apache.hadoop.hbase.HRegionInfo, byte[], java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, java.util.concurrent.atomic.AtomicLong)", "public org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaReplayCallable(org.apache.hadoop.hbase.client.ClusterConnection, org.apache.hadoop.hbase.ipc.RpcControllerFactory, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HRegionLocation, org.apache.hadoop.hbase.HRegionInfo, byte[], java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, java.util.concurrent.atomic.AtomicLong)"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaReplayCallable.call(int)", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$ReplicateWALEntryResponse call(int) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaReplayCallable.call(int)", "public java.lang.Object call(int) throws java.lang.Exception"], ["org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaSinkWriter", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaSinkWriter(org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink, org.apache.hadoop.hbase.client.ClusterConnection, java.util.concurrent.ExecutorService, int)", "public org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaSinkWriter(org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaOutputSink, org.apache.hadoop.hbase.client.ClusterConnection, java.util.concurrent.ExecutorService, int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RegionReplicaSinkWriter.append(org.apache.hadoop.hbase.TableName, byte[], byte[], java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>)", "public void append(org.apache.hadoop.hbase.TableName, byte[], byte[], java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RetryingRpcCallable", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RetryingRpcCallable(org.apache.hadoop.hbase.client.RpcRetryingCallerFactory, org.apache.hadoop.hbase.client.RetryingCallable<V>, int)", "public org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RetryingRpcCallable(org.apache.hadoop.hbase.client.RpcRetryingCallerFactory, org.apache.hadoop.hbase.client.RetryingCallable<V>, int)"], ["V", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$RetryingRpcCallable.call()", "public V call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint$SkipReplayedEditsFilter.filter(org.apache.hadoop.hbase.wal.WAL$Entry)", "public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry)"], ["org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint()", "public org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint()"], ["org.apache.hadoop.hbase.replication.WALEntryFilter", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.getWALEntryfilter()", "public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context)", "public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)", "public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.canReplicateToSameCluster()", "public boolean canReplicateToSameCluster()"], ["org.apache.hadoop.hbase.replication.regionserver.Replication$ReplicationStatisticsThread", "org.apache.hadoop.hbase.replication.regionserver.Replication$ReplicationStatisticsThread(org.apache.hadoop.hbase.replication.regionserver.ReplicationSink, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager)", "public org.apache.hadoop.hbase.replication.regionserver.Replication$ReplicationStatisticsThread(org.apache.hadoop.hbase.replication.regionserver.ReplicationSink, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication$ReplicationStatisticsThread.run()", "public void run()"], ["org.apache.hadoop.hbase.replication.regionserver.Replication", "org.apache.hadoop.hbase.replication.regionserver.Replication(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.replication.regionserver.Replication(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.regionserver.Replication", "org.apache.hadoop.hbase.replication.regionserver.Replication()", "public org.apache.hadoop.hbase.replication.regionserver.Replication()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.initialize(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void initialize(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.Replication.isReplication(org.apache.hadoop.conf.Configuration)", "public static boolean isReplication(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.regionserver.wal.WALActionsListener", "org.apache.hadoop.hbase.replication.regionserver.Replication.getWALActionsListener()", "public org.apache.hadoop.hbase.regionserver.wal.WALActionsListener getWALActionsListener()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.stopReplicationService()", "public void stopReplicationService()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.join()", "public void join()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.replicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void replicateLogEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.startReplicationService()", "public void startReplicationService() throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager", "org.apache.hadoop.hbase.replication.regionserver.Replication.getReplicationManager()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager getReplicationManager()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public void visitLogEntryBeforeWrite(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.scopeWALEdits(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public static void scopeWALEdits(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public void postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.Replication.decorateMasterConfiguration(org.apache.hadoop.conf.Configuration)", "public static void decorateMasterConfiguration(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad", "org.apache.hadoop.hbase.replication.regionserver.Replication.refreshAndGetReplicationLoad()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad refreshAndGetReplicationLoad()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.buildReplicationLoad(java.util.List<org.apache.hadoop.hbase.replication.regionserver.MetricsSource>, org.apache.hadoop.hbase.replication.regionserver.MetricsSink)", "public void buildReplicationLoad(java.util.List<org.apache.hadoop.hbase.replication.regionserver.MetricsSource>, org.apache.hadoop.hbase.replication.regionserver.MetricsSink)"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.sourceToString()", "public java.lang.String sourceToString()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.sinkToString()", "public java.lang.String sinkToString()"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ReplicationLoadSink", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.getReplicationLoadSink()", "public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ReplicationLoadSink getReplicationLoadSink()"], ["java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ReplicationLoadSource>", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.getReplicationLoadSourceList()", "public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ReplicationLoadSource> getReplicationLoadSourceList()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSink", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Stoppable)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Stoppable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.replicateEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void replicateEntries(java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.stopReplicationSinkServices()", "public void stopReplicationSinkServices()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.getStats()", "public java.lang.String getStats()"], ["org.apache.hadoop.hbase.replication.regionserver.MetricsSink", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.getSinkMetrics()", "public org.apache.hadoop.hbase.replication.regionserver.MetricsSink getSinkMetrics()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface)"], ["org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer.getRegionServer()", "public org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingInterface getRegionServer()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager(org.apache.hadoop.hbase.client.HConnection, java.lang.String, org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager(org.apache.hadoop.hbase.client.HConnection, java.lang.String, org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.getReplicationSink()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer getReplicationSink() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.reportBadSink(org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer)", "public void reportBadSink(org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.reportSinkSuccess(org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer)", "public void reportSinkSuccess(org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager$SinkPeer)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$1.uncaughtException(java.lang.Thread, java.lang.Throwable)", "public void uncaughtException(java.lang.Thread, java.lang.Throwable)"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$LogsComparator", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$LogsComparator()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$LogsComparator()"], ["int", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$LogsComparator.compare(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public int compare(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["int", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$LogsComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSource", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSource()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, org.apache.hadoop.hbase.Stoppable, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationEndpoint, org.apache.hadoop.hbase.replication.regionserver.MetricsSource)", "public void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, org.apache.hadoop.hbase.Stoppable, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationEndpoint, org.apache.hadoop.hbase.replication.regionserver.MetricsSource) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.enqueueLog(org.apache.hadoop.fs.Path)", "public void enqueueLog(org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run()", "public void run()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.startup()", "public void startup()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.terminate(java.lang.String)", "public void terminate(java.lang.String)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.terminate(java.lang.String, java.lang.Exception)", "public void terminate(java.lang.String, java.lang.Exception)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.terminate(java.lang.String, java.lang.Exception, boolean)", "public void terminate(java.lang.String, java.lang.Exception, boolean)"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.getPeerClusterZnode()", "public java.lang.String getPeerClusterZnode()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.getPeerClusterId()", "public java.lang.String getPeerClusterId()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.getCurrentPath()", "public org.apache.hadoop.fs.Path getCurrentPath()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.getStats()", "public java.lang.String getStats()"], ["org.apache.hadoop.hbase.replication.regionserver.MetricsSource", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.getSourceMetrics()", "public org.apache.hadoop.hbase.replication.regionserver.MetricsSource getSourceMetrics()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$NodeFailoverWorker", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$NodeFailoverWorker(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, java.util.UUID)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$NodeFailoverWorker(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, java.util.UUID)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$NodeFailoverWorker.run()", "public void run()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager(org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, org.apache.hadoop.hbase.replication.ReplicationTracker, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.UUID)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager(org.apache.hadoop.hbase.replication.ReplicationQueues, org.apache.hadoop.hbase.replication.ReplicationPeers, org.apache.hadoop.hbase.replication.ReplicationTracker, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.UUID)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(org.apache.hadoop.fs.Path, java.lang.String, long, boolean, boolean)", "public void logPositionAndCleanOldLogs(org.apache.hadoop.fs.Path, java.lang.String, long, boolean, boolean)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.cleanOldLogs(java.lang.String, java.lang.String, boolean)", "public void cleanOldLogs(java.lang.String, java.lang.String, boolean)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.deleteSource(java.lang.String, boolean)", "public void deleteSource(java.lang.String, boolean)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.join()", "public void join()"], ["java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface>", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getSources()", "public java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface> getSources()"], ["java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface>", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getOldSources()", "public java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface> getOldSources()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.closeRecoveredQueue(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface)", "public void closeRecoveredQueue(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.removePeer(java.lang.String)", "public void removePeer(java.lang.String)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.regionServerRemoved(java.lang.String)", "public void regionServerRemoved(java.lang.String)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.peerRemoved(java.lang.String)", "public void peerRemoved(java.lang.String)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.peerListChanged(java.util.List<java.lang.String>)", "public void peerListChanged(java.util.List<java.lang.String>)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getOldLogDir()", "public org.apache.hadoop.fs.Path getOldLogDir()"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getLogDir()", "public org.apache.hadoop.fs.Path getLogDir()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getFs()", "public org.apache.hadoop.fs.FileSystem getFs()"], ["java.lang.String", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.getStats()", "public java.lang.String getStats()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$1.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$1.isAborted()", "public boolean isAborted()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getConfiguration()", "public org.apache.hadoop.conf.Configuration getConfiguration()"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getZooKeeper()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher getZooKeeper()"], ["org.apache.hadoop.hbase.CoordinatedStateManager", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getCoordinatedStateManager()", "public org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager()"], ["org.apache.hadoop.hbase.zookeeper.MetaTableLocator", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getMetaTableLocator()", "public org.apache.hadoop.hbase.zookeeper.MetaTableLocator getMetaTableLocator()"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getServerName()", "public org.apache.hadoop.hbase.ServerName getServerName()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.isAborted()", "public boolean isAborted()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.client.ClusterConnection", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getConnection()", "public org.apache.hadoop.hbase.client.ClusterConnection getConnection()"], ["org.apache.hadoop.hbase.ChoreService", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer.getChoreService()", "public org.apache.hadoop.hbase.ChoreService getChoreService()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp()", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp.setConfigure(org.apache.hadoop.conf.Configuration)", "public static void setConfigure(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler", "org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler(double)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler(double)"], ["boolean", "org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler.isEnabled()", "public boolean isEnabled()"], ["long", "org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler.getNextSleepInterval(int)", "public long getNextSleepInterval(int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler.addPushSize(int)", "public void addPushSize(int)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler.resetStartTick()", "public void resetStartTick()"], ["org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.openReader(org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.wal.WAL$Reader openReader(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.readNextAndSetPosition()", "public org.apache.hadoop.hbase.wal.WAL$Entry readNextAndSetPosition() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.seek()", "public void seek() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.getPosition()", "public long getPosition()"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.setPosition(long)", "public void setPosition(long)"], ["void", "org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.closeReader()", "public void closeReader() throws java.io.IOException"], ["org.apache.hadoop.hbase.security.HBasePolicyProvider", "org.apache.hadoop.hbase.security.HBasePolicyProvider()", "public org.apache.hadoop.hbase.security.HBasePolicyProvider()"], ["org.apache.hadoop.security.authorize.Service[]", "org.apache.hadoop.hbase.security.HBasePolicyProvider.getServices()", "public org.apache.hadoop.security.authorize.Service[] getServices()"], ["void", "org.apache.hadoop.hbase.security.HBasePolicyProvider.init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.ServiceAuthorizationManager)", "public static void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.ServiceAuthorizationManager)"], ["org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslDigestCallbackHandler", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslDigestCallbackHandler(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, org.apache.hadoop.hbase.ipc.RpcServer$Connection)", "public org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslDigestCallbackHandler(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, org.apache.hadoop.hbase.ipc.RpcServer$Connection)"], ["void", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslDigestCallbackHandler.handle(javax.security.auth.callback.Callback[])", "public void handle(javax.security.auth.callback.Callback[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, javax.security.auth.callback.UnsupportedCallbackException"], ["org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslGssCallbackHandler", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslGssCallbackHandler()", "public org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslGssCallbackHandler()"], ["void", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer$SaslGssCallbackHandler.handle(javax.security.auth.callback.Callback[])", "public void handle(javax.security.auth.callback.Callback[]) throws javax.security.auth.callback.UnsupportedCallbackException"], ["org.apache.hadoop.hbase.security.HBaseSaslRpcServer", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer()", "public org.apache.hadoop.hbase.security.HBaseSaslRpcServer()"], ["void", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer.init(org.apache.hadoop.conf.Configuration)", "public static void init(org.apache.hadoop.conf.Configuration)"], ["<T extends org.apache.hadoop.security.token.TokenIdentifier> T", "org.apache.hadoop.hbase.security.HBaseSaslRpcServer.getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>)", "public static <T extends org.apache.hadoop.security.token.TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken"], ["org.apache.hadoop.hbase.security.SecurityUtil", "org.apache.hadoop.hbase.security.SecurityUtil()", "public org.apache.hadoop.hbase.security.SecurityUtil()"], ["java.lang.String", "org.apache.hadoop.hbase.security.SecurityUtil.getUserFromPrincipal(java.lang.String)", "public static java.lang.String getUserFromPrincipal(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.SecurityUtil.getPrincipalWithoutRealm(java.lang.String)", "public static java.lang.String getPrincipalWithoutRealm(java.lang.String)"], ["org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy[]", "org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy.values()", "public static org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy[] values()"], ["org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy", "org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.filter.Filter$ReturnCode", "org.apache.hadoop.hbase.security.access.AccessControlFilter.filterKeyValue(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterKeyValue(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.security.access.AccessControlFilter.transformCell(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell transformCell(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.security.access.AccessControlFilter.reset()", "public void reset() throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.security.access.AccessControlFilter.toByteArray()", "public byte[] toByteArray()"], ["org.apache.hadoop.hbase.security.access.AccessControlFilter", "org.apache.hadoop.hbase.security.access.AccessControlFilter.parseFrom(byte[])", "public static org.apache.hadoop.hbase.security.access.AccessControlFilter parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException"], ["org.apache.hadoop.hbase.security.access.AccessControlLists", "org.apache.hadoop.hbase.security.access.AccessControlLists()", "public org.apache.hadoop.hbase.security.access.AccessControlLists()"], ["byte[]", "org.apache.hadoop.hbase.security.access.AccessControlLists.writePermissionsAsBytes(com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.TablePermission>, org.apache.hadoop.conf.Configuration)", "public static byte[] writePermissionsAsBytes(com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.TablePermission>, org.apache.hadoop.conf.Configuration)"], ["com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.TablePermission>", "org.apache.hadoop.hbase.security.access.AccessControlLists.readPermissions(byte[], org.apache.hadoop.conf.Configuration)", "public static com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.TablePermission> readPermissions(byte[], org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.exceptions.DeserializationException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessControlLists.isNamespaceEntry(java.lang.String)", "public static boolean isNamespaceEntry(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessControlLists.isNamespaceEntry(byte[])", "public static boolean isNamespaceEntry(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AccessControlLists.toNamespaceEntry(java.lang.String)", "public static java.lang.String toNamespaceEntry(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AccessControlLists.fromNamespaceEntry(java.lang.String)", "public static java.lang.String fromNamespaceEntry(java.lang.String)"], ["byte[]", "org.apache.hadoop.hbase.security.access.AccessControlLists.toNamespaceEntry(byte[])", "public static byte[] toNamespaceEntry(byte[])"], ["byte[]", "org.apache.hadoop.hbase.security.access.AccessControlLists.fromNamespaceEntry(byte[])", "public static byte[] fromNamespaceEntry(byte[])"], ["java.util.List<org.apache.hadoop.hbase.security.access.Permission>", "org.apache.hadoop.hbase.security.access.AccessControlLists.getCellPermissionsForUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.Cell)", "public static java.util.List<org.apache.hadoop.hbase.security.access.Permission> getCellPermissionsForUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$1.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$1.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.security.access.UserPermission>", "org.apache.hadoop.hbase.security.access.AccessController$10.run()", "public java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$10.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.security.access.UserPermission>", "org.apache.hadoop.hbase.security.access.AccessController$11.run()", "public java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$11.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.util.List<org.apache.hadoop.hbase.security.access.UserPermission>", "org.apache.hadoop.hbase.security.access.AccessController$12.run()", "public java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$12.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$2.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$2.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$3.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$3.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$4.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$4.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$5.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$5.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$6.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$6.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$7.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$7.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$8.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$8.run()", "public java.lang.Object run() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.security.access.AccessController$9.run()", "public java.lang.Void run() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.AccessController$9.run()", "public java.lang.Object run() throws java.lang.Exception"], ["org.apache.hadoop.hbase.security.access.AccessController$OpType[]", "org.apache.hadoop.hbase.security.access.AccessController$OpType.values()", "public static org.apache.hadoop.hbase.security.access.AccessController$OpType[] values()"], ["org.apache.hadoop.hbase.security.access.AccessController$OpType", "org.apache.hadoop.hbase.security.access.AccessController$OpType.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.security.access.AccessController$OpType valueOf(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AccessController$OpType.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.security.access.AccessController", "org.apache.hadoop.hbase.security.access.AccessController()", "public org.apache.hadoop.hbase.security.access.AccessController()"], ["org.apache.hadoop.hbase.regionserver.Region", "org.apache.hadoop.hbase.security.access.AccessController.getRegion()", "public org.apache.hadoop.hbase.regionserver.Region getRegion()"], ["org.apache.hadoop.hbase.security.access.TableAuthManager", "org.apache.hadoop.hbase.security.access.AccessController.getAuthManager()", "public org.apache.hadoop.hbase.security.access.TableAuthManager getAuthManager()"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.requireNamespacePermission(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...)", "public void requireNamespacePermission(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.requireNamespacePermission(java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, org.apache.hadoop.hbase.security.access.Permission$Action...)", "public void requireNamespacePermission(java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public void postCreateTableHandler(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void postDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName)", "public void preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean)", "public void preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo)", "public void preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean)", "public boolean preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor)", "public void preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public void preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor)", "public void preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String)", "public void preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>)", "public void postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[])", "public void preSplit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.InternalScanner", "org.apache.hadoop.hbase.security.access.AccessController.preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType)", "public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result)", "public void preGetClosestRowBefore(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean)", "public boolean preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability)", "public void postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.regionserver.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean)", "public boolean preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean)", "public boolean preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.filter.CompareFilter$CompareOp, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.security.access.AccessController.preIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean)", "public long preIncrementColumnValue(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], long, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.access.AccessController.preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.access.AccessController.preAppendAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppendAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.access.AccessController.preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.access.AccessController.preIncrementAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrementAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.security.access.AccessController.postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.security.access.AccessController.preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.security.access.AccessController.postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.AccessController.preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean)", "public boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>)", "public void preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.prePrepareBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadRequest)", "public void prePrepareBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadRequest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preCleanupBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadRequest)", "public void preCleanupBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadRequest) throws java.io.IOException"], ["com.google.protobuf.Message", "org.apache.hadoop.hbase.security.access.AccessController.preEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message)", "public com.google.protobuf.Message preEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message, com.google.protobuf.Message$Builder)", "public void postEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, com.google.protobuf.Service, java.lang.String, com.google.protobuf.Message, com.google.protobuf.Message$Builder) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.grant(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GrantRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GrantResponse>)", "public void grant(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GrantRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GrantResponse>)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.revoke(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$RevokeRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$RevokeResponse>)", "public void revoke(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$RevokeRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$RevokeResponse>)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GetUserPermissionsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GetUserPermissionsResponse>)", "public void getUserPermissions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GetUserPermissionsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$GetUserPermissionsResponse>)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.checkPermissions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$CheckPermissionsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$CheckPermissionsResponse>)", "public void checkPermissions(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$CheckPermissionsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$CheckPermissionsResponse>)"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.security.access.AccessController.getService()", "public com.google.protobuf.Service getService()"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean)", "public void preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String)", "public void postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.HTableDescriptor>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void preMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.client.Mutation>)", "public void preMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postMergeCommit(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void preRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region)", "public void postRollBackMerge(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.Region) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>)", "public void postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint", "org.apache.hadoop.hbase.security.access.AccessController.postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.postReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner)", "public void postReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.AccessController.preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas)", "public void preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params()", "public org.apache.hadoop.hbase.security.access.AuthResult$Params()"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params.setNamespace(java.lang.String)", "public org.apache.hadoop.hbase.security.access.AuthResult$Params setNamespace(java.lang.String)"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params.setTableName(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.security.access.AuthResult$Params setTableName(org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params.setFamilies(java.util.Map<byte[], ? extends java.util.Collection<?>>)", "public org.apache.hadoop.hbase.security.access.AuthResult$Params setFamilies(java.util.Map<byte[], ? extends java.util.Collection<?>>)"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params.setFamily(byte[])", "public org.apache.hadoop.hbase.security.access.AuthResult$Params setFamily(byte[])"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult$Params.setQualifier(byte[])", "public org.apache.hadoop.hbase.security.access.AuthResult$Params setQualifier(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult$Params.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])", "public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)", "public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)", "public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.security.access.AuthResult.isAllowed()", "public boolean isAllowed()"], ["org.apache.hadoop.hbase.security.User", "org.apache.hadoop.hbase.security.access.AuthResult.getUser()", "public org.apache.hadoop.hbase.security.User getUser()"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult.getReason()", "public java.lang.String getReason()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.security.access.AuthResult.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["byte[]", "org.apache.hadoop.hbase.security.access.AuthResult.getFamily()", "public byte[] getFamily()"], ["byte[]", "org.apache.hadoop.hbase.security.access.AuthResult.getQualifier()", "public byte[] getQualifier()"], ["org.apache.hadoop.hbase.security.access.Permission$Action", "org.apache.hadoop.hbase.security.access.AuthResult.getAction()", "public org.apache.hadoop.hbase.security.access.Permission$Action getAction()"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult.getRequest()", "public java.lang.String getRequest()"], ["org.apache.hadoop.hbase.security.access.AuthResult$Params", "org.apache.hadoop.hbase.security.access.AuthResult.getParams()", "public org.apache.hadoop.hbase.security.access.AuthResult$Params getParams()"], ["void", "org.apache.hadoop.hbase.security.access.AuthResult.setAllowed(boolean)", "public void setAllowed(boolean)"], ["void", "org.apache.hadoop.hbase.security.access.AuthResult.setReason(java.lang.String)", "public void setReason(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult.toContextString()", "public java.lang.String toContextString()"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)", "public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])", "public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)", "public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)", "public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String)"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])", "public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[])"], ["org.apache.hadoop.hbase.security.access.AuthResult", "org.apache.hadoop.hbase.security.access.AuthResult.deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)", "public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>)"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.AuthResult.toFamilyString()", "public java.lang.String toFamilyString()"], ["org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance()", "public org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance()"], ["org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance(java.lang.Class<?>, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance(java.lang.Class<?>, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration$NullInstance.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.getWritableSize()", "public long getWritableSize()"], ["void", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.access.HbaseObjectWritableFor96Migration.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["java.lang.Boolean", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run()", "public java.lang.Boolean run()"], ["java.lang.Object", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run()", "public java.lang.Object run()"], ["org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener(org.apache.hadoop.fs.FileSystem, java.lang.String, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener(org.apache.hadoop.fs.FileSystem, java.lang.String, org.apache.hadoop.conf.Configuration)"], ["java.lang.String", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener.prepareBulkLoad(byte[], java.lang.String)", "public java.lang.String prepareBulkLoad(byte[], java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener.doneBulkLoad(byte[], java.lang.String)", "public void doneBulkLoad(byte[], java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$SecureBulkLoadListener.failedBulkLoad(byte[], java.lang.String)", "public void failedBulkLoad(byte[], java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint()", "public org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint()"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.prepareBulkLoad(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadResponse>)", "public void prepareBulkLoad(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$PrepareBulkLoadResponse>)"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.cleanupBulkLoad(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadResponse>)", "public void cleanupBulkLoad(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$CleanupBulkLoadResponse>)"], ["void", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.secureBulkLoadHFiles(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadHFilesRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadHFilesResponse>)", "public void secureBulkLoadHFiles(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadHFilesRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadHFilesResponse>)"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.getService()", "public com.google.protobuf.Service getService()"], ["java.util.List<T>", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.getUser(java.lang.String)", "public java.util.List<T> getUser(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.putUser(java.lang.String, T)", "public void putUser(java.lang.String, T)"], ["java.util.List<T>", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.replaceUser(java.lang.String, java.lang.Iterable<? extends T>)", "public java.util.List<T> replaceUser(java.lang.String, java.lang.Iterable<? extends T>)"], ["java.util.List<T>", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.getGroup(java.lang.String)", "public java.util.List<T> getGroup(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.putGroup(java.lang.String, T)", "public void putGroup(java.lang.String, T)"], ["java.util.List<T>", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.replaceGroup(java.lang.String, java.lang.Iterable<? extends T>)", "public java.util.List<T> replaceGroup(java.lang.String, java.lang.Iterable<? extends T>)"], ["com.google.common.collect.ListMultimap<java.lang.String, T>", "org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache.getAllPermissions()", "public com.google.common.collect.ListMultimap<java.lang.String, T> getAllPermissions()"], ["org.apache.hadoop.hbase.security.access.ZKPermissionWatcher", "org.apache.hadoop.hbase.security.access.TableAuthManager.getZKPermissionWatcher()", "public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZKPermissionWatcher()"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.refreshTableCacheFromWritable(org.apache.hadoop.hbase.TableName, byte[])", "public void refreshTableCacheFromWritable(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.refreshNamespaceCacheFromWritable(java.lang.String, byte[])", "public void refreshNamespaceCacheFromWritable(java.lang.String, byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorize(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorizeUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorizeUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorizeUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorizeUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.userHasAccess(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean userHasAccess(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorizeGroup(java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorizeGroup(java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorizeGroup(java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorizeGroup(java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.groupHasAccess(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean groupHasAccess(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.hasAccess(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean hasAccess(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean authorize(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.matchPermission(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean matchPermission(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["boolean", "org.apache.hadoop.hbase.security.access.TableAuthManager.matchPermission(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)", "public boolean matchPermission(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.removeNamespace(byte[])", "public void removeNamespace(byte[])"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.removeTable(org.apache.hadoop.hbase.TableName)", "public void removeTable(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.setTableUserPermissions(java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void setTableUserPermissions(java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.setTableGroupPermissions(java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void setTableGroupPermissions(java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.setNamespaceUserPermissions(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void setNamespaceUserPermissions(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.setNamespaceGroupPermissions(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void setNamespaceGroupPermissions(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.writeTableToZooKeeper(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void writeTableToZooKeeper(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["void", "org.apache.hadoop.hbase.security.access.TableAuthManager.writeNamespaceToZooKeeper(java.lang.String, org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache<org.apache.hadoop.hbase.security.access.TablePermission>)", "public void writeNamespaceToZooKeeper(java.lang.String, org.apache.hadoop.hbase.security.access.TableAuthManager$PermissionCache<org.apache.hadoop.hbase.security.access.TablePermission>)"], ["long", "org.apache.hadoop.hbase.security.access.TableAuthManager.getMTime()", "public long getMTime()"], ["synchronized", "org.apache.hadoop.hbase.security.access.TableAuthManager.org.apache.hadoop.hbase.security.access.TableAuthManager get(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.conf.Configuration)", "public static synchronized org.apache.hadoop.hbase.security.access.TableAuthManager get(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.access.ZKPermissionWatcher", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.access.TableAuthManager, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.access.TableAuthManager, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.start()", "public void start() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.writeToZookeeper(byte[], byte[])", "public void writeToZookeeper(byte[], byte[])"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.deleteTableACLNode(org.apache.hadoop.hbase.TableName)", "public void deleteTableACLNode(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.deleteNamespaceACLNode(java.lang.String)", "public void deleteNamespaceACLNode(java.lang.String)"], ["org.apache.hadoop.hbase.security.token.AuthenticationKey", "org.apache.hadoop.hbase.security.token.AuthenticationKey()", "public org.apache.hadoop.hbase.security.token.AuthenticationKey()"], ["org.apache.hadoop.hbase.security.token.AuthenticationKey", "org.apache.hadoop.hbase.security.token.AuthenticationKey(int, long, javax.crypto.SecretKey)", "public org.apache.hadoop.hbase.security.token.AuthenticationKey(int, long, javax.crypto.SecretKey)"], ["int", "org.apache.hadoop.hbase.security.token.AuthenticationKey.getKeyId()", "public int getKeyId()"], ["long", "org.apache.hadoop.hbase.security.token.AuthenticationKey.getExpiration()", "public long getExpiration()"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationKey.setExpiration(long)", "public void setExpiration(long)"], ["int", "org.apache.hadoop.hbase.security.token.AuthenticationKey.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.security.token.AuthenticationKey.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.security.token.AuthenticationKey.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationKey.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationKey.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector(org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector(org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.isMaster()", "public boolean isMaster()"], ["boolean", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.isStopped()", "public boolean isStopped()"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(java.lang.String)", "public void stop(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.run()", "public void run()"], ["org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, long, long)", "public org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, long, long)"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.stop()", "public void stop()"], ["boolean", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.isMaster()", "public boolean isMaster()"], ["java.lang.String", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.getName()", "public java.lang.String getName()"], ["byte[]", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.retrievePassword(org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier)", "public byte[] retrievePassword(org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken"], ["org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.createIdentifier()", "public org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier createIdentifier()"], ["org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier>", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.generateToken(java.lang.String)", "public org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> generateToken(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.void addKey(org.apache.hadoop.hbase.security.token.AuthenticationKey)", "public synchronized void addKey(org.apache.hadoop.hbase.security.token.AuthenticationKey) throws java.io.IOException"], ["javax.crypto.SecretKey", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.createSecretKey(byte[])", "public static javax.crypto.SecretKey createSecretKey(byte[])"], ["org.apache.hadoop.security.token.TokenIdentifier", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.createIdentifier()", "public org.apache.hadoop.security.token.TokenIdentifier createIdentifier()"], ["byte[]", "org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)", "public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken"], ["org.apache.hadoop.hbase.security.token.FsDelegationToken", "org.apache.hadoop.hbase.security.token.FsDelegationToken(org.apache.hadoop.hbase.security.UserProvider, java.lang.String)", "public org.apache.hadoop.hbase.security.token.FsDelegationToken(org.apache.hadoop.hbase.security.UserProvider, java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.token.FsDelegationToken.acquireDelegationToken(org.apache.hadoop.fs.FileSystem)", "public void acquireDelegationToken(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.token.FsDelegationToken.releaseDelegationToken()", "public void releaseDelegationToken()"], ["org.apache.hadoop.hbase.security.UserProvider", "org.apache.hadoop.hbase.security.token.FsDelegationToken.getUserProvider()", "public org.apache.hadoop.hbase.security.UserProvider getUserProvider()"], ["java.lang.String", "org.apache.hadoop.hbase.security.token.FsDelegationToken.getRenewer()", "public java.lang.String getRenewer()"], ["org.apache.hadoop.security.token.Token<?>", "org.apache.hadoop.hbase.security.token.FsDelegationToken.getUserToken()", "public org.apache.hadoop.security.token.Token<?> getUserToken()"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.security.token.FsDelegationToken.getFileSystem()", "public org.apache.hadoop.fs.FileSystem getFileSystem()"], ["org.apache.hadoop.hbase.security.token.TokenProvider", "org.apache.hadoop.hbase.security.token.TokenProvider()", "public org.apache.hadoop.hbase.security.token.TokenProvider()"], ["void", "org.apache.hadoop.hbase.security.token.TokenProvider.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment)"], ["void", "org.apache.hadoop.hbase.security.token.TokenProvider.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.security.token.TokenProvider.getService()", "public com.google.protobuf.Service getService()"], ["void", "org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenResponse>)", "public void getAuthenticationToken(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenResponse>)"], ["void", "org.apache.hadoop.hbase.security.token.TokenProvider.whoAmI(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$WhoAmIRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$WhoAmIResponse>)", "public void whoAmI(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$WhoAmIRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$WhoAmIResponse>)"], ["org.apache.hadoop.hbase.security.token.ZKSecretWatcher", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager)", "public org.apache.hadoop.hbase.security.token.ZKSecretWatcher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.start()", "public void start() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.getRootKeyZNode()", "public java.lang.String getRootKeyZNode()"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.removeKeyFromZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)", "public void removeKeyFromZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.addKeyToZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)", "public void addKeyToZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)"], ["void", "org.apache.hadoop.hbase.security.token.ZKSecretWatcher.updateKeyInZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)", "public void updateKeyInZK(org.apache.hadoop.hbase.security.token.AuthenticationKey)"], ["boolean", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl$1.evaluate(org.apache.hadoop.hbase.Cell)", "public boolean evaluate(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl$2.evaluate(org.apache.hadoop.hbase.Cell)", "public boolean evaluate(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl()", "public org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl()"], ["void", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.init(org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment)", "public void init(org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.addLabels(java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] addLabels(java.util.List<byte[]>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.setAuths(byte[], java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] setAuths(byte[], java.util.List<byte[]>) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.OperationStatus[]", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.clearAuths(byte[], java.util.List<byte[]>)", "public org.apache.hadoop.hbase.regionserver.OperationStatus[] clearAuths(byte[], java.util.List<byte[]>) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getAuths(byte[], boolean)", "public java.util.List<java.lang.String> getAuths(byte[], boolean) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getUserAuths(byte[], boolean)", "public java.util.List<java.lang.String> getUserAuths(byte[], boolean) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getGroupAuths(java.lang.String[], boolean)", "public java.util.List<java.lang.String> getGroupAuths(java.lang.String[], boolean) throws java.io.IOException"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.listLabels(java.lang.String)", "public java.util.List<java.lang.String> listLabels(java.lang.String) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.Tag>", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.createVisibilityExpTags(java.lang.String, boolean, boolean)", "public java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String, boolean, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.getVisibilityExpEvaluator(org.apache.hadoop.hbase.security.visibility.Authorizations)", "public org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator getVisibilityExpEvaluator(org.apache.hadoop.hbase.security.visibility.Authorizations) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.havingSystemAuth(byte[])", "public boolean havingSystemAuth(byte[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.havingSystemAuth(org.apache.hadoop.hbase.security.User)", "public boolean havingSystemAuth(org.apache.hadoop.hbase.security.User) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.matchVisibility(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte, java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte)", "public boolean matchVisibility(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte, java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException"], ["byte[]", "org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.encodeVisibilityForReplication(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte)", "public byte[] encodeVisibilityForReplication(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator", "org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator()", "public org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator()"], ["void", "org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator.getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)", "public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)"], ["org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator", "org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator()", "public org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator()"], ["void", "org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator.getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)", "public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)"], ["org.apache.hadoop.hbase.security.visibility.ExpressionExpander", "org.apache.hadoop.hbase.security.visibility.ExpressionExpander()", "public org.apache.hadoop.hbase.security.visibility.ExpressionExpander()"], ["org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode", "org.apache.hadoop.hbase.security.visibility.ExpressionExpander.expand(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode)", "public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode expand(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode)"], ["org.apache.hadoop.hbase.security.visibility.ExpressionParser", "org.apache.hadoop.hbase.security.visibility.ExpressionParser()", "public org.apache.hadoop.hbase.security.visibility.ExpressionParser()"], ["org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode", "org.apache.hadoop.hbase.security.visibility.ExpressionParser.parse(java.lang.String)", "public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode parse(java.lang.String) throws org.apache.hadoop.hbase.security.visibility.ParseException"], ["org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator", "org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator()", "public org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator()"], ["void", "org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator.getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)", "public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)"], ["org.apache.hadoop.hbase.security.visibility.ParseException", "org.apache.hadoop.hbase.security.visibility.ParseException()", "public org.apache.hadoop.hbase.security.visibility.ParseException()"], ["org.apache.hadoop.hbase.security.visibility.ParseException", "org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String)", "public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String)"], ["org.apache.hadoop.hbase.security.visibility.ParseException", "org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.Throwable)", "public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.Throwable)"], ["org.apache.hadoop.hbase.security.visibility.ParseException", "org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String, java.lang.Throwable)", "public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String, java.lang.Throwable)"], ["org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator", "org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator()", "public org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator()"], ["void", "org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator.getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)", "public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations)"], ["org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter", "org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte)", "public org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte)"], ["org.apache.hadoop.hbase.filter.Filter$ReturnCode", "org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter.filterKeyValue(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterKeyValue(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter.transformCell(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell transformCell(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication", "org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication()", "public org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication()"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["org.apache.hadoop.hbase.replication.ReplicationEndpoint", "org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication.postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)", "public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint)"], ["org.apache.hadoop.hbase.security.visibility.VisibilityController", "org.apache.hadoop.hbase.security.visibility.VisibilityController()", "public org.apache.hadoop.hbase.security.visibility.VisibilityController()"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.start(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.stop(org.apache.hadoop.hbase.CoprocessorEnvironment)", "public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>)", "public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor)", "public void preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preAddColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor)", "public void preModifyColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.HColumnDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[])", "public void preDeleteColumn(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName)", "public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void postLogReplay(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get)", "public void prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.DeleteTracker)", "public org.apache.hadoop.hbase.regionserver.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.DeleteTracker) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.RegionScanner", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner)", "public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean)", "public boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner)", "public void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>)", "public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append)", "public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Result", "org.apache.hadoop.hbase.security.visibility.VisibilityController.preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment)", "public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.security.visibility.VisibilityController.postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["com.google.protobuf.Service", "org.apache.hadoop.hbase.security.visibility.VisibilityController.getService()", "public com.google.protobuf.Service getService()"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityController.void addLabels(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)", "public synchronized void addLabels(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityController.void setAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)", "public synchronized void setAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityController.void getAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$GetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$GetAuthsResponse>)", "public synchronized void getAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$GetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$GetAuthsResponse>)"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityController.void clearAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)", "public synchronized void clearAuths(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>)"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityController.void listLabels(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$ListLabelsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$ListLabelsResponse>)", "public synchronized void listLabels(com.google.protobuf.RpcController, org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$ListLabelsRequest, com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$ListLabelsResponse>)"], ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter(org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator, java.util.Map<org.apache.hadoop.hbase.util.ByteRange, java.lang.Integer>)", "public org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter(org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator, java.util.Map<org.apache.hadoop.hbase.util.ByteRange, java.lang.Integer>)"], ["org.apache.hadoop.hbase.filter.Filter$ReturnCode", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter.filterKeyValue(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterKeyValue(org.apache.hadoop.hbase.Cell) throws java.io.IOException"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter.transformCell(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.Cell transformCell(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter.reset()", "public void reset() throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager.getInstance()", "public static org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager getInstance()"], ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelService", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager.getVisibilityLabelService(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.security.visibility.VisibilityLabelService getVisibilityLabelService(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelService", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager.getVisibilityLabelService()", "public org.apache.hadoop.hbase.security.visibility.VisibilityLabelService getVisibilityLabelService()"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache createAndGet(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.conf.Configuration)", "public static synchronized org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache createAndGet(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.get()", "public static org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache get()"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.refreshLabelsCache(byte[])", "public void refreshLabelsCache(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.refreshUserAuthsCache(byte[])", "public void refreshUserAuthsCache(byte[]) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getLabelOrdinal(java.lang.String)", "public int getLabelOrdinal(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getLabel(int)", "public java.lang.String getLabel(int)"], ["int", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getLabelsCount()", "public int getLabelsCount()"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getUserAuths(java.lang.String)", "public java.util.List<java.lang.String> getUserAuths(java.lang.String)"], ["java.util.List<java.lang.String>", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getGroupAuths(java.lang.String[])", "public java.util.List<java.lang.String> getGroupAuths(java.lang.String[])"], ["java.util.Set<java.lang.Integer>", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getUserAuthsAsOrdinals(java.lang.String)", "public java.util.Set<java.lang.Integer> getUserAuthsAsOrdinals(java.lang.String)"], ["java.util.Set<java.lang.Integer>", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.getGroupAuthsAsOrdinals(java.lang.String[])", "public java.util.Set<java.lang.Integer> getGroupAuthsAsOrdinals(java.lang.String[])"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.writeToZookeeper(byte[], boolean)", "public void writeToZookeeper(byte[], boolean)"], ["org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint, org.apache.hadoop.hbase.security.visibility.VisibilityLabelService)", "public org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint, org.apache.hadoop.hbase.security.visibility.VisibilityLabelService)"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context)", "public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)", "public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext)"], ["synchronized", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.java.util.UUID getPeerUUID()", "public synchronized java.util.UUID getPeerUUID()"], ["boolean", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.canReplicateToSameCluster()", "public boolean canReplicateToSameCluster()"], ["org.apache.hadoop.hbase.replication.WALEntryFilter", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.getWALEntryfilter()", "public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter()"], ["boolean", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.isRunning()", "public boolean isRunning()"], ["com.google.common.util.concurrent.ListenableFuture<com.google.common.util.concurrent.Service$State>", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.start()", "public com.google.common.util.concurrent.ListenableFuture<com.google.common.util.concurrent.Service$State> start()"], ["com.google.common.util.concurrent.Service$State", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.startAndWait()", "public com.google.common.util.concurrent.Service$State startAndWait()"], ["com.google.common.util.concurrent.Service$State", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.state()", "public com.google.common.util.concurrent.Service$State state()"], ["com.google.common.util.concurrent.ListenableFuture<com.google.common.util.concurrent.Service$State>", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.stop()", "public com.google.common.util.concurrent.ListenableFuture<com.google.common.util.concurrent.Service$State> stop()"], ["com.google.common.util.concurrent.Service$State", "org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint.stopAndWait()", "public com.google.common.util.concurrent.Service$State stopAndWait()"], ["org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker", "org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker()", "public org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker()"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.add(org.apache.hadoop.hbase.Cell)", "public void add(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult", "org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.isDeleted(org.apache.hadoop.hbase.Cell)", "public org.apache.hadoop.hbase.regionserver.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell)"], ["void", "org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.reset()", "public void reset()"], ["org.apache.hadoop.hbase.security.visibility.VisibilityUtils", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils()", "public org.apache.hadoop.hbase.security.visibility.VisibilityUtils()"], ["byte[]", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.getDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.lang.Integer>)", "public static byte[] getDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.lang.Integer>)"], ["byte[]", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.getUserAuthsDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.util.List<java.lang.Integer>>)", "public static byte[] getUserAuthsDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.util.List<java.lang.Integer>>)"], ["java.util.List<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabel>", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.readLabelsFromZKData(byte[])", "public static java.util.List<org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$VisibilityLabel> readLabelsFromZKData(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException"], ["org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$MultiUserAuthorizations", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.readUserAuthsFromZKData(byte[])", "public static org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos$MultiUserAuthorizations readUserAuthsFromZKData(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException"], ["java.util.List<org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator>", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.getScanLabelGenerators(org.apache.hadoop.conf.Configuration)", "public static java.util.List<org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator> getScanLabelGenerators(org.apache.hadoop.conf.Configuration)"], ["java.lang.Byte", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.extractVisibilityTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>)", "public static java.lang.Byte extractVisibilityTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>)"], ["java.lang.Byte", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.extractAndPartitionTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>, java.util.List<org.apache.hadoop.hbase.Tag>)", "public static java.lang.Byte extractAndPartitionTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>, java.util.List<org.apache.hadoop.hbase.Tag>)"], ["boolean", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.isVisibilityTagsPresent(org.apache.hadoop.hbase.Cell)", "public static boolean isVisibilityTagsPresent(org.apache.hadoop.hbase.Cell)"], ["org.apache.hadoop.hbase.filter.Filter", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.createVisibilityLabelFilter(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.security.visibility.Authorizations)", "public static org.apache.hadoop.hbase.filter.Filter createVisibilityLabelFilter(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.security.visibility.Authorizations) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.User", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.getActiveUser()", "public static org.apache.hadoop.hbase.security.User getActiveUser() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.Tag>", "org.apache.hadoop.hbase.security.visibility.VisibilityUtils.createVisibilityExpTags(java.lang.String, boolean, boolean, java.util.Set<java.lang.Integer>, org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider)", "public static java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String, boolean, boolean, java.util.Set<java.lang.Integer>, org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider) throws java.io.IOException"], ["org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache, org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.start()", "public void start() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.nodeCreated(java.lang.String)", "public void nodeCreated(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.writeToZookeeper(byte[], boolean)", "public void writeToZookeeper(byte[], boolean)"], ["org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode(java.lang.String)", "public org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.getIdentifier()", "public java.lang.String getIdentifier()"], ["int", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.isSingleNode()", "public boolean isSingleNode()"], ["org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.deepClone()", "public org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode deepClone()"], ["org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode.deepClone()", "public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode deepClone()"], ["org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode()", "public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode()"], ["org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator)", "public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator)"], ["org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>)", "public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>)"], ["org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode...)", "public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode...)"], ["org.apache.hadoop.hbase.security.visibility.expression.Operator", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.getOperator()", "public org.apache.hadoop.hbase.security.visibility.expression.Operator getOperator()"], ["java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.getChildExps()", "public java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode> getChildExps()"], ["void", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.addChildExp(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode)", "public void addChildExp(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode)"], ["void", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.addChildExps(java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>)", "public void addChildExps(java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>)"], ["java.lang.String", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.toString()", "public java.lang.String toString()"], ["boolean", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.isSingleNode()", "public boolean isSingleNode()"], ["org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.deepClone()", "public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode deepClone()"], ["org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode", "org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode.deepClone()", "public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode deepClone()"], ["org.apache.hadoop.hbase.security.visibility.expression.Operator[]", "org.apache.hadoop.hbase.security.visibility.expression.Operator.values()", "public static org.apache.hadoop.hbase.security.visibility.expression.Operator[] values()"], ["org.apache.hadoop.hbase.security.visibility.expression.Operator", "org.apache.hadoop.hbase.security.visibility.expression.Operator.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.security.visibility.expression.Operator valueOf(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.security.visibility.expression.Operator.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.snapshot.CreateSnapshot", "org.apache.hadoop.hbase.snapshot.CreateSnapshot()", "public org.apache.hadoop.hbase.snapshot.CreateSnapshot()"], ["void", "org.apache.hadoop.hbase.snapshot.CreateSnapshot.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$1.storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile)", "public void storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$1.logFile(java.lang.String, java.lang.String)", "public void logFile(java.lang.String, java.lang.String) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$2.compare(org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>)", "public int compare(org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>)"], ["int", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$2.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter[]", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter.values()", "public static org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter[] values()"], ["org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context)", "public void setup(org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context)", "public void map(org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.mapreduce.Mapper<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable, org.apache.hadoop.io.NullWritable>.Context) throws java.lang.InterruptedException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context)", "public void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit()", "public org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit()"], ["org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>>)", "public org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotFileInfo, java.lang.Long>>)"], ["long", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit.getLength()", "public long getLength() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.String[]", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit.getLocations()", "public java.lang.String[] getLocations() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.close()", "public void close()"], ["org.apache.hadoop.io.BytesWritable", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.getCurrentKey()", "public org.apache.hadoop.io.BytesWritable getCurrentKey()"], ["org.apache.hadoop.io.NullWritable", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.getCurrentValue()", "public org.apache.hadoop.io.NullWritable getCurrentValue()"], ["float", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.getProgress()", "public float getProgress()"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public void initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)"], ["boolean", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.nextKeyValue()", "public boolean nextKeyValue()"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.getCurrentValue()", "public java.lang.Object getCurrentValue() throws java.io.IOException, java.lang.InterruptedException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader.getCurrentKey()", "public java.lang.Object getCurrentKey() throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable>", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)", "public org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.BytesWritable, org.apache.hadoop.io.NullWritable> createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) throws java.io.IOException, java.lang.InterruptedException"], ["java.util.List<org.apache.hadoop.mapreduce.InputSplit>", "org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat.getSplits(org.apache.hadoop.mapreduce.JobContext)", "public java.util.List<org.apache.hadoop.mapreduce.InputSplit> getSplits(org.apache.hadoop.mapreduce.JobContext) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.snapshot.ExportSnapshot", "org.apache.hadoop.hbase.snapshot.ExportSnapshot()", "public org.apache.hadoop.hbase.snapshot.ExportSnapshot()"], ["int", "org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$1.editRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void editRegion(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$2.editRegion(org.apache.hadoop.hbase.HRegionInfo)", "public void editRegion(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$3.fillRegion(org.apache.hadoop.hbase.regionserver.HRegion)", "public void fillRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.hasRegionsToAdd()", "public boolean hasRegionsToAdd()"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.getRegionsToAdd()", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> getRegionsToAdd()"], ["boolean", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.hasRegionsToRestore()", "public boolean hasRegionsToRestore()"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.getRegionsToRestore()", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> getRegionsToRestore()"], ["boolean", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.hasRegionsToRemove()", "public boolean hasRegionsToRemove()"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.getRegionsToRemove()", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> getRegionsToRemove()"], ["void", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges.updateMetaParentRegions(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void updateMetaParentRegions(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, org.apache.hadoop.hbase.monitoring.MonitoredTask)", "public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, org.apache.hadoop.hbase.monitoring.MonitoredTask)"], ["org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.restoreHdfsRegions()", "public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges restoreHdfsRegions() throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.cloneRegionInfo(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.HRegionInfo cloneRegionInfo(org.apache.hadoop.hbase.HRegionInfo)"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.cloneTableSchema(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.TableName)", "public static org.apache.hadoop.hbase.HTableDescriptor cloneTableSchema(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.copySnapshotForScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String)", "public static void copySnapshotForScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter(org.apache.hadoop.fs.FileSystem)"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getMaxMasterTimeout(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription$Type, long)", "public static long getMaxMasterTimeout(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription$Type, long)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getSnapshotRootDir(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getSnapshotRootDir(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getCompletedSnapshotDir(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getCompletedSnapshotDir(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getCompletedSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getCompletedSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getWorkingSnapshotDir(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getWorkingSnapshotDir(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getWorkingSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.getSnapshotsDir(org.apache.hadoop.fs.Path)", "public static final org.apache.hadoop.fs.Path getSnapshotsDir(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.validate(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription validate(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.conf.Configuration) throws java.lang.IllegalArgumentException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.writeSnapshotInfo(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem)", "public static void writeSnapshotInfo(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.readSnapshotInfo(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription readSnapshotInfo(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.completeSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem)", "public static void completeSnapshot(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem) throws org.apache.hadoop.hbase.snapshot.SnapshotCreationException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.isSnapshotOwner(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.security.User)", "public static boolean isSnapshotOwner(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.security.User)"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$1.storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile)", "public void storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$1.logFile(java.lang.String, java.lang.String)", "public void logFile(java.lang.String, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$2.storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile)", "public void storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$2.logFile(java.lang.String, java.lang.String)", "public void logFile(java.lang.String, java.lang.String) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats$FileInfo.inArchive()", "public boolean inArchive()"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats$FileInfo.isCorrupted()", "public boolean isCorrupted()"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats$FileInfo.isMissing()", "public boolean isMissing()"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats$FileInfo.getSize()", "public long getSize()"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getSnapshotDescription()", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription getSnapshotDescription()"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.isSnapshotCorrupted()", "public boolean isSnapshotCorrupted()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getStoreFilesCount()", "public int getStoreFilesCount()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getArchivedStoreFilesCount()", "public int getArchivedStoreFilesCount()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getLogsCount()", "public int getLogsCount()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getMissingStoreFilesCount()", "public int getMissingStoreFilesCount()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getCorruptedStoreFilesCount()", "public int getCorruptedStoreFilesCount()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getMissingLogsCount()", "public int getMissingLogsCount()"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getStoreFilesSize()", "public long getStoreFilesSize()"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getSharedStoreFilesSize()", "public long getSharedStoreFilesSize()"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getArchivedStoreFileSize()", "public long getArchivedStoreFileSize()"], ["float", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getSharedStoreFilePercentage()", "public float getSharedStoreFilePercentage()"], ["long", "org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats.getLogsSize()", "public long getLogsSize()"], ["org.apache.hadoop.hbase.snapshot.SnapshotInfo", "org.apache.hadoop.hbase.snapshot.SnapshotInfo()", "public org.apache.hadoop.hbase.snapshot.SnapshotInfo()"], ["int", "org.apache.hadoop.hbase.snapshot.SnapshotInfo.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats", "org.apache.hadoop.hbase.snapshot.SnapshotInfo.getSnapshotStats(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public static org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats getSnapshotStats(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription>", "org.apache.hadoop.hbase.snapshot.SnapshotInfo.getSnapshotList(org.apache.hadoop.conf.Configuration)", "public static java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription> getSnapshotList(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotInfo.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.snapshot.SnapshotManifest", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare)", "public static org.apache.hadoop.hbase.snapshot.SnapshotManifest create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare)"], ["org.apache.hadoop.hbase.snapshot.SnapshotManifest", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.open(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public static org.apache.hadoop.hbase.snapshot.SnapshotManifest open(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.addTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor)", "public void addTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.addRegion(org.apache.hadoop.hbase.regionserver.HRegion)", "public void addRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.addRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo)", "public void addRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.getSnapshotDir()", "public org.apache.hadoop.fs.Path getSnapshotDir()"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.getSnapshotDescription()", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription getSnapshotDescription()"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.getTableDescriptor()", "public org.apache.hadoop.hbase.HTableDescriptor getTableDescriptor()"], ["java.util.List<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest>", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.getRegionManifests()", "public java.util.List<org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> getRegionManifests()"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest>", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.getRegionManifestsMap()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> getRegionManifestsMap()"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.consolidate()", "public void consolidate() throws java.io.IOException"], ["java.util.concurrent.ThreadPoolExecutor", "org.apache.hadoop.hbase.snapshot.SnapshotManifest.createExecutor(org.apache.hadoop.conf.Configuration, java.lang.String)", "public static java.util.concurrent.ThreadPoolExecutor createExecutor(org.apache.hadoop.conf.Configuration, java.lang.String)"], ["org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$1.call()", "public org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$1.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.regionserver.HRegionFileSystem", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.regionOpen(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.regionserver.HRegionFileSystem regionOpen(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.regionClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem)", "public void regionClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.familyOpen(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, byte[])", "public org.apache.hadoop.fs.Path familyOpen(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, byte[])"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.familyClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path)", "public void familyClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path)"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.storeFile(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.regionserver.StoreFileInfo)", "public void storeFile(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo)", "public void storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.familyClose(java.lang.Object, java.lang.Object)", "public void familyClose(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.familyOpen(java.lang.Object, byte[])", "public java.lang.Object familyOpen(java.lang.Object, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.regionClose(java.lang.Object)", "public void regionClose(java.lang.Object) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder.regionOpen(org.apache.hadoop.hbase.HRegionInfo)", "public java.lang.Object regionOpen(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$2.call()", "public org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.regionOpen(org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder regionOpen(org.apache.hadoop.hbase.HRegionInfo)"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.regionClose(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder)", "public void regionClose(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder) throws java.io.IOException"], ["org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.familyOpen(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, byte[])", "public org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder familyOpen(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, byte[])"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.familyClose(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder)", "public void familyClose(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder)"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.storeFile(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder, org.apache.hadoop.hbase.regionserver.StoreFileInfo)", "public void storeFile(org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo)", "public void storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.familyClose(java.lang.Object, java.lang.Object)", "public void familyClose(java.lang.Object, java.lang.Object) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.familyOpen(java.lang.Object, byte[])", "public java.lang.Object familyOpen(java.lang.Object, byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.regionClose(java.lang.Object)", "public void regionClose(java.lang.Object) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder.regionOpen(org.apache.hadoop.hbase.HRegionInfo)", "public java.lang.Object regionOpen(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.snapshot.SnapshotManifestV2", "org.apache.hadoop.hbase.snapshot.SnapshotManifestV2()", "public org.apache.hadoop.hbase.snapshot.SnapshotManifestV2()"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$1.storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile)", "public void storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException"], ["java.lang.Void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$2.call()", "public java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$3.storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile)", "public void storeFile(org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$4.logFile(java.lang.String, java.lang.String)", "public void logFile(java.lang.String, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getLogsDir(org.apache.hadoop.fs.Path, java.lang.String)", "public static org.apache.hadoop.fs.Path getLogsDir(org.apache.hadoop.fs.Path, java.lang.String)"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor)", "public static void visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor)", "public static void visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.visitLogFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$LogFileVisitor)", "public static void visitLogFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$LogFileVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription)", "public static void verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$SnapshotDescription) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest)", "public static void verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.concurrentVisitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor)", "public static void concurrentVisitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor) throws java.io.IOException"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getHFileNames(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.Set<java.lang.String> getHFileNames(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.Set<java.lang.String>", "org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getWALNames(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.Set<java.lang.String> getWALNames(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.setFormat(java.lang.String)", "public void setFormat(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getFormat()", "public java.lang.String getFormat()"], ["boolean", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getFormat__IsNotDefault()", "public boolean getFormat__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor)", "public void setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor)"], ["org.apache.hadoop.hbase.monitoring.TaskMonitor", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getTaskMonitor()", "public org.apache.hadoop.hbase.monitoring.TaskMonitor getTaskMonitor()"], ["boolean", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getTaskMonitor__IsNotDefault()", "public boolean getTaskMonitor__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.setFilter(java.lang.String)", "public void setFilter(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getFilter()", "public java.lang.String getFilter()"], ["boolean", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData.getFilter__IsNotDefault()", "public boolean getFilter__IsNotDefault()"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl()", "public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl()"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.setFormat(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setFormat(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor)", "public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor)"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.setFilter(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setFilter(java.lang.String)"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.makeRenderer()", "public org.jamon.Renderer makeRenderer()"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.render(java.io.Writer)", "public void render(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData.setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)", "public void setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)"], ["org.apache.hadoop.hbase.master.AssignmentManager", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData.getAssignmentManager()", "public org.apache.hadoop.hbase.master.AssignmentManager getAssignmentManager()"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData.setLimit(int)", "public void setLimit(int)"], ["int", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData.getLimit()", "public int getLimit()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData.getLimit__IsNotDefault()", "public boolean getLimit__IsNotDefault()"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl()", "public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl()"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.setLimit(int)", "public final org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl setLimit(int)"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.makeRenderer(org.apache.hadoop.hbase.master.AssignmentManager)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.AssignmentManager)"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.render(java.io.Writer, org.apache.hadoop.hbase.master.AssignmentManager)", "public void render(java.io.Writer, org.apache.hadoop.hbase.master.AssignmentManager) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.AssignmentManager)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.AssignmentManager) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData.setMaster(org.apache.hadoop.hbase.master.HMaster)", "public void setMaster(org.apache.hadoop.hbase.master.HMaster)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData.getMaster()", "public org.apache.hadoop.hbase.master.HMaster getMaster()"], ["org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl()", "public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl()"], ["org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData getImplData()"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.makeRenderer(org.apache.hadoop.hbase.master.HMaster)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster)"], ["void", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setMaster(org.apache.hadoop.hbase.master.HMaster)", "public void setMaster(org.apache.hadoop.hbase.master.HMaster)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getMaster()", "public org.apache.hadoop.hbase.master.HMaster getMaster()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setFormat(java.lang.String)", "public void setFormat(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFormat()", "public java.lang.String getFormat()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFormat__IsNotDefault()", "public boolean getFormat__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>)", "public void setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>)"], ["java.util.Set<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getDeadServers()", "public java.util.Set<org.apache.hadoop.hbase.ServerName> getDeadServers()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getDeadServers__IsNotDefault()", "public boolean getDeadServers__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setFilter(java.lang.String)", "public void setFilter(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFilter()", "public java.lang.String getFilter()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFilter__IsNotDefault()", "public boolean getFilter__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setMetaLocation(org.apache.hadoop.hbase.ServerName)", "public void setMetaLocation(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getMetaLocation()", "public org.apache.hadoop.hbase.ServerName getMetaLocation()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getMetaLocation__IsNotDefault()", "public boolean getMetaLocation__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setFrags(java.util.Map<java.lang.String, java.lang.Integer>)", "public void setFrags(java.util.Map<java.lang.String, java.lang.Integer>)"], ["java.util.Map<java.lang.String, java.lang.Integer>", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFrags()", "public java.util.Map<java.lang.String, java.lang.Integer> getFrags()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getFrags__IsNotDefault()", "public boolean getFrags__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setServerManager(org.apache.hadoop.hbase.master.ServerManager)", "public void setServerManager(org.apache.hadoop.hbase.master.ServerManager)"], ["org.apache.hadoop.hbase.master.ServerManager", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getServerManager()", "public org.apache.hadoop.hbase.master.ServerManager getServerManager()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getServerManager__IsNotDefault()", "public boolean getServerManager__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public void setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getServers()", "public java.util.List<org.apache.hadoop.hbase.ServerName> getServers()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getServers__IsNotDefault()", "public boolean getServers__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)", "public void setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)"], ["org.apache.hadoop.hbase.master.AssignmentManager", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getAssignmentManager()", "public org.apache.hadoop.hbase.master.AssignmentManager getAssignmentManager()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getAssignmentManager__IsNotDefault()", "public boolean getAssignmentManager__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.setCatalogJanitorEnabled(boolean)", "public void setCatalogJanitorEnabled(boolean)"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getCatalogJanitorEnabled()", "public boolean getCatalogJanitorEnabled()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData.getCatalogJanitorEnabled__IsNotDefault()", "public boolean getCatalogJanitorEnabled__IsNotDefault()"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl()", "public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl()"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setFormat(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFormat(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setFilter(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFilter(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setMetaLocation(org.apache.hadoop.hbase.ServerName)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setMetaLocation(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setFrags(java.util.Map<java.lang.String, java.lang.Integer>)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFrags(java.util.Map<java.lang.String, java.lang.Integer>)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setServerManager(org.apache.hadoop.hbase.master.ServerManager)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setServerManager(org.apache.hadoop.hbase.master.ServerManager)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setAssignmentManager(org.apache.hadoop.hbase.master.AssignmentManager)"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.setCatalogJanitorEnabled(boolean)", "public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setCatalogJanitorEnabled(boolean)"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.makeRenderer(org.apache.hadoop.hbase.master.HMaster)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster)"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl.formatZKString()", "public java.lang.String formatZKString()"], ["org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData.setMaster(org.apache.hadoop.hbase.master.HMaster)", "public void setMaster(org.apache.hadoop.hbase.master.HMaster)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData.getMaster()", "public org.apache.hadoop.hbase.master.HMaster getMaster()"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData.setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public void setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData.getServers()", "public java.util.List<org.apache.hadoop.hbase.ServerName> getServers()"], ["boolean", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData.getServers__IsNotDefault()", "public boolean getServers__IsNotDefault()"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl()", "public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl()"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)", "public final org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl setServers(java.util.List<org.apache.hadoop.hbase.ServerName>)"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.makeRenderer(org.apache.hadoop.hbase.master.HMaster)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster)"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData.setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public void setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData.getCacheConfig()", "public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData.setConfig(org.apache.hadoop.conf.Configuration)", "public void setConfig(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData.getConfig()", "public org.apache.hadoop.conf.Configuration getConfig()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData getImplData()"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public void render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)", "public void setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig)"], ["org.apache.hadoop.hbase.io.hfile.CacheConfig", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.getCacheConfig()", "public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.setBcn(java.lang.String)", "public void setBcn(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.getBcn()", "public java.lang.String getBcn()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.setBcv(java.lang.String)", "public void setBcv(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData.getBcv()", "public java.lang.String getBcv()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData getImplData()"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)", "public void render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public void setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getRegionServer()", "public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.setBcv(java.lang.String)", "public void setBcv(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getBcv()", "public java.lang.String getBcv()"], ["boolean", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getBcv__IsNotDefault()", "public boolean getBcv__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.setFormat(java.lang.String)", "public void setFormat(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getFormat()", "public java.lang.String getFormat()"], ["boolean", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getFormat__IsNotDefault()", "public boolean getFormat__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.setBcn(java.lang.String)", "public void setBcn(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getBcn()", "public java.lang.String getBcn()"], ["boolean", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getBcn__IsNotDefault()", "public boolean getBcn__IsNotDefault()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.setFilter(java.lang.String)", "public void setFilter(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getFilter()", "public java.lang.String getFilter()"], ["boolean", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData.getFilter__IsNotDefault()", "public boolean getFilter__IsNotDefault()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl()", "public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.setBcv(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setBcv(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.setFormat(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setFormat(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.setBcn(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setBcn(java.lang.String)"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.setFilter(java.lang.String)", "public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setFilter(java.lang.String)"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData.setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer)", "public void setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData.getRegionServer()", "public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData.setOnlineRegions(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void setOnlineRegions(java.util.List<org.apache.hadoop.hbase.HRegionInfo>)"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData.getOnlineRegions()", "public java.util.List<org.apache.hadoop.hbase.HRegionInfo> getOnlineRegions()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl()", "public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData getImplData()"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$1.renderTo(java.io.Writer)", "public void renderTo(java.io.Writer) throws java.io.IOException"], ["org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData()"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData.setMWrap(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)", "public void setMWrap(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)"], ["org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData.getMWrap()", "public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper getMWrap()"], ["org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl(org.jamon.TemplateManager)", "public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl(org.jamon.TemplateManager)"], ["org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl()", "public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl()"], ["org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.getImplData()", "public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData getImplData()"], ["org.jamon.AbstractTemplateImpl", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)", "public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>)"], ["org.jamon.Renderer", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.makeRenderer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)", "public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.render(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)", "public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper)", "public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper) throws java.io.IOException"], ["org.jamon.AbstractTemplateProxy$ImplData", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.getImplData()", "public org.jamon.AbstractTemplateProxy$ImplData getImplData()"], ["org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData)", "public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData)"], ["void", "org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl.renderNoFlush(java.io.Writer)", "public void renderNoFlush(java.io.Writer) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.tool.Canary$Monitor.isDone()", "public boolean isDone()"], ["boolean", "org.apache.hadoop.hbase.tool.Canary$Monitor.hasError()", "public boolean hasError()"], ["void", "org.apache.hadoop.hbase.tool.Canary$Monitor.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.tool.Canary$RegionMonitor", "org.apache.hadoop.hbase.tool.Canary$RegionMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.Canary$Sink)", "public org.apache.hadoop.hbase.tool.Canary$RegionMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.Canary$Sink)"], ["void", "org.apache.hadoop.hbase.tool.Canary$RegionMonitor.run()", "public void run()"], ["org.apache.hadoop.hbase.tool.Canary$RegionServerMonitor", "org.apache.hadoop.hbase.tool.Canary$RegionServerMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.Canary$ExtendedSink)", "public org.apache.hadoop.hbase.tool.Canary$RegionServerMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.Canary$ExtendedSink)"], ["void", "org.apache.hadoop.hbase.tool.Canary$RegionServerMonitor.run()", "public void run()"], ["org.apache.hadoop.hbase.tool.Canary$RegionServerStdOutSink", "org.apache.hadoop.hbase.tool.Canary$RegionServerStdOutSink()", "public org.apache.hadoop.hbase.tool.Canary$RegionServerStdOutSink()"], ["void", "org.apache.hadoop.hbase.tool.Canary$RegionServerStdOutSink.publishReadFailure(java.lang.String, java.lang.String)", "public void publishReadFailure(java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.tool.Canary$RegionServerStdOutSink.publishReadTiming(java.lang.String, java.lang.String, long)", "public void publishReadTiming(java.lang.String, java.lang.String, long)"], ["org.apache.hadoop.hbase.tool.Canary$StdOutSink", "org.apache.hadoop.hbase.tool.Canary$StdOutSink()", "public org.apache.hadoop.hbase.tool.Canary$StdOutSink()"], ["void", "org.apache.hadoop.hbase.tool.Canary$StdOutSink.publishReadFailure(org.apache.hadoop.hbase.HRegionInfo, java.lang.Exception)", "public void publishReadFailure(org.apache.hadoop.hbase.HRegionInfo, java.lang.Exception)"], ["void", "org.apache.hadoop.hbase.tool.Canary$StdOutSink.publishReadFailure(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HColumnDescriptor, java.lang.Exception)", "public void publishReadFailure(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HColumnDescriptor, java.lang.Exception)"], ["void", "org.apache.hadoop.hbase.tool.Canary$StdOutSink.publishReadTiming(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HColumnDescriptor, long)", "public void publishReadTiming(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HColumnDescriptor, long)"], ["org.apache.hadoop.hbase.tool.Canary", "org.apache.hadoop.hbase.tool.Canary()", "public org.apache.hadoop.hbase.tool.Canary()"], ["org.apache.hadoop.hbase.tool.Canary", "org.apache.hadoop.hbase.tool.Canary(org.apache.hadoop.hbase.tool.Canary$Sink)", "public org.apache.hadoop.hbase.tool.Canary(org.apache.hadoop.hbase.tool.Canary$Sink)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.tool.Canary.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.tool.Canary.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.tool.Canary.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.tool.Canary$Monitor", "org.apache.hadoop.hbase.tool.Canary.newMonitor(org.apache.hadoop.hbase.client.Connection, int, java.lang.String[])", "public org.apache.hadoop.hbase.tool.Canary$Monitor newMonitor(org.apache.hadoop.hbase.client.Connection, int, java.lang.String[])"], ["void", "org.apache.hadoop.hbase.tool.Canary.sniff(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.TableName)", "public static void sniff(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.TableName) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.tool.Canary.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.tool.WriteSinkCoprocessor", "org.apache.hadoop.hbase.tool.WriteSinkCoprocessor()", "public org.apache.hadoop.hbase.tool.WriteSinkCoprocessor()"], ["void", "org.apache.hadoop.hbase.tool.WriteSinkCoprocessor.preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>)", "public void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.tool.WriteSinkCoprocessor.preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>)", "public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.AbstractHBaseTool", "org.apache.hadoop.hbase.util.AbstractHBaseTool()", "public org.apache.hadoop.hbase.util.AbstractHBaseTool()"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.util.AbstractHBaseTool.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["void", "org.apache.hadoop.hbase.util.AbstractHBaseTool.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.util.AbstractHBaseTool.run(java.lang.String[])", "public final int run(java.lang.String[]) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.util.AbstractHBaseTool.parseLong(java.lang.String, long, long)", "public static long parseLong(java.lang.String, long, long)"], ["int", "org.apache.hadoop.hbase.util.AbstractHBaseTool.parseInt(java.lang.String, int, int)", "public static int parseInt(java.lang.String, int, int)"], ["org.apache.hadoop.hbase.util.BloomFilter", "org.apache.hadoop.hbase.util.BloomFilterFactory.createFromMeta(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader)", "public static org.apache.hadoop.hbase.util.BloomFilter createFromMeta(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader) throws java.lang.IllegalArgumentException, java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.BloomFilterFactory.isGeneralBloomEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isGeneralBloomEnabled(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.util.BloomFilterFactory.isDeleteFamilyBloomEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isDeleteFamilyBloomEnabled(org.apache.hadoop.conf.Configuration)"], ["float", "org.apache.hadoop.hbase.util.BloomFilterFactory.getErrorRate(org.apache.hadoop.conf.Configuration)", "public static float getErrorRate(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.util.BloomFilterFactory.getMaxFold(org.apache.hadoop.conf.Configuration)", "public static int getMaxFold(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.util.BloomFilterFactory.getBloomBlockSize(org.apache.hadoop.conf.Configuration)", "public static int getBloomBlockSize(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.util.BloomFilterFactory.getMaxKeys(org.apache.hadoop.conf.Configuration)", "public static int getMaxKeys(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.util.BloomFilterWriter", "org.apache.hadoop.hbase.util.BloomFilterFactory.createGeneralBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer)", "public static org.apache.hadoop.hbase.util.BloomFilterWriter createGeneralBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer)"], ["org.apache.hadoop.hbase.util.BloomFilterWriter", "org.apache.hadoop.hbase.util.BloomFilterFactory.createDeleteBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer)", "public static org.apache.hadoop.hbase.util.BloomFilterWriter createDeleteBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer)"], ["org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue()", "public org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue()"], ["org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue(long)", "public org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue(long)"], ["boolean", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.addAll(java.util.Collection<? extends T>)", "public boolean addAll(java.util.Collection<? extends T>)"], ["void", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.clear()", "public void clear()"], ["boolean", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.offer(T)", "public boolean offer(T)"], ["T", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.poll()", "public T poll()"], ["boolean", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.remove(java.lang.Object)", "public boolean remove(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.size()", "public int size()"], ["void", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.drainTo(java.util.Collection<T>)", "public void drainTo(java.util.Collection<T>)"], ["long", "org.apache.hadoop.hbase.util.BoundedConcurrentLinkedQueue.remainingCapacity()", "public long remainingCapacity()"], ["org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue(int, java.util.Comparator<? super E>)", "public org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue(int, java.util.Comparator<? super E>)"], ["void", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.add(E)", "public void add(E)"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.peek()", "public E peek()"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.poll()", "public E poll()"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.size()", "public int size()"], ["java.util.Comparator<? super E>", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.comparator()", "public java.util.Comparator<? super E> comparator()"], ["boolean", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.contains(java.lang.Object)", "public boolean contains(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue.remainingCapacity()", "public int remainingCapacity()"], ["org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue(int, java.util.Comparator<? super E>)", "public org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue(int, java.util.Comparator<? super E>)"], ["boolean", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.offer(E)", "public boolean offer(E)"], ["void", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.put(E)", "public void put(E) throws java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.offer(E, long, java.util.concurrent.TimeUnit)", "public boolean offer(E, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.take()", "public E take() throws java.lang.InterruptedException"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.poll()", "public E poll()"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.poll(long, java.util.concurrent.TimeUnit)", "public E poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException"], ["E", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.peek()", "public E peek()"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.size()", "public int size()"], ["java.util.Iterator<E>", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.iterator()", "public java.util.Iterator<E> iterator()"], ["java.util.Comparator<? super E>", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.comparator()", "public java.util.Comparator<? super E> comparator()"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.remainingCapacity()", "public int remainingCapacity()"], ["boolean", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.remove(java.lang.Object)", "public boolean remove(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.contains(java.lang.Object)", "public boolean contains(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.drainTo(java.util.Collection<? super E>)", "public int drainTo(java.util.Collection<? super E>)"], ["int", "org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue.drainTo(java.util.Collection<? super E>, int)", "public int drainTo(java.util.Collection<? super E>, int)"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter$DataWriter.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter$DataWriter.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter$MetaWriter.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter$MetaWriter.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.ByteBloomFilter", "org.apache.hadoop.hbase.util.ByteBloomFilter(java.io.DataInput)", "public org.apache.hadoop.hbase.util.ByteBloomFilter(java.io.DataInput) throws java.io.IOException, java.lang.IllegalArgumentException"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.computeBitSize(long, double)", "public static long computeBitSize(long, double)"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.idealMaxKeys(long, double)", "public static long idealMaxKeys(long, double)"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.computeMaxKeys(long, double, int)", "public static long computeMaxKeys(long, double, int)"], ["double", "org.apache.hadoop.hbase.util.ByteBloomFilter.actualErrorRate()", "public double actualErrorRate()"], ["double", "org.apache.hadoop.hbase.util.ByteBloomFilter.actualErrorRate(long, long, int)", "public static double actualErrorRate(long, long, int)"], ["int", "org.apache.hadoop.hbase.util.ByteBloomFilter.computeFoldableByteSize(long, int)", "public static int computeFoldableByteSize(long, int)"], ["org.apache.hadoop.hbase.util.ByteBloomFilter", "org.apache.hadoop.hbase.util.ByteBloomFilter(int, double, int, int)", "public org.apache.hadoop.hbase.util.ByteBloomFilter(int, double, int, int) throws java.lang.IllegalArgumentException"], ["org.apache.hadoop.hbase.util.ByteBloomFilter", "org.apache.hadoop.hbase.util.ByteBloomFilter.createBySize(int, double, int, int)", "public static org.apache.hadoop.hbase.util.ByteBloomFilter createBySize(int, double, int, int)"], ["org.apache.hadoop.hbase.util.ByteBloomFilter", "org.apache.hadoop.hbase.util.ByteBloomFilter.createAnother()", "public org.apache.hadoop.hbase.util.ByteBloomFilter createAnother()"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.allocBloom()", "public void allocBloom()"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.add(byte[])", "public void add(byte[])"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.add(byte[], int, int)", "public void add(byte[], int, int)"], ["boolean", "org.apache.hadoop.hbase.util.ByteBloomFilter.contains(byte[], int, int, java.nio.ByteBuffer)", "public boolean contains(byte[], int, int, java.nio.ByteBuffer)"], ["boolean", "org.apache.hadoop.hbase.util.ByteBloomFilter.contains(byte[], int, int, java.nio.ByteBuffer, int, int, org.apache.hadoop.hbase.util.Hash, int)", "public static boolean contains(byte[], int, int, java.nio.ByteBuffer, int, int, org.apache.hadoop.hbase.util.Hash, int)"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.getKeyCount()", "public long getKeyCount()"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.getMaxKeys()", "public long getMaxKeys()"], ["long", "org.apache.hadoop.hbase.util.ByteBloomFilter.getByteSize()", "public long getByteSize()"], ["int", "org.apache.hadoop.hbase.util.ByteBloomFilter.getHashType()", "public int getHashType()"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.compactBloom()", "public void compactBloom()"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.writeBloom(java.io.DataOutput)", "public void writeBloom(java.io.DataOutput) throws java.io.IOException"], ["org.apache.hadoop.io.Writable", "org.apache.hadoop.hbase.util.ByteBloomFilter.getMetaWriter()", "public org.apache.hadoop.io.Writable getMetaWriter()"], ["org.apache.hadoop.io.Writable", "org.apache.hadoop.hbase.util.ByteBloomFilter.getDataWriter()", "public org.apache.hadoop.io.Writable getDataWriter()"], ["int", "org.apache.hadoop.hbase.util.ByteBloomFilter.getHashCount()", "public int getHashCount()"], ["boolean", "org.apache.hadoop.hbase.util.ByteBloomFilter.supportsAutoLoading()", "public boolean supportsAutoLoading()"], ["void", "org.apache.hadoop.hbase.util.ByteBloomFilter.setFakeLookupMode(boolean)", "public static void setFakeLookupMode(boolean)"], ["byte[]", "org.apache.hadoop.hbase.util.ByteBloomFilter.createBloomKey(byte[], int, int, byte[], int, int)", "public byte[] createBloomKey(byte[], int, int, byte[], int, int)"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.util.ByteBloomFilter.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["java.lang.String", "org.apache.hadoop.hbase.util.ByteBloomFilter.formatStats(org.apache.hadoop.hbase.util.BloomFilterBase)", "public static java.lang.String formatStats(org.apache.hadoop.hbase.util.BloomFilterBase)"], ["java.lang.String", "org.apache.hadoop.hbase.util.ByteBloomFilter.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.CollectionBackedScanner", "org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>)", "public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>)"], ["org.apache.hadoop.hbase.util.CollectionBackedScanner", "org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.util.CollectionBackedScanner", "org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>)", "public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>)"], ["org.apache.hadoop.hbase.util.CollectionBackedScanner", "org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["org.apache.hadoop.hbase.util.CollectionBackedScanner", "org.apache.hadoop.hbase.util.CollectionBackedScanner(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell...)", "public org.apache.hadoop.hbase.util.CollectionBackedScanner(org.apache.hadoop.hbase.KeyValue$KVComparator, org.apache.hadoop.hbase.Cell...)"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.util.CollectionBackedScanner.peek()", "public org.apache.hadoop.hbase.Cell peek()"], ["org.apache.hadoop.hbase.Cell", "org.apache.hadoop.hbase.util.CollectionBackedScanner.next()", "public org.apache.hadoop.hbase.Cell next()"], ["boolean", "org.apache.hadoop.hbase.util.CollectionBackedScanner.seek(org.apache.hadoop.hbase.Cell)", "public boolean seek(org.apache.hadoop.hbase.Cell)"], ["boolean", "org.apache.hadoop.hbase.util.CollectionBackedScanner.reseek(org.apache.hadoop.hbase.Cell)", "public boolean reseek(org.apache.hadoop.hbase.Cell)"], ["long", "org.apache.hadoop.hbase.util.CollectionBackedScanner.getSequenceID()", "public long getSequenceID()"], ["void", "org.apache.hadoop.hbase.util.CollectionBackedScanner.close()", "public void close()"], ["org.apache.hadoop.hbase.util.CompoundBloomFilter", "org.apache.hadoop.hbase.util.CompoundBloomFilter(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader)", "public org.apache.hadoop.hbase.util.CompoundBloomFilter(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.CompoundBloomFilter.contains(byte[], int, int, java.nio.ByteBuffer)", "public boolean contains(byte[], int, int, java.nio.ByteBuffer)"], ["boolean", "org.apache.hadoop.hbase.util.CompoundBloomFilter.supportsAutoLoading()", "public boolean supportsAutoLoading()"], ["int", "org.apache.hadoop.hbase.util.CompoundBloomFilter.getNumChunks()", "public int getNumChunks()"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.util.CompoundBloomFilter.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilter.enableTestingStats()", "public void enableTestingStats()"], ["java.lang.String", "org.apache.hadoop.hbase.util.CompoundBloomFilter.formatTestingStats()", "public java.lang.String formatTestingStats()"], ["long", "org.apache.hadoop.hbase.util.CompoundBloomFilter.getNumQueriesForTesting(int)", "public long getNumQueriesForTesting(int)"], ["long", "org.apache.hadoop.hbase.util.CompoundBloomFilter.getNumPositivesForTesting(int)", "public long getNumPositivesForTesting(int)"], ["java.lang.String", "org.apache.hadoop.hbase.util.CompoundBloomFilter.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.CompoundBloomFilterBase", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase()", "public org.apache.hadoop.hbase.util.CompoundBloomFilterBase()"], ["long", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase.getMaxKeys()", "public long getMaxKeys()"], ["long", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase.getKeyCount()", "public long getKeyCount()"], ["long", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase.getByteSize()", "public long getByteSize()"], ["byte[]", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase.createBloomKey(byte[], int, int, byte[], int, int)", "public byte[] createBloomKey(byte[], int, int, byte[], int, int)"], ["org.apache.hadoop.hbase.KeyValue$KVComparator", "org.apache.hadoop.hbase.util.CompoundBloomFilterBase.getComparator()", "public org.apache.hadoop.hbase.KeyValue$KVComparator getComparator()"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter$MetaWriter.readFields(java.io.DataInput)", "public void readFields(java.io.DataInput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter$MetaWriter.write(java.io.DataOutput)", "public void write(java.io.DataOutput) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.CompoundBloomFilterWriter", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter(int, float, int, int, boolean, org.apache.hadoop.hbase.KeyValue$KVComparator)", "public org.apache.hadoop.hbase.util.CompoundBloomFilterWriter(int, float, int, int, boolean, org.apache.hadoop.hbase.KeyValue$KVComparator)"], ["boolean", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.shouldWriteBlock(boolean)", "public boolean shouldWriteBlock(boolean)"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.add(byte[], int, int)", "public void add(byte[], int, int)"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.writeInlineBlock(java.io.DataOutput)", "public void writeInlineBlock(java.io.DataOutput) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.blockWritten(long, int, int)", "public void blockWritten(long, int, int)"], ["org.apache.hadoop.hbase.io.hfile.BlockType", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.getInlineBlockType()", "public org.apache.hadoop.hbase.io.hfile.BlockType getInlineBlockType()"], ["org.apache.hadoop.io.Writable", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.getMetaWriter()", "public org.apache.hadoop.io.Writable getMetaWriter()"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.compactBloom()", "public void compactBloom()"], ["void", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.allocBloom()", "public void allocBloom()"], ["org.apache.hadoop.io.Writable", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.getDataWriter()", "public org.apache.hadoop.io.Writable getDataWriter()"], ["boolean", "org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.getCacheOnWrite()", "public boolean getCacheOnWrite()"], ["org.apache.hadoop.hbase.util.CompressionTest", "org.apache.hadoop.hbase.util.CompressionTest()", "public org.apache.hadoop.hbase.util.CompressionTest()"], ["boolean", "org.apache.hadoop.hbase.util.CompressionTest.testCompression(java.lang.String)", "public static boolean testCompression(java.lang.String)"], ["void", "org.apache.hadoop.hbase.util.CompressionTest.testCompression(org.apache.hadoop.hbase.io.compress.Compression$Algorithm)", "public static void testCompression(org.apache.hadoop.hbase.io.compress.Compression$Algorithm) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.CompressionTest.usage()", "public static void usage()"], ["void", "org.apache.hadoop.hbase.util.CompressionTest.doSmokeTest(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String)", "public static void doSmokeTest(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.util.CompressionTest.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.ConfigUtil", "org.apache.hadoop.hbase.util.ConfigUtil()", "public org.apache.hadoop.hbase.util.ConfigUtil()"], ["boolean", "org.apache.hadoop.hbase.util.ConfigUtil.useZKForAssignment(org.apache.hadoop.conf.Configuration)", "public static boolean useZKForAssignment(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.util.ConnectionCache$2.stop(java.lang.String)", "public void stop(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.ConnectionCache$2.isStopped()", "public boolean isStopped()"], ["org.apache.hadoop.hbase.util.ConnectionCache", "org.apache.hadoop.hbase.util.ConnectionCache(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.security.UserProvider, int, int)", "public org.apache.hadoop.hbase.util.ConnectionCache(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.security.UserProvider, int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ConnectionCache.setEffectiveUser(java.lang.String)", "public void setEffectiveUser(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.util.ConnectionCache.getEffectiveUser()", "public java.lang.String getEffectiveUser()"], ["void", "org.apache.hadoop.hbase.util.ConnectionCache.shutdown()", "public void shutdown()"], ["org.apache.hadoop.hbase.client.Admin", "org.apache.hadoop.hbase.util.ConnectionCache.getAdmin()", "public org.apache.hadoop.hbase.client.Admin getAdmin() throws java.io.IOException"], ["org.apache.hadoop.hbase.client.Table", "org.apache.hadoop.hbase.util.ConnectionCache.getTable(java.lang.String)", "public org.apache.hadoop.hbase.client.Table getTable(java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.client.RegionLocator", "org.apache.hadoop.hbase.util.ConnectionCache.getRegionLocator(byte[])", "public org.apache.hadoop.hbase.client.RegionLocator getRegionLocator(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.DirectMemoryUtils", "org.apache.hadoop.hbase.util.DirectMemoryUtils()", "public org.apache.hadoop.hbase.util.DirectMemoryUtils()"], ["long", "org.apache.hadoop.hbase.util.DirectMemoryUtils.getDirectMemorySize()", "public static long getDirectMemorySize()"], ["long", "org.apache.hadoop.hbase.util.DirectMemoryUtils.getDirectMemoryUsage()", "public static long getDirectMemoryUsage()"], ["void", "org.apache.hadoop.hbase.util.DirectMemoryUtils.destroyDirectByteBuffer(java.nio.ByteBuffer)", "public static void destroyDirectByteBuffer(java.nio.ByteBuffer) throws java.lang.IllegalArgumentException, java.lang.IllegalAccessException, java.lang.reflect.InvocationTargetException, java.lang.SecurityException, java.lang.NoSuchMethodException"], ["void", "org.apache.hadoop.hbase.util.EncryptionTest.testKeyProvider(org.apache.hadoop.conf.Configuration)", "public static void testKeyProvider(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.EncryptionTest.testCipherProvider(org.apache.hadoop.conf.Configuration)", "public static void testCipherProvider(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.EncryptionTest.testEncryption(org.apache.hadoop.conf.Configuration, java.lang.String, byte[])", "public static void testEncryption(org.apache.hadoop.conf.Configuration, java.lang.String, byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.FSHDFSUtils", "org.apache.hadoop.hbase.util.FSHDFSUtils()", "public org.apache.hadoop.hbase.util.FSHDFSUtils()"], ["boolean", "org.apache.hadoop.hbase.util.FSHDFSUtils.isSameHdfs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem)", "public static boolean isSameHdfs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem)"], ["void", "org.apache.hadoop.hbase.util.FSHDFSUtils.recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable)", "public void recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.FSMapRUtils", "org.apache.hadoop.hbase.util.FSMapRUtils()", "public org.apache.hadoop.hbase.util.FSMapRUtils()"], ["void", "org.apache.hadoop.hbase.util.FSMapRUtils.recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable)", "public void recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSRegionScanner.run()", "public void run()"], ["org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir", "org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir()", "public org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir()"], ["void", "org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir.migrateFSTableDescriptorsIfNecessary(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static void migrateFSTableDescriptorsIfNecessary(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.FSTableDescriptors$1.compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus)", "public int compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus)"], ["int", "org.apache.hadoop.hbase.util.FSTableDescriptors$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors$2.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSTableDescriptors", "org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.FSTableDescriptors", "org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.FSTableDescriptors", "org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean)", "public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSTableDescriptors.setCacheOn()", "public void setCacheOn() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSTableDescriptors.setCacheOff()", "public void setCacheOff() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors.isUsecache()", "public boolean isUsecache()"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.get(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.HTableDescriptor get(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.HTableDescriptor>", "org.apache.hadoop.hbase.util.FSTableDescriptors.getAll()", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.HTableDescriptor> getAll() throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.hbase.HTableDescriptor>", "org.apache.hadoop.hbase.util.FSTableDescriptors.getByNamespace(java.lang.String)", "public java.util.Map<java.lang.String, org.apache.hadoop.hbase.HTableDescriptor> getByNamespace(java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSTableDescriptors.add(org.apache.hadoop.hbase.HTableDescriptor)", "public void add(org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.remove(org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.HTableDescriptor remove(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors.isTableInfoExists(org.apache.hadoop.hbase.TableName)", "public boolean isTableInfoExists(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus", "org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.FileStatus getTableInfoPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)", "public static org.apache.hadoop.hbase.HTableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, boolean)", "public static org.apache.hadoop.hbase.HTableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.HTableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.HTableDescriptor", "org.apache.hadoop.hbase.util.FSTableDescriptors.getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean)", "public static org.apache.hadoop.hbase.HTableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSTableDescriptors.deleteTableDescriptorIfExists(org.apache.hadoop.hbase.TableName)", "public void deleteTableDescriptorIfExists(org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor)", "public boolean createTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor, boolean)", "public boolean createTableDescriptor(org.apache.hadoop.hbase.HTableDescriptor, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptorForTableDirectory(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, boolean)", "public boolean createTableDescriptorForTableDirectory(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter", "org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>)", "public org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$DirFilter", "org.apache.hadoop.hbase.util.FSUtils$DirFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$DirFilter(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter", "org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$FileFilter", "org.apache.hadoop.hbase.util.FSUtils$FileFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$FileFilter(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$FileFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$HFileFilter", "org.apache.hadoop.hbase.util.FSUtils$HFileFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$HFileFilter(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$HFileFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter", "org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter", "org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter(org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils$UserTableDirFilter", "org.apache.hadoop.hbase.util.FSUtils$UserTableDirFilter(org.apache.hadoop.fs.FileSystem)", "public org.apache.hadoop.hbase.util.FSUtils$UserTableDirFilter(org.apache.hadoop.fs.FileSystem)"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setStoragePolicy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)", "public static void setStoragePolicy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isStartingWithPath(org.apache.hadoop.fs.Path, java.lang.String)", "public static boolean isStartingWithPath(org.apache.hadoop.fs.Path, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isMatchingTail(org.apache.hadoop.fs.Path, java.lang.String)", "public static boolean isMatchingTail(org.apache.hadoop.fs.Path, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isMatchingTail(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static boolean isMatchingTail(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.util.FSUtils", "org.apache.hadoop.hbase.util.FSUtils.getInstance(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.util.FSUtils getInstance(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.deleteDirectory(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean deleteDirectory(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.deleteRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo)", "public static boolean deleteRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.util.FSUtils.getDefaultBlockSize(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static long getDefaultBlockSize(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["short", "org.apache.hadoop.hbase.util.FSUtils.getDefaultReplication(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static short getDefaultReplication(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.FSUtils.getDefaultBufferSize(org.apache.hadoop.fs.FileSystem)", "public static int getDefaultBufferSize(org.apache.hadoop.fs.FileSystem)"], ["org.apache.hadoop.fs.FSDataOutputStream", "org.apache.hadoop.hbase.util.FSUtils.create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.net.InetSocketAddress[])", "public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.net.InetSocketAddress[]) throws java.io.IOException"], ["org.apache.hadoop.fs.FSDataOutputStream", "org.apache.hadoop.hbase.util.FSUtils.create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)", "public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException"], ["org.apache.hadoop.fs.permission.FsPermission", "org.apache.hadoop.hbase.util.FSUtils.getFilePermissions(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, java.lang.String)", "public static org.apache.hadoop.fs.permission.FsPermission getFilePermissions(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, java.lang.String)"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkFileSystemAvailable(org.apache.hadoop.fs.FileSystem)", "public static void checkFileSystemAvailable(org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkDfsSafeMode(org.apache.hadoop.conf.Configuration)", "public static void checkDfsSafeMode(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.util.FSUtils.getVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.lang.String getVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean)", "public static void checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, int, int)", "public static void checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, int, int) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, int, int)", "public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, int, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, int, int)", "public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, int, int) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.checkClusterIdExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, int)", "public static boolean checkClusterIdExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.ClusterId", "org.apache.hadoop.hbase.util.FSUtils.getClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.ClusterId getClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.ClusterId, int)", "public static void setClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.ClusterId, int) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.FSUtils.validateRootPath(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path validateRootPath(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.util.FSUtils.removeRootPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)", "public static java.lang.String removeRootPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.waitOnSafeMode(org.apache.hadoop.conf.Configuration, long)", "public static void waitOnSafeMode(org.apache.hadoop.conf.Configuration, long) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.util.FSUtils.getPath(org.apache.hadoop.fs.Path)", "public static java.lang.String getPath(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.FSUtils.getRootDir(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.fs.Path getRootDir(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setRootDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public static void setRootDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setFsDefault(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public static void setFsDefault(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.metaRegionExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean metaRegionExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.HDFSBlocksDistribution", "org.apache.hadoop.hbase.util.FSUtils.computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, long, long)", "public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isMajorCompacted(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean isMajorCompacted(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.FSUtils.getTotalTableFragmentation(org.apache.hadoop.hbase.master.HMaster)", "public static int getTotalTableFragmentation(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["java.util.Map<java.lang.String, java.lang.Integer>", "org.apache.hadoop.hbase.util.FSUtils.getTableFragmentation(org.apache.hadoop.hbase.master.HMaster)", "public static java.util.Map<java.lang.String, java.lang.Integer> getTableFragmentation(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException"], ["java.util.Map<java.lang.String, java.lang.Integer>", "org.apache.hadoop.hbase.util.FSUtils.getTableFragmentation(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.Map<java.lang.String, java.lang.Integer> getTableFragmentation(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.FSUtils.getTableDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)", "public static org.apache.hadoop.fs.Path getTableDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.util.FSUtils.getTableName(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.TableName getTableName(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.FSUtils.getNamespaceDir(org.apache.hadoop.fs.Path, java.lang.String)", "public static org.apache.hadoop.fs.Path getNamespaceDir(org.apache.hadoop.fs.Path, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isAppendSupported(org.apache.hadoop.conf.Configuration)", "public static boolean isAppendSupported(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isHDFS(org.apache.hadoop.conf.Configuration)", "public static boolean isHDFS(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.List<org.apache.hadoop.fs.Path> getTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getLocalTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.List<org.apache.hadoop.fs.Path> getLocalTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isRecoveredEdits(org.apache.hadoop.fs.Path)", "public static boolean isRecoveredEdits(org.apache.hadoop.fs.Path)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getRegionDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.List<org.apache.hadoop.fs.Path> getRegionDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getFamilyDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.List<org.apache.hadoop.fs.Path> getFamilyDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getReferenceFilePaths(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.List<org.apache.hadoop.fs.Path> getReferenceFilePaths(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.fs.FileSystem", "org.apache.hadoop.hbase.util.FSUtils.getCurrentFileSystem(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.fs.FileSystem getCurrentFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)", "public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)", "public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.FSUtils.getRegionReferenceFileCount(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static int getRegionReferenceFileCount(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)"], ["java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.FSUtils.getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)", "public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus[]", "org.apache.hadoop.hbase.util.FSUtils.listStatus(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)", "public static org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException"], ["org.apache.hadoop.fs.FileStatus[]", "org.apache.hadoop.hbase.util.FSUtils.listStatus(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.delete(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean)", "public static boolean delete(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.isExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static boolean isExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkAccess(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction)", "public static void checkAccess(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.hbase.security.AccessDeniedException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.logFileSystemState(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.commons.logging.Log)", "public static void logFileSystemState(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.commons.logging.Log) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)", "public static boolean renameAndSetModifyTime(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>", "org.apache.hadoop.hbase.util.FSUtils.getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration)", "public static java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>> getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>", "org.apache.hadoop.hbase.util.FSUtils.getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration, java.lang.String, int)", "public static java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>> getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration, java.lang.String, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSUtils.setupShortCircuitRead(org.apache.hadoop.conf.Configuration)", "public static void setupShortCircuitRead(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.util.FSUtils.checkShortCircuitReadBufferSize(org.apache.hadoop.conf.Configuration)", "public static void checkShortCircuitReadBufferSize(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitRegions(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RegionVisitor)", "public static void visitRegions(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RegionVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitTableStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor)", "public static void visitTableStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitRegionStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor)", "public static void visitRegionStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitTableRecoveredEdits(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RecoveredEditsVisitor)", "public static void visitTableRecoveredEdits(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RecoveredEditsVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitRegionRecoveredEdits(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RecoveredEditsVisitor)", "public static void visitRegionRecoveredEdits(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$RecoveredEditsVisitor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.FSVisitor.visitLogFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$LogFileVisitor)", "public static void visitLogFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$LogFileVisitor) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.GetJavaProperty", "org.apache.hadoop.hbase.util.GetJavaProperty()", "public org.apache.hadoop.hbase.util.GetJavaProperty()"], ["void", "org.apache.hadoop.hbase.util.GetJavaProperty.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["org.apache.hadoop.hbase.util.HBaseConfTool", "org.apache.hadoop.hbase.util.HBaseConfTool()", "public org.apache.hadoop.hbase.util.HBaseConfTool()"], ["void", "org.apache.hadoop.hbase.util.HBaseConfTool.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$1.run()", "public void run()"], ["java.lang.Void", "org.apache.hadoop.hbase.util.HBaseFsck$2.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Void connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$2.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Object connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$3.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$3.isAborted()", "public boolean isAborted()"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$4$1.compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)", "public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$4$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$4.processRow(org.apache.hadoop.hbase.client.Result)", "public boolean processRow(org.apache.hadoop.hbase.client.Result) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$5.compare(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public int compare(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$5.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$CheckRegionConsistencyWorkItem.java.lang.Void call()", "public synchronized java.lang.Void call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$CheckRegionConsistencyWorkItem.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE[]", "org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE.values()", "public static org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE[] values()"], ["org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE", "org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable", "org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable(org.apache.hadoop.hbase.util.HBaseFsck, org.apache.hadoop.hbase.util.RetryCounter)", "public org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable(org.apache.hadoop.hbase.util.HBaseFsck, org.apache.hadoop.hbase.util.RetryCounter)"], ["org.apache.hadoop.fs.FSDataOutputStream", "org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable.call()", "public org.apache.hadoop.fs.FSDataOutputStream call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable.call()", "public java.lang.Object call() throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$HBaseFsckTool.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getReplicaId()", "public int getReplicaId()"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.void addServer(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)", "public synchronized void addServer(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.java.lang.String toString()", "public synchronized java.lang.String toString()"], ["byte[]", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getStartKey()", "public byte[] getStartKey()"], ["byte[]", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getEndKey()", "public byte[] getEndKey()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["java.lang.String", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getRegionNameAsString()", "public java.lang.String getRegionNameAsString()"], ["byte[]", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getRegionName()", "public byte[] getRegionName()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.getPrimaryHRIForDeployedReplica()", "public org.apache.hadoop.hbase.HRegionInfo getPrimaryHRIForDeployedReplica()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.setSkipChecks(boolean)", "public void setSkipChecks(boolean)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.isSkipChecks()", "public boolean isSkipChecks()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.setMerged(boolean)", "public void setMerged(boolean)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo.isMerged()", "public boolean isMerged()"], ["org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry", "org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long)", "public org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long)"], ["org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry", "org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)", "public org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry(org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.ServerName, long, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$MetaEntry.hashCode()", "public int hashCode()"], ["java.lang.String", "org.apache.hadoop.hbase.util.HBaseFsck$OnlineEntry.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.clear()", "public void clear()"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String)", "public synchronized void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)", "public synchronized void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public synchronized void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public synchronized void reportError(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HBaseFsck$TableInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void reportError(java.lang.String)", "public synchronized void reportError(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void report(java.lang.String)", "public synchronized void report(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.int summarize()", "public synchronized int summarize()"], ["java.util.ArrayList<org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE>", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.getErrorList()", "public java.util.ArrayList<org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter$ERROR_CODE> getErrorList()"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void print(java.lang.String)", "public synchronized void print(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.tableHasErrors(org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)", "public boolean tableHasErrors(org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.resetErrors()", "public void resetErrors()"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void detail(java.lang.String)", "public synchronized void detail(java.lang.String)"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter.void progress()", "public synchronized void progress()"], ["java.lang.String", "org.apache.hadoop.hbase.util.HBaseFsck$RegionBoundariesInformation.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.HBaseFsck$RegionRepairException", "org.apache.hadoop.hbase.util.HBaseFsck$RegionRepairException(java.lang.String, java.io.IOException)", "public org.apache.hadoop.hbase.util.HBaseFsck$RegionRepairException(java.lang.String, java.io.IOException)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$HDFSIntegrityFixer.handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$HDFSIntegrityFixer.handleRegionEndKeyNotEmpty(byte[])", "public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$HDFSIntegrityFixer.handleHoleInRegionChain(byte[], byte[])", "public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$HDFSIntegrityFixer.handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)", "public void handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleRegionEndKeyNotEmpty(byte[])", "public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleDegenerateRegion(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleDegenerateRegion(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo$IntegrityFixSuggester.handleHoleInRegionChain(byte[], byte[])", "public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.addRegionInfo(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void addRegionInfo(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.addServer(org.apache.hadoop.hbase.ServerName)", "public void addServer(org.apache.hadoop.hbase.ServerName)"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.getName()", "public org.apache.hadoop.hbase.TableName getName()"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.getNumRegions()", "public int getNumRegions()"], ["com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.getRegionsFromMeta()", "public synchronized com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.HRegionInfo> getRegionsFromMeta()"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.checkRegionChain(org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler)", "public boolean checkRegionChain(org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsDir.java.lang.Void call()", "public synchronized java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsDir.call()", "public java.lang.Object call() throws java.lang.Exception"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsRegionInfo.java.lang.Void call()", "public synchronized java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsRegionInfo.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemOverlapMerge.call()", "public java.lang.Void call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemOverlapMerge.call()", "public java.lang.Object call() throws java.lang.Exception"], ["synchronized", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemRegion.java.lang.Void call()", "public synchronized java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HBaseFsck$WorkItemRegion.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.HBaseFsck", "org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.MasterNotRunningException, org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException, java.lang.ClassNotFoundException"], ["org.apache.hadoop.hbase.util.HBaseFsck", "org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService)", "public org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService) throws org.apache.hadoop.hbase.MasterNotRunningException, org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException, java.lang.ClassNotFoundException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.connect()", "public void connect() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.offlineHdfsIntegrityRepair()", "public void offlineHdfsIntegrityRepair() throws java.io.IOException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair()", "public int onlineConsistencyRepair() throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck()", "public int onlineHbck() throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, com.google.protobuf.ServiceException"], ["byte[]", "org.apache.hadoop.hbase.util.HBaseFsck.keyOnly(byte[])", "public static byte[] keyOnly(byte[])"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.checkRegionBoundaries()", "public void checkRegionBoundaries()"], ["org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter", "org.apache.hadoop.hbase.util.HBaseFsck.getErrors()", "public org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter getErrors()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.fixEmptyMetaCells()", "public void fixEmptyMetaCells() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.fixOrphanTables()", "public void fixOrphanTables() throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck.rebuildMeta(boolean)", "public boolean rebuildMeta(boolean) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegionDirs()", "public void loadHdfsRegionDirs() throws java.io.IOException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.mergeRegionDirs(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public int mergeRegionDirs(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.dumpOverlapProblems(com.google.common.collect.Multimap<byte[], org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)", "public void dumpOverlapProblems(com.google.common.collect.Multimap<byte[], org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.dumpSidelinedRegions(java.util.Map<org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)", "public void dumpSidelinedRegions(java.util.Map<org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)"], ["com.google.common.collect.Multimap<byte[], org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>", "org.apache.hadoop.hbase.util.HBaseFsck.getOverlapGroups(org.apache.hadoop.hbase.TableName)", "public com.google.common.collect.Multimap<byte[], org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo> getOverlapGroups(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setDisplayFullReport()", "public static void setDisplayFullReport()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixTableLocks(boolean)", "public void setFixTableLocks(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixTableZNodes(boolean)", "public void setFixTableZNodes(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixAssignments(boolean)", "public void setFixAssignments(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixMeta(boolean)", "public void setFixMeta(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixEmptyMetaCells(boolean)", "public void setFixEmptyMetaCells(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setCheckHdfs(boolean)", "public void setCheckHdfs(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixHdfsHoles(boolean)", "public void setFixHdfsHoles(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixTableOrphans(boolean)", "public void setFixTableOrphans(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixHdfsOverlaps(boolean)", "public void setFixHdfsOverlaps(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixHdfsOrphans(boolean)", "public void setFixHdfsOrphans(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixVersionFile(boolean)", "public void setFixVersionFile(boolean)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck.shouldFixVersionFile()", "public boolean shouldFixVersionFile()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setSidelineBigOverlaps(boolean)", "public void setSidelineBigOverlaps(boolean)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck.shouldSidelineBigOverlaps()", "public boolean shouldSidelineBigOverlaps()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixSplitParents(boolean)", "public void setFixSplitParents(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setFixReferenceFiles(boolean)", "public void setFixReferenceFiles(boolean)"], ["boolean", "org.apache.hadoop.hbase.util.HBaseFsck.shouldIgnorePreCheckPermission()", "public boolean shouldIgnorePreCheckPermission()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setIgnorePreCheckPermission(boolean)", "public void setIgnorePreCheckPermission(boolean)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setMaxMerge(int)", "public void setMaxMerge(int)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.getMaxMerge()", "public int getMaxMerge()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setMaxOverlapsToSideline(int)", "public void setMaxOverlapsToSideline(int)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.getMaxOverlapsToSideline()", "public int getMaxOverlapsToSideline()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.includeTable(org.apache.hadoop.hbase.TableName)", "public void includeTable(org.apache.hadoop.hbase.TableName)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setTimeLag(long)", "public void setTimeLag(long)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setSidelineDir(java.lang.String)", "public void setSidelineDir(java.lang.String)"], ["org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker", "org.apache.hadoop.hbase.util.HBaseFsck.getHFilecorruptionChecker()", "public org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker getHFilecorruptionChecker()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setHFileCorruptionChecker(org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker)", "public void setHFileCorruptionChecker(org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker)"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.setRetCode(int)", "public void setRetCode(int)"], ["int", "org.apache.hadoop.hbase.util.HBaseFsck.getRetCode()", "public int getRetCode()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.HBaseFsck", "org.apache.hadoop.hbase.util.HBaseFsck.exec(java.util.concurrent.ExecutorService, java.lang.String[])", "public org.apache.hadoop.hbase.util.HBaseFsck exec(java.util.concurrent.ExecutorService, java.lang.String[]) throws org.apache.zookeeper.KeeperException, java.io.IOException, com.google.protobuf.ServiceException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public static void debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsck.debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)", "public static void debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.HBaseFsckRepair", "org.apache.hadoop.hbase.util.HBaseFsckRepair()", "public org.apache.hadoop.hbase.util.HBaseFsckRepair()"], ["void", "org.apache.hadoop.hbase.util.HBaseFsckRepair.fixMultiAssignment(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>)", "public static void fixMultiAssignment(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.hbase.HRegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsckRepair.fixUnassigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.HRegionInfo)", "public static void fixUnassigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsckRepair.waitUntilAssigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.HRegionInfo)", "public static void waitUntilAssigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsckRepair.closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo)", "public static void closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.HConnection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.HBaseFsckRepair.fixMetaHoleOnlineAndAddReplicas(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, java.util.Collection<org.apache.hadoop.hbase.ServerName>, int)", "public static void fixMetaHoleOnlineAndAddReplicas(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, java.util.Collection<org.apache.hadoop.hbase.ServerName>, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.util.HBaseFsckRepair.createHDFSRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor)", "public static org.apache.hadoop.hbase.regionserver.HRegion createHDFSRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HTableDescriptor) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String)", "public static org.apache.hadoop.fs.Path getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[])", "public static org.apache.hadoop.fs.Path getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String)", "public static org.apache.hadoop.fs.Path getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getTableArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)", "public static org.apache.hadoop.fs.Path getTableArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getTableArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName)", "public static org.apache.hadoop.fs.Path getTableArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileArchiveUtil.getArchivePath(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.fs.Path getArchivePath(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.util.HFileV1Detector$1.call()", "public org.apache.hadoop.fs.Path call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HFileV1Detector$1.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.HFileV1Detector", "org.apache.hadoop.hbase.util.HFileV1Detector()", "public org.apache.hadoop.hbase.util.HFileV1Detector()"], ["int", "org.apache.hadoop.hbase.util.HFileV1Detector.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.io.IOException, org.apache.commons.cli.ParseException"], ["org.apache.hadoop.hbase.io.FileLink", "org.apache.hadoop.hbase.util.HFileV1Detector.getFileLinkWithPreNSPath(org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.io.FileLink getFileLinkWithPreNSPath(org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HFileV1Detector.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["java.lang.Boolean", "org.apache.hadoop.hbase.util.HMerge$1.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Boolean connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.HMerge$1.connect(org.apache.hadoop.hbase.client.HConnection)", "public java.lang.Object connect(org.apache.hadoop.hbase.client.HConnection) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HMerge.merge(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName)", "public static void merge(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.HMerge.merge(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName, boolean)", "public static void merge(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.HashedBytes", "org.apache.hadoop.hbase.util.HashedBytes(byte[])", "public org.apache.hadoop.hbase.util.HashedBytes(byte[])"], ["byte[]", "org.apache.hadoop.hbase.util.HashedBytes.getBytes()", "public byte[] getBytes()"], ["int", "org.apache.hadoop.hbase.util.HashedBytes.hashCode()", "public int hashCode()"], ["boolean", "org.apache.hadoop.hbase.util.HashedBytes.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["java.lang.String", "org.apache.hadoop.hbase.util.HashedBytes.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.HttpServerUtil", "org.apache.hadoop.hbase.util.HttpServerUtil()", "public org.apache.hadoop.hbase.util.HttpServerUtil()"], ["void", "org.apache.hadoop.hbase.util.HttpServerUtil.constrainHttpMethods(org.mortbay.jetty.servlet.Context)", "public static void constrainHttpMethods(org.mortbay.jetty.servlet.Context)"], ["java.lang.String", "org.apache.hadoop.hbase.util.IdLock$Entry.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.IdLock", "org.apache.hadoop.hbase.util.IdLock()", "public org.apache.hadoop.hbase.util.IdLock()"], ["org.apache.hadoop.hbase.util.IdLock$Entry", "org.apache.hadoop.hbase.util.IdLock.getLockEntry(long)", "public org.apache.hadoop.hbase.util.IdLock$Entry getLockEntry(long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.IdLock.releaseLockEntry(org.apache.hadoop.hbase.util.IdLock$Entry)", "public void releaseLockEntry(org.apache.hadoop.hbase.util.IdLock$Entry)"], ["void", "org.apache.hadoop.hbase.util.IdLock.waitForWaiters(long, int)", "public void waitForWaiters(long, int) throws java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.util.JSONBean$1.flush()", "public void flush() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.JSONBean$1.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.JSONBean$1.write(java.lang.String, java.lang.String)", "public void write(java.lang.String, java.lang.String) throws org.codehaus.jackson.JsonGenerationException, java.io.IOException"], ["int", "org.apache.hadoop.hbase.util.JSONBean$1.write(javax.management.MBeanServer, javax.management.ObjectName, java.lang.String, boolean)", "public int write(javax.management.MBeanServer, javax.management.ObjectName, java.lang.String, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JSONBean", "org.apache.hadoop.hbase.util.JSONBean()", "public org.apache.hadoop.hbase.util.JSONBean()"], ["org.apache.hadoop.hbase.util.JSONBean$Writer", "org.apache.hadoop.hbase.util.JSONBean.open(java.io.PrintWriter)", "public org.apache.hadoop.hbase.util.JSONBean$Writer open(java.io.PrintWriter) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.util.JSONBean.dumpRegionServerMetrics()", "public static java.lang.String dumpRegionServerMetrics() throws javax.management.MalformedObjectNameException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.JSONBean.dumpAllBeans()", "public static void dumpAllBeans() throws java.io.IOException, javax.management.MalformedObjectNameException"], ["void", "org.apache.hadoop.hbase.util.JSONBean.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException, javax.management.MalformedObjectNameException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread(org.apache.hadoop.hbase.master.HMaster, int)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread(org.apache.hadoop.hbase.master.HMaster, int)"], ["org.apache.hadoop.hbase.master.HMaster", "org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread.getMaster()", "public org.apache.hadoop.hbase.master.HMaster getMaster()"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread(org.apache.hadoop.hbase.regionserver.HRegionServer, int)", "public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread(org.apache.hadoop.hbase.regionserver.HRegionServer, int)"], ["org.apache.hadoop.hbase.regionserver.HRegionServer", "org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread.getRegionServer()", "public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer()"], ["void", "org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread.waitForServerOnline()", "public void waitForServerOnline()"], ["org.apache.hadoop.hbase.util.JVMClusterUtil", "org.apache.hadoop.hbase.util.JVMClusterUtil()", "public org.apache.hadoop.hbase.util.JVMClusterUtil()"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread", "org.apache.hadoop.hbase.util.JVMClusterUtil.createRegionServerThread(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>, int)", "public static org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread createRegionServerThread(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>, int) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread", "org.apache.hadoop.hbase.util.JVMClusterUtil.createMasterThread(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, int)", "public static org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread createMasterThread(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CoordinatedStateManager, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, int) throws java.io.IOException"], ["java.lang.String", "org.apache.hadoop.hbase.util.JVMClusterUtil.startup(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>)", "public static java.lang.String startup(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.JVMClusterUtil.shutdown(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>)", "public static void shutdown(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>)"], ["java.lang.String", "org.apache.hadoop.hbase.util.JvmPauseMonitor$GcTimes.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.util.JvmPauseMonitor$Monitor.run()", "public void run()"], ["org.apache.hadoop.hbase.util.JvmPauseMonitor", "org.apache.hadoop.hbase.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.util.JvmPauseMonitor.start()", "public void start()"], ["void", "org.apache.hadoop.hbase.util.JvmPauseMonitor.stop()", "public void stop()"], ["void", "org.apache.hadoop.hbase.util.JvmPauseMonitor.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.JvmVersion", "org.apache.hadoop.hbase.util.JvmVersion()", "public org.apache.hadoop.hbase.util.JvmVersion()"], ["boolean", "org.apache.hadoop.hbase.util.JvmVersion.isBadJvmVersion()", "public static boolean isBadJvmVersion()"], ["org.apache.hadoop.hbase.util.ManualEnvironmentEdge", "org.apache.hadoop.hbase.util.ManualEnvironmentEdge()", "public org.apache.hadoop.hbase.util.ManualEnvironmentEdge()"], ["void", "org.apache.hadoop.hbase.util.ManualEnvironmentEdge.setValue(long)", "public void setValue(long)"], ["void", "org.apache.hadoop.hbase.util.ManualEnvironmentEdge.incValue(long)", "public void incValue(long)"], ["long", "org.apache.hadoop.hbase.util.ManualEnvironmentEdge.currentTime()", "public long currentTime()"], ["org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool", "org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool()", "public org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool()"], ["void", "org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool.setConf(org.apache.hadoop.conf.Configuration)", "public void setConf(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.conf.Configuration", "org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool.getConf()", "public org.apache.hadoop.conf.Configuration getConf()"], ["int", "org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.Merge", "org.apache.hadoop.hbase.util.Merge()", "public org.apache.hadoop.hbase.util.Merge()"], ["org.apache.hadoop.hbase.util.Merge", "org.apache.hadoop.hbase.util.Merge(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.util.Merge(org.apache.hadoop.conf.Configuration)"], ["int", "org.apache.hadoop.hbase.util.Merge.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.util.Merge.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["org.apache.hadoop.hbase.util.MetaUtils", "org.apache.hadoop.hbase.util.MetaUtils()", "public org.apache.hadoop.hbase.util.MetaUtils() throws java.io.IOException"], ["org.apache.hadoop.hbase.util.MetaUtils", "org.apache.hadoop.hbase.util.MetaUtils(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.util.MetaUtils(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.util.MetaUtils.org.apache.hadoop.hbase.wal.WAL getLog(org.apache.hadoop.hbase.HRegionInfo)", "public synchronized org.apache.hadoop.hbase.wal.WAL getLog(org.apache.hadoop.hbase.HRegionInfo) throws java.io.IOException"], ["org.apache.hadoop.hbase.regionserver.HRegion", "org.apache.hadoop.hbase.util.MetaUtils.getMetaRegion()", "public org.apache.hadoop.hbase.regionserver.HRegion getMetaRegion() throws java.io.IOException"], ["synchronized", "org.apache.hadoop.hbase.util.MetaUtils.void shutdown()", "public synchronized void shutdown()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.util.ModifyRegionUtils$1.call()", "public org.apache.hadoop.hbase.HRegionInfo call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.ModifyRegionUtils$1.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.util.ModifyRegionUtils$2.call()", "public java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.ModifyRegionUtils$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["java.lang.Thread", "org.apache.hadoop.hbase.util.ModifyRegionUtils$3.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["org.apache.hadoop.hbase.HRegionInfo[]", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createHRegionInfos(org.apache.hadoop.hbase.HTableDescriptor, byte[][])", "public static org.apache.hadoop.hbase.HRegionInfo[] createHRegionInfos(org.apache.hadoop.hbase.HTableDescriptor, byte[][])"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[])", "public static java.util.List<org.apache.hadoop.hbase.HRegionInfo> createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[]) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask)", "public static java.util.List<org.apache.hadoop.hbase.HRegionInfo> createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask)", "public static java.util.List<org.apache.hadoop.hbase.HRegionInfo> createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.HRegionInfo>", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask)", "public static java.util.List<org.apache.hadoop.hbase.HRegionInfo> createRegions(java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask)", "public static org.apache.hadoop.hbase.HRegionInfo createRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ModifyRegionUtils.editRegions(java.util.concurrent.ThreadPoolExecutor, java.util.Collection<org.apache.hadoop.hbase.HRegionInfo>, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask)", "public static void editRegions(java.util.concurrent.ThreadPoolExecutor, java.util.Collection<org.apache.hadoop.hbase.HRegionInfo>, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ModifyRegionUtils.assignRegions(org.apache.hadoop.hbase.master.AssignmentManager, java.util.List<org.apache.hadoop.hbase.HRegionInfo>)", "public static void assignRegions(org.apache.hadoop.hbase.master.AssignmentManager, java.util.List<org.apache.hadoop.hbase.HRegionInfo>) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.MultiHConnection", "org.apache.hadoop.hbase.util.MultiHConnection(org.apache.hadoop.conf.Configuration, int)", "public org.apache.hadoop.hbase.util.MultiHConnection(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.MultiHConnection.close()", "public void close()"], ["<R> void", "org.apache.hadoop.hbase.util.MultiHConnection.processBatchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, org.apache.hadoop.hbase.TableName, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>)", "public <R> void processBatchCallback(java.util.List<? extends org.apache.hadoop.hbase.client.Row>, org.apache.hadoop.hbase.TableName, java.lang.Object[], org.apache.hadoop.hbase.client.coprocessor.Batch$Callback<R>) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.MunkresAssignment", "org.apache.hadoop.hbase.util.MunkresAssignment(float[][])", "public org.apache.hadoop.hbase.util.MunkresAssignment(float[][])"], ["int[]", "org.apache.hadoop.hbase.util.MunkresAssignment.solve()", "public int[] solve()"], ["org.apache.hadoop.hbase.util.ProtoUtil", "org.apache.hadoop.hbase.util.ProtoUtil()", "public org.apache.hadoop.hbase.util.ProtoUtil()"], ["int", "org.apache.hadoop.hbase.util.ProtoUtil.readRawVarint32(java.io.DataInput)", "public static int readRawVarint32(java.io.DataInput) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.RegionSizeCalculator", "org.apache.hadoop.hbase.util.RegionSizeCalculator(org.apache.hadoop.hbase.client.HTable)", "public org.apache.hadoop.hbase.util.RegionSizeCalculator(org.apache.hadoop.hbase.client.HTable) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.RegionSizeCalculator", "org.apache.hadoop.hbase.util.RegionSizeCalculator(org.apache.hadoop.hbase.client.RegionLocator, org.apache.hadoop.hbase.client.Admin)", "public org.apache.hadoop.hbase.util.RegionSizeCalculator(org.apache.hadoop.hbase.client.RegionLocator, org.apache.hadoop.hbase.client.Admin) throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.util.RegionSizeCalculator.getRegionSize(byte[])", "public long getRegionSize(byte[])"], ["java.util.Map<byte[], java.lang.Long>", "org.apache.hadoop.hbase.util.RegionSizeCalculator.getRegionSizeMap()", "public java.util.Map<byte[], java.lang.Long> getRegionSizeMap()"], ["int", "org.apache.hadoop.hbase.util.RegionSplitCalculator$1.compare(byte[], byte[])", "public int compare(byte[], byte[])"], ["int", "org.apache.hadoop.hbase.util.RegionSplitCalculator$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.util.RegionSplitCalculator", "org.apache.hadoop.hbase.util.RegionSplitCalculator(java.util.Comparator<R>)", "public org.apache.hadoop.hbase.util.RegionSplitCalculator(java.util.Comparator<R>)"], ["boolean", "org.apache.hadoop.hbase.util.RegionSplitCalculator.add(R)", "public boolean add(R)"], ["com.google.common.collect.Multimap<byte[], R>", "org.apache.hadoop.hbase.util.RegionSplitCalculator.calcCoverage()", "public com.google.common.collect.Multimap<byte[], R> calcCoverage()"], ["java.util.TreeSet<byte[]>", "org.apache.hadoop.hbase.util.RegionSplitCalculator.getSplits()", "public java.util.TreeSet<byte[]> getSplits()"], ["com.google.common.collect.Multimap<byte[], R>", "org.apache.hadoop.hbase.util.RegionSplitCalculator.getStarts()", "public com.google.common.collect.Multimap<byte[], R> getStarts()"], ["<R extends org.apache.hadoop.hbase.util.KeyRange> java.util.List<R>", "org.apache.hadoop.hbase.util.RegionSplitCalculator.findBigRanges(java.util.Collection<R>, int)", "public static <R extends org.apache.hadoop.hbase.util.KeyRange> java.util.List<R> findBigRanges(java.util.Collection<R>, int)"], ["int", "org.apache.hadoop.hbase.util.RegionSplitter$1.compare(java.lang.String, java.lang.String)", "public int compare(java.lang.String, java.lang.String)"], ["int", "org.apache.hadoop.hbase.util.RegionSplitter$1.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit()", "public org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit()"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.split(byte[], byte[])", "public byte[] split(byte[], byte[])"], ["byte[][]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.split(int)", "public byte[][] split(int)"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.firstRow()", "public byte[] firstRow()"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.lastRow()", "public byte[] lastRow()"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.setFirstRow(java.lang.String)", "public void setFirstRow(java.lang.String)"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.setLastRow(java.lang.String)", "public void setLastRow(java.lang.String)"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.strToRow(java.lang.String)", "public byte[] strToRow(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.rowToStr(byte[])", "public java.lang.String rowToStr(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.separator()", "public java.lang.String separator()"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.setFirstRow(byte[])", "public void setFirstRow(byte[])"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.setLastRow(byte[])", "public void setLastRow(byte[])"], ["java.math.BigInteger", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.split2(java.math.BigInteger, java.math.BigInteger)", "public java.math.BigInteger split2(java.math.BigInteger, java.math.BigInteger)"], ["byte[][]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.convertToBytes(java.math.BigInteger[])", "public byte[][] convertToBytes(java.math.BigInteger[])"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.convertToByte(java.math.BigInteger, int)", "public static byte[] convertToByte(java.math.BigInteger, int)"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.convertToByte(java.math.BigInteger)", "public byte[] convertToByte(java.math.BigInteger)"], ["java.math.BigInteger", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.convertToBigInteger(byte[])", "public java.math.BigInteger convertToBigInteger(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit()", "public org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit()"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.split(byte[], byte[])", "public byte[] split(byte[], byte[])"], ["byte[][]", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.split(int)", "public byte[][] split(int)"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.firstRow()", "public byte[] firstRow()"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.lastRow()", "public byte[] lastRow()"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.setFirstRow(java.lang.String)", "public void setFirstRow(java.lang.String)"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.setLastRow(java.lang.String)", "public void setLastRow(java.lang.String)"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.setFirstRow(byte[])", "public void setFirstRow(byte[])"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.setLastRow(byte[])", "public void setLastRow(byte[])"], ["byte[]", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.strToRow(java.lang.String)", "public byte[] strToRow(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.rowToStr(byte[])", "public java.lang.String rowToStr(byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.separator()", "public java.lang.String separator()"], ["java.lang.String", "org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.util.RegionSplitter", "org.apache.hadoop.hbase.util.RegionSplitter()", "public org.apache.hadoop.hbase.util.RegionSplitter()"], ["void", "org.apache.hadoop.hbase.util.RegionSplitter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException, java.lang.InterruptedException, org.apache.commons.cli.ParseException"], ["org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm", "org.apache.hadoop.hbase.util.RegionSplitter.newSplitAlgoInstance(org.apache.hadoop.conf.Configuration, java.lang.String)", "public static org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm newSplitAlgoInstance(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.util.ServerCommandLine", "org.apache.hadoop.hbase.util.ServerCommandLine()", "public org.apache.hadoop.hbase.util.ServerCommandLine()"], ["void", "org.apache.hadoop.hbase.util.ServerCommandLine.logJVMInfo()", "public static void logJVMInfo()"], ["void", "org.apache.hadoop.hbase.util.ServerCommandLine.logProcessInfo(org.apache.hadoop.conf.Configuration)", "public static void logProcessInfo(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.util.ServerCommandLine.doMain(java.lang.String[])", "public void doMain(java.lang.String[])"], ["org.apache.hadoop.hbase.util.ServerRegionReplicaUtil", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil()", "public org.apache.hadoop.hbase.util.ServerRegionReplicaUtil()"], ["org.apache.hadoop.hbase.HRegionInfo", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.getRegionInfoForFs(org.apache.hadoop.hbase.HRegionInfo)", "public static org.apache.hadoop.hbase.HRegionInfo getRegionInfoForFs(org.apache.hadoop.hbase.HRegionInfo)"], ["boolean", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.isReadOnly(org.apache.hadoop.hbase.regionserver.HRegion)", "public static boolean isReadOnly(org.apache.hadoop.hbase.regionserver.HRegion)"], ["boolean", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.shouldReplayRecoveredEdits(org.apache.hadoop.hbase.regionserver.HRegion)", "public static boolean shouldReplayRecoveredEdits(org.apache.hadoop.hbase.regionserver.HRegion)"], ["org.apache.hadoop.hbase.regionserver.StoreFileInfo", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.getStoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.regionserver.StoreFileInfo getStoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HRegionInfo, java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.setupRegionReplicaReplication(org.apache.hadoop.conf.Configuration)", "public static void setupRegionReplicaReplication(org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.isRegionReplicaReplicationEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isRegionReplicaReplicationEnabled(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.isRegionReplicaWaitForPrimaryFlushEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isRegionReplicaWaitForPrimaryFlushEnabled(org.apache.hadoop.conf.Configuration)"], ["boolean", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.isRegionReplicaStoreFileRefreshEnabled(org.apache.hadoop.conf.Configuration)", "public static boolean isRegionReplicaStoreFileRefreshEnabled(org.apache.hadoop.conf.Configuration)"], ["double", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.getRegionReplicaStoreFileRefreshMultiplier(org.apache.hadoop.conf.Configuration)", "public static double getRegionReplicaStoreFileRefreshMultiplier(org.apache.hadoop.conf.Configuration)"], ["java.lang.String", "org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.getReplicationPeerId()", "public static java.lang.String getReplicationPeerId()"], ["void", "org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV1.addShutdownHook(java.lang.Thread, int)", "public void addShutdownHook(java.lang.Thread, int)"], ["boolean", "org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV1.removeShutdownHook(java.lang.Runnable)", "public boolean removeShutdownHook(java.lang.Runnable)"], ["void", "org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV2.addShutdownHook(java.lang.Thread, int)", "public void addShutdownHook(java.lang.Thread, int)"], ["boolean", "org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV2.removeShutdownHook(java.lang.Runnable)", "public boolean removeShutdownHook(java.lang.Runnable)"], ["org.apache.hadoop.hbase.util.ShutdownHookManager", "org.apache.hadoop.hbase.util.ShutdownHookManager()", "public org.apache.hadoop.hbase.util.ShutdownHookManager()"], ["void", "org.apache.hadoop.hbase.util.ShutdownHookManager.affixShutdownHook(java.lang.Thread, int)", "public static void affixShutdownHook(java.lang.Thread, int)"], ["boolean", "org.apache.hadoop.hbase.util.ShutdownHookManager.deleteShutdownHook(java.lang.Runnable)", "public static boolean deleteShutdownHook(java.lang.Runnable)"], ["org.apache.hadoop.hbase.util.SortedCopyOnWriteSet", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet()", "public org.apache.hadoop.hbase.util.SortedCopyOnWriteSet()"], ["org.apache.hadoop.hbase.util.SortedCopyOnWriteSet", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet(java.util.Collection<? extends E>)", "public org.apache.hadoop.hbase.util.SortedCopyOnWriteSet(java.util.Collection<? extends E>)"], ["org.apache.hadoop.hbase.util.SortedCopyOnWriteSet", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet(java.util.Comparator<? super E>)", "public org.apache.hadoop.hbase.util.SortedCopyOnWriteSet(java.util.Comparator<? super E>)"], ["int", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.size()", "public int size()"], ["boolean", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.isEmpty()", "public boolean isEmpty()"], ["boolean", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.contains(java.lang.Object)", "public boolean contains(java.lang.Object)"], ["java.util.Iterator<E>", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.iterator()", "public java.util.Iterator<E> iterator()"], ["java.lang.Object[]", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.toArray()", "public java.lang.Object[] toArray()"], ["<T> T[]", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.toArray(T[])", "public <T> T[] toArray(T[])"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.boolean add(E)", "public synchronized boolean add(E)"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.boolean remove(java.lang.Object)", "public synchronized boolean remove(java.lang.Object)"], ["boolean", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.containsAll(java.util.Collection<?>)", "public boolean containsAll(java.util.Collection<?>)"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.boolean addAll(java.util.Collection<? extends E>)", "public synchronized boolean addAll(java.util.Collection<? extends E>)"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.boolean retainAll(java.util.Collection<?>)", "public synchronized boolean retainAll(java.util.Collection<?>)"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.boolean removeAll(java.util.Collection<?>)", "public synchronized boolean removeAll(java.util.Collection<?>)"], ["synchronized", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.void clear()", "public synchronized void clear()"], ["java.util.Comparator<? super E>", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.comparator()", "public java.util.Comparator<? super E> comparator()"], ["java.util.SortedSet<E>", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.subSet(E, E)", "public java.util.SortedSet<E> subSet(E, E)"], ["java.util.SortedSet<E>", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.headSet(E)", "public java.util.SortedSet<E> headSet(E)"], ["java.util.SortedSet<E>", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.tailSet(E)", "public java.util.SortedSet<E> tailSet(E)"], ["E", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.first()", "public E first()"], ["E", "org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.last()", "public E last()"], ["com.yammer.metrics.core.Histogram", "org.apache.hadoop.hbase.util.YammerHistogramUtils.newHistogram(com.yammer.metrics.stats.Sample)", "public static com.yammer.metrics.core.Histogram newHistogram(com.yammer.metrics.stats.Sample)"], ["java.lang.String", "org.apache.hadoop.hbase.util.YammerHistogramUtils.getShortHistogramReport(com.yammer.metrics.core.Histogram)", "public static java.lang.String getShortHistogramReport(com.yammer.metrics.core.Histogram)"], ["java.lang.String", "org.apache.hadoop.hbase.util.YammerHistogramUtils.getHistogramReport(com.yammer.metrics.core.Histogram)", "public static java.lang.String getHistogramReport(com.yammer.metrics.core.Histogram)"], ["void", "org.apache.hadoop.hbase.util.ZKDataMigrator$ZKDataMigratorAbortable.abort(java.lang.String, java.lang.Throwable)", "public void abort(java.lang.String, java.lang.Throwable)"], ["boolean", "org.apache.hadoop.hbase.util.ZKDataMigrator$ZKDataMigratorAbortable.isAborted()", "public boolean isAborted()"], ["org.apache.hadoop.hbase.util.ZKDataMigrator", "org.apache.hadoop.hbase.util.ZKDataMigrator()", "public org.apache.hadoop.hbase.util.ZKDataMigrator()"], ["int", "org.apache.hadoop.hbase.util.ZKDataMigrator.run(java.lang.String[])", "public int run(java.lang.String[]) throws java.lang.Exception"], ["void", "org.apache.hadoop.hbase.util.ZKDataMigrator.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["java.lang.Void", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call()", "public java.lang.Void call() throws java.io.IOException"], ["java.lang.Object", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService, boolean)", "public org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService, boolean) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.checkTables(java.util.Collection<org.apache.hadoop.fs.Path>)", "public void checkTables(java.util.Collection<org.apache.hadoop.fs.Path>) throws java.io.IOException"], ["java.util.Collection<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.getFailures()", "public java.util.Collection<org.apache.hadoop.fs.Path> getFailures()"], ["java.util.Collection<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.getCorrupted()", "public java.util.Collection<org.apache.hadoop.fs.Path> getCorrupted()"], ["int", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.getHFilesChecked()", "public int getHFilesChecked()"], ["java.util.Collection<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.getQuarantined()", "public java.util.Collection<org.apache.hadoop.fs.Path> getQuarantined()"], ["java.util.Collection<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.getMissing()", "public java.util.Collection<org.apache.hadoop.fs.Path> getMissing()"], ["void", "org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.report(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)", "public void report(org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)"], ["org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair", "org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair()", "public org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair()"], ["void", "org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl()", "public org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl()"], ["org.apache.hadoop.hbase.util.HBaseFsck$TableInfo", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.getTableInfo()", "public org.apache.hadoop.hbase.util.HBaseFsck$TableInfo getTableInfo()"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.setTableInfo(org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)", "public void setTableInfo(org.apache.hadoop.hbase.util.HBaseFsck$TableInfo)"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleRegionEndKeyNotEmpty(byte[])", "public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleDegenerateRegion(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleDegenerateRegion(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo)", "public void handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo, org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleHoleInRegionChain(byte[], byte[])", "public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>)", "public void handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HBaseFsck$HbckInfo>) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableLockChecker$1.handleMetadata(byte[])", "public void handleMetadata(byte[])"], ["org.apache.hadoop.hbase.util.hbck.TableLockChecker", "org.apache.hadoop.hbase.util.hbck.TableLockChecker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)", "public org.apache.hadoop.hbase.util.hbck.TableLockChecker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.util.HBaseFsck$ErrorReporter)"], ["void", "org.apache.hadoop.hbase.util.hbck.TableLockChecker.checkTableLocks()", "public void checkTableLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.util.hbck.TableLockChecker.fixExpiredTableLocks()", "public void fixExpiredTableLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.BoundedRegionGroupingProvider.init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String)", "public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.BoundedRegionGroupingProvider.shutdown()", "public void shutdown() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.BoundedRegionGroupingProvider.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.DefaultWALProvider", "org.apache.hadoop.hbase.wal.DefaultWALProvider()", "public org.apache.hadoop.hbase.wal.DefaultWALProvider()"], ["void", "org.apache.hadoop.hbase.wal.DefaultWALProvider.init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String)", "public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getWAL(byte[])", "public org.apache.hadoop.hbase.wal.WAL getWAL(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.DefaultWALProvider.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.DefaultWALProvider.shutdown()", "public void shutdown() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getNumLogFiles(org.apache.hadoop.hbase.wal.WALFactory)", "public static long getNumLogFiles(org.apache.hadoop.hbase.wal.WALFactory)"], ["long", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getLogFileSize(org.apache.hadoop.hbase.wal.WALFactory)", "public static long getLogFileSize(org.apache.hadoop.hbase.wal.WALFactory)"], ["int", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getNumRolledLogFiles(org.apache.hadoop.hbase.wal.WAL)", "public static int getNumRolledLogFiles(org.apache.hadoop.hbase.wal.WAL)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getCurrentFileName(org.apache.hadoop.hbase.wal.WAL)", "public static org.apache.hadoop.fs.Path getCurrentFileName(org.apache.hadoop.hbase.wal.WAL)"], ["long", "org.apache.hadoop.hbase.wal.DefaultWALProvider.extractFileNumFromWAL(org.apache.hadoop.hbase.wal.WAL)", "public static long extractFileNumFromWAL(org.apache.hadoop.hbase.wal.WAL)"], ["boolean", "org.apache.hadoop.hbase.wal.DefaultWALProvider.validateWALFilename(java.lang.String)", "public static boolean validateWALFilename(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getWALDirectoryName(java.lang.String)", "public static java.lang.String getWALDirectoryName(java.lang.String)"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getServerNameFromWALDirectoryName(org.apache.hadoop.conf.Configuration, java.lang.String)", "public static org.apache.hadoop.hbase.ServerName getServerNameFromWALDirectoryName(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.ServerName", "org.apache.hadoop.hbase.wal.DefaultWALProvider.getServerNameFromWALDirectoryName(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.hbase.ServerName getServerNameFromWALDirectoryName(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.wal.DefaultWALProvider.isMetaFile(org.apache.hadoop.fs.Path)", "public static boolean isMetaFile(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.wal.DefaultWALProvider.isMetaFile(java.lang.String)", "public static boolean isMetaFile(java.lang.String)"], ["org.apache.hadoop.hbase.wal.DefaultWALProvider$Writer", "org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean)", "public static org.apache.hadoop.hbase.wal.DefaultWALProvider$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>)", "public org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>)"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)", "public void registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)"], ["boolean", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)", "public boolean unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener)"], ["byte[][]", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.rollWriter()", "public byte[][] rollWriter()"], ["byte[][]", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.rollWriter(boolean)", "public byte[][] rollWriter(boolean)"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.shutdown()", "public void shutdown()"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.close()", "public void close()"], ["long", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.append(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, java.util.concurrent.atomic.AtomicLong, boolean, java.util.List<org.apache.hadoop.hbase.Cell>)", "public long append(org.apache.hadoop.hbase.HTableDescriptor, org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit, java.util.concurrent.atomic.AtomicLong, boolean, java.util.List<org.apache.hadoop.hbase.Cell>)"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.sync()", "public void sync()"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.sync(long)", "public void sync(long)"], ["java.lang.Long", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.startCacheFlush(byte[], java.util.Set<byte[]>)", "public java.lang.Long startCacheFlush(byte[], java.util.Set<byte[]>)"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.completeCacheFlush(byte[])", "public void completeCacheFlush(byte[])"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.abortCacheFlush(byte[])", "public void abortCacheFlush(byte[])"], ["org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.getCoprocessorHost()", "public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost getCoprocessorHost()"], ["long", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.getEarliestMemstoreSeqNum(byte[])", "public long getEarliestMemstoreSeqNum(byte[])"], ["long", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.getEarliestMemstoreSeqNum(byte[], byte[])", "public long getEarliestMemstoreSeqNum(byte[], byte[])"], ["java.lang.String", "org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL.toString()", "public java.lang.String toString()"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider.init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String)", "public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.wal.DisabledWALProvider.getWAL(byte[])", "public org.apache.hadoop.hbase.wal.WAL getWAL(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.DisabledWALProvider.shutdown()", "public void shutdown() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.RegionGroupingProvider$IdentityGroupingStrategy.init(org.apache.hadoop.conf.Configuration)", "public void init(org.apache.hadoop.conf.Configuration)"], ["byte[]", "org.apache.hadoop.hbase.wal.RegionGroupingProvider$IdentityGroupingStrategy.group(byte[])", "public byte[] group(byte[])"], ["org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies[]", "org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies.values()", "public static org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies[] values()"], ["org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies", "org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies valueOf(java.lang.String)"], ["void", "org.apache.hadoop.hbase.wal.RegionGroupingProvider.init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String)", "public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.wal.RegionGroupingProvider.getWAL(byte[])", "public org.apache.hadoop.hbase.wal.WAL getWAL(byte[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.RegionGroupingProvider.shutdown()", "public void shutdown() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.RegionGroupingProvider.close()", "public void close() throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.wal.WAL$Entry()", "public org.apache.hadoop.hbase.wal.WAL$Entry()"], ["org.apache.hadoop.hbase.wal.WAL$Entry", "org.apache.hadoop.hbase.wal.WAL$Entry(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)", "public org.apache.hadoop.hbase.wal.WAL$Entry(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit)"], ["org.apache.hadoop.hbase.regionserver.wal.WALEdit", "org.apache.hadoop.hbase.wal.WAL$Entry.getEdit()", "public org.apache.hadoop.hbase.regionserver.wal.WALEdit getEdit()"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WAL$Entry.getKey()", "public org.apache.hadoop.hbase.wal.WALKey getKey()"], ["void", "org.apache.hadoop.hbase.wal.WAL$Entry.setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public void setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["java.lang.String", "org.apache.hadoop.hbase.wal.WAL$Entry.toString()", "public java.lang.String toString()"], ["org.apache.hadoop.hbase.wal.WALFactory$Providers[]", "org.apache.hadoop.hbase.wal.WALFactory$Providers.values()", "public static org.apache.hadoop.hbase.wal.WALFactory$Providers[] values()"], ["org.apache.hadoop.hbase.wal.WALFactory$Providers", "org.apache.hadoop.hbase.wal.WALFactory$Providers.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.wal.WALFactory$Providers valueOf(java.lang.String)"], ["org.apache.hadoop.hbase.wal.WALFactory", "org.apache.hadoop.hbase.wal.WALFactory(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String)", "public org.apache.hadoop.hbase.wal.WALFactory(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, java.lang.String) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALFactory.close()", "public void close() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALFactory.shutdown()", "public void shutdown() throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.wal.WALFactory.getWAL(byte[])", "public org.apache.hadoop.hbase.wal.WAL getWAL(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL", "org.apache.hadoop.hbase.wal.WALFactory.getMetaWAL(byte[])", "public org.apache.hadoop.hbase.wal.WAL getMetaWAL(byte[]) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.wal.WALFactory.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.wal.WALFactory.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable)", "public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.wal.WALFactory.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable, boolean)", "public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable, boolean) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALProvider$Writer", "org.apache.hadoop.hbase.wal.WALFactory.createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.wal.WALProvider$Writer createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALProvider$Writer", "org.apache.hadoop.hbase.wal.WALFactory.createRecoveredEditsWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public org.apache.hadoop.hbase.wal.WALProvider$Writer createRecoveredEditsWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALFactory", "org.apache.hadoop.hbase.wal.WALFactory.getInstance(org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.wal.WALFactory getInstance(org.apache.hadoop.conf.Configuration)"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.wal.WALFactory.createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WAL$Reader", "org.apache.hadoop.hbase.wal.WALFactory.createReaderIgnoreCustomClass(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.wal.WAL$Reader createReaderIgnoreCustomClass(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALProvider$Writer", "org.apache.hadoop.hbase.wal.WALFactory.createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)", "public static org.apache.hadoop.hbase.wal.WALProvider$Writer createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALKey$Version[]", "org.apache.hadoop.hbase.wal.WALKey$Version.values()", "public static org.apache.hadoop.hbase.wal.WALKey$Version[] values()"], ["org.apache.hadoop.hbase.wal.WALKey$Version", "org.apache.hadoop.hbase.wal.WALKey$Version.valueOf(java.lang.String)", "public static org.apache.hadoop.hbase.wal.WALKey$Version valueOf(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.wal.WALKey$Version.atLeast(org.apache.hadoop.hbase.wal.WALKey$Version)", "public boolean atLeast(org.apache.hadoop.hbase.wal.WALKey$Version)"], ["org.apache.hadoop.hbase.wal.WALKey$Version", "org.apache.hadoop.hbase.wal.WALKey$Version.fromCode(int)", "public static org.apache.hadoop.hbase.wal.WALKey$Version fromCode(int)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey()", "public org.apache.hadoop.hbase.wal.WALKey()"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long)"], ["org.apache.hadoop.hbase.wal.WALKey", "org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, long)", "public org.apache.hadoop.hbase.wal.WALKey(byte[], org.apache.hadoop.hbase.TableName, long, long, long)"], ["void", "org.apache.hadoop.hbase.wal.WALKey.setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)", "public void setCompressionContext(org.apache.hadoop.hbase.regionserver.wal.CompressionContext)"], ["byte[]", "org.apache.hadoop.hbase.wal.WALKey.getEncodedRegionName()", "public byte[] getEncodedRegionName()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.wal.WALKey.getTablename()", "public org.apache.hadoop.hbase.TableName getTablename()"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getLogSeqNum()", "public long getLogSeqNum()"], ["void", "org.apache.hadoop.hbase.wal.WALKey.setLogSeqNum(long)", "public void setLogSeqNum(long)"], ["void", "org.apache.hadoop.hbase.wal.WALKey.setOrigLogSeqNum(long)", "public void setOrigLogSeqNum(long)"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getOrigLogSeqNum()", "public long getOrigLogSeqNum()"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getSequenceId()", "public long getSequenceId() throws java.io.IOException"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getWriteTime()", "public long getWriteTime()"], ["java.util.NavigableMap<byte[], java.lang.Integer>", "org.apache.hadoop.hbase.wal.WALKey.getScopes()", "public java.util.NavigableMap<byte[], java.lang.Integer> getScopes()"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getNonceGroup()", "public long getNonceGroup()"], ["long", "org.apache.hadoop.hbase.wal.WALKey.getNonce()", "public long getNonce()"], ["void", "org.apache.hadoop.hbase.wal.WALKey.setScopes(java.util.NavigableMap<byte[], java.lang.Integer>)", "public void setScopes(java.util.NavigableMap<byte[], java.lang.Integer>)"], ["void", "org.apache.hadoop.hbase.wal.WALKey.readOlderScopes(java.util.NavigableMap<byte[], java.lang.Integer>)", "public void readOlderScopes(java.util.NavigableMap<byte[], java.lang.Integer>)"], ["void", "org.apache.hadoop.hbase.wal.WALKey.addClusterId(java.util.UUID)", "public void addClusterId(java.util.UUID)"], ["java.util.List<java.util.UUID>", "org.apache.hadoop.hbase.wal.WALKey.getClusterIds()", "public java.util.List<java.util.UUID> getClusterIds()"], ["java.util.UUID", "org.apache.hadoop.hbase.wal.WALKey.getOriginatingClusterId()", "public java.util.UUID getOriginatingClusterId()"], ["java.lang.String", "org.apache.hadoop.hbase.wal.WALKey.toString()", "public java.lang.String toString()"], ["java.util.Map<java.lang.String, java.lang.Object>", "org.apache.hadoop.hbase.wal.WALKey.toStringMap()", "public java.util.Map<java.lang.String, java.lang.Object> toStringMap()"], ["boolean", "org.apache.hadoop.hbase.wal.WALKey.equals(java.lang.Object)", "public boolean equals(java.lang.Object)"], ["int", "org.apache.hadoop.hbase.wal.WALKey.hashCode()", "public int hashCode()"], ["int", "org.apache.hadoop.hbase.wal.WALKey.compareTo(org.apache.hadoop.hbase.wal.WALKey)", "public int compareTo(org.apache.hadoop.hbase.wal.WALKey)"], ["org.apache.hadoop.hbase.protobuf.generated.WALProtos$WALKey$Builder", "org.apache.hadoop.hbase.wal.WALKey.getBuilder(org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor)", "public org.apache.hadoop.hbase.protobuf.generated.WALProtos$WALKey$Builder getBuilder(org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALKey.readFieldsFromPb(org.apache.hadoop.hbase.protobuf.generated.WALProtos$WALKey, org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor)", "public void readFieldsFromPb(org.apache.hadoop.hbase.protobuf.generated.WALProtos$WALKey, org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor) throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.wal.WALKey.compareTo(java.lang.Object)", "public int compareTo(java.lang.Object)"], ["org.apache.hadoop.hbase.wal.WALPrettyPrinter", "org.apache.hadoop.hbase.wal.WALPrettyPrinter()", "public org.apache.hadoop.hbase.wal.WALPrettyPrinter()"], ["org.apache.hadoop.hbase.wal.WALPrettyPrinter", "org.apache.hadoop.hbase.wal.WALPrettyPrinter(boolean, boolean, long, java.lang.String, java.lang.String, boolean, java.io.PrintStream)", "public org.apache.hadoop.hbase.wal.WALPrettyPrinter(boolean, boolean, long, java.lang.String, java.lang.String, boolean, java.io.PrintStream)"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.enableValues()", "public void enableValues()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.disableValues()", "public void disableValues()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.enableJSON()", "public void enableJSON()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.disableJSON()", "public void disableJSON()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.setSequenceFilter(long)", "public void setSequenceFilter(long)"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.setRegionFilter(java.lang.String)", "public void setRegionFilter(java.lang.String)"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.setRowFilter(java.lang.String)", "public void setRowFilter(java.lang.String)"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.beginPersistentOutput()", "public void beginPersistentOutput()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.endPersistentOutput()", "public void endPersistentOutput()"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.processFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)", "public void processFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALPrettyPrinter.run(java.lang.String[])", "public static void run(java.lang.String[]) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter$1.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter$2.accept(org.apache.hadoop.fs.Path)", "public boolean accept(org.apache.hadoop.fs.Path)"], ["org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers", "org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, long)", "public org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, long)"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers.appendEntry(org.apache.hadoop.hbase.wal.WAL$Entry)", "public void appendEntry(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.lang.InterruptedException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers.waitUntilDrained()", "public void waitUntilDrained()"], ["java.lang.Thread", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink$1.newThread(java.lang.Runnable)", "public java.lang.Thread newThread(java.lang.Runnable)"], ["java.lang.Void", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink$2.call()", "public java.lang.Void call() throws java.lang.Exception"], ["java.lang.Object", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink$2.call()", "public java.lang.Object call() throws java.lang.Exception"], ["org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)", "public org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose()", "public java.util.List<org.apache.hadoop.fs.Path> finishWritingAndClose() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink.append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer)", "public void append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer) throws java.io.IOException"], ["java.util.Map<byte[], java.lang.Long>", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink.getOutputCounts()", "public java.util.Map<byte[], java.lang.Long> getOutputCounts()"], ["int", "org.apache.hadoop.hbase.wal.WALSplitter$LogRecoveredEditsOutputSink.getNumberOfRecoveredRegions()", "public int getNumberOfRecoveredRegions()"], ["org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)", "public org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer)", "public void append(org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.flush()", "public boolean flush() throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.finishWritingAndClose()", "public java.util.List<org.apache.hadoop.fs.Path> finishWritingAndClose() throws java.io.IOException"], ["java.util.Map<byte[], java.lang.Long>", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.getOutputCounts()", "public java.util.Map<byte[], java.lang.Long> getOutputCounts()"], ["int", "org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.getNumberOfRecoveredRegions()", "public int getNumberOfRecoveredRegions()"], ["org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay", "org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutationProto$MutationType, org.apache.hadoop.hbase.client.Mutation, long, long)", "public org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MutationProto$MutationType, org.apache.hadoop.hbase.client.Mutation, long, long)"], ["org.apache.hadoop.hbase.wal.WALSplitter$OutputSink", "org.apache.hadoop.hbase.wal.WALSplitter$OutputSink(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)", "public org.apache.hadoop.hbase.wal.WALSplitter$OutputSink(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.WALSplitter$EntryBuffers, int)"], ["synchronized", "org.apache.hadoop.hbase.wal.WALSplitter$OutputSink.void startWriterThreads()", "public synchronized void startWriterThreads()"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter$OutputSink.flush()", "public boolean flush() throws java.io.IOException"], ["org.apache.hadoop.hbase.wal.WALSplitter$PipelineController", "org.apache.hadoop.hbase.wal.WALSplitter$PipelineController()", "public org.apache.hadoop.hbase.wal.WALSplitter$PipelineController()"], ["long", "org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer.heapSize()", "public long heapSize()"], ["byte[]", "org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer.getEncodedRegionName()", "public byte[] getEncodedRegionName()"], ["java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>", "org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer.getEntryBuffer()", "public java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry> getEntryBuffer()"], ["org.apache.hadoop.hbase.TableName", "org.apache.hadoop.hbase.wal.WALSplitter$RegionEntryBuffer.getTableName()", "public org.apache.hadoop.hbase.TableName getTableName()"], ["org.apache.hadoop.hbase.wal.WALSplitter$SinkWriter", "org.apache.hadoop.hbase.wal.WALSplitter$SinkWriter()", "public org.apache.hadoop.hbase.wal.WALSplitter$SinkWriter()"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.run()", "public void run()"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode, org.apache.hadoop.hbase.wal.WALFactory)", "public static boolean splitLogFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.CoordinatedStateManager, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$SplitLogTask$RecoveryMode, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.wal.WALSplitter.split(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.wal.WALFactory)", "public static java.util.List<org.apache.hadoop.fs.Path> split(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.wal.WALSplitter.finishSplitLogFile(java.lang.String, org.apache.hadoop.conf.Configuration)", "public static void finishSplitLogFile(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.wal.WALSplitter.getRegionDirRecoveredEditsDir(org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path getRegionDirRecoveredEditsDir(org.apache.hadoop.fs.Path)"], ["java.util.NavigableSet<org.apache.hadoop.fs.Path>", "org.apache.hadoop.hbase.wal.WALSplitter.getSplitEditFilesSorted(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static java.util.NavigableSet<org.apache.hadoop.fs.Path> getSplitEditFilesSorted(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.wal.WALSplitter.moveAsideBadEditsFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)", "public static org.apache.hadoop.fs.Path moveAsideBadEditsFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.wal.WALSplitter.isSequenceIdFile(org.apache.hadoop.fs.Path)", "public static boolean isSequenceIdFile(org.apache.hadoop.fs.Path)"], ["long", "org.apache.hadoop.hbase.wal.WALSplitter.writeRegionSequenceIdFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long, long)", "public static long writeRegionSequenceIdFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long, long) throws java.io.IOException"], ["java.util.List<org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay>", "org.apache.hadoop.hbase.wal.WALSplitter.getMutationsFromWALEntry(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry, org.apache.hadoop.hbase.CellScanner, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit>, org.apache.hadoop.hbase.client.Durability)", "public static java.util.List<org.apache.hadoop.hbase.wal.WALSplitter$MutationReplay> getMutationsFromWALEntry(org.apache.hadoop.hbase.protobuf.generated.AdminProtos$WALEntry, org.apache.hadoop.hbase.CellScanner, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.regionserver.wal.WALEdit>, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException"], ["org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker", "org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker.isClusterUp()", "public boolean isClusterUp()"], ["void", "org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker.setClusterUp()", "public void setClusterUp() throws org.apache.zookeeper.KeeperException"], ["void", "org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker.setClusterDown()", "public void setClusterDown() throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.zookeeper.DeletionListener", "org.apache.hadoop.hbase.zookeeper.DeletionListener(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.util.concurrent.CountDownLatch)", "public org.apache.hadoop.hbase.zookeeper.DeletionListener(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.util.concurrent.CountDownLatch)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.DeletionListener.hasException()", "public boolean hasException()"], ["java.lang.Throwable", "org.apache.hadoop.hbase.zookeeper.DeletionListener.getException()", "public java.lang.Throwable getException()"], ["void", "org.apache.hadoop.hbase.zookeeper.DeletionListener.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.DeletionListener.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["org.apache.hadoop.hbase.zookeeper.DrainingServerTracker", "org.apache.hadoop.hbase.zookeeper.DrainingServerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.hbase.master.ServerManager)", "public org.apache.hadoop.hbase.zookeeper.DrainingServerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.hbase.master.ServerManager)"], ["void", "org.apache.hadoop.hbase.zookeeper.DrainingServerTracker.start()", "public void start() throws org.apache.zookeeper.KeeperException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.zookeeper.DrainingServerTracker.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.DrainingServerTracker.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker", "org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable)", "public org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Abortable)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker.isBalancerOn()", "public boolean isBalancerOn()"], ["void", "org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker.setBalancerOn(boolean)", "public void setBalancerOn(boolean) throws org.apache.zookeeper.KeeperException"], ["org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster()", "public org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster()"], ["org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster(org.apache.hadoop.conf.Configuration)", "public org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.addClientPort(int)", "public void addClientPort(int)"], ["java.util.List<java.lang.Integer>", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.getClientPortList()", "public java.util.List<java.lang.Integer> getClientPortList()"], ["void", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setDefaultClientPort(int)", "public void setDefaultClientPort(int)"], ["void", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.setTickTime(int)", "public void setTickTime(int)"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.getBackupZooKeeperServerNum()", "public int getBackupZooKeeperServerNum()"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.getZooKeeperServerNum()", "public int getZooKeeperServerNum()"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(java.io.File)", "public int startup(java.io.File) throws java.io.IOException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(java.io.File, int)", "public int startup(java.io.File, int) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.shutdown()", "public void shutdown() throws java.io.IOException"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.killCurrentActiveZooKeeperServer()", "public int killCurrentActiveZooKeeperServer() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.killOneBackupZooKeeperServer()", "public void killOneBackupZooKeeperServer() throws java.io.IOException, java.lang.InterruptedException"], ["int", "org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.getClientPort()", "public int getClientPort()"], ["org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher", "org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.regionserver.HRegionServer)", "public org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.regionserver.HRegionServer)"], ["void", "org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher.nodeDataChanged(java.lang.String)", "public void nodeDataChanged(java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.RecoveringRegionWatcher.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.zookeeper.RegionServerTracker", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.ServerManager)", "public org.apache.hadoop.hbase.zookeeper.RegionServerTracker(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.ServerManager)"], ["void", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker.start()", "public void start() throws org.apache.zookeeper.KeeperException, java.io.IOException"], ["void", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker.nodeDeleted(java.lang.String)", "public void nodeDeleted(java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker.nodeChildrenChanged(java.lang.String)", "public void nodeChildrenChanged(java.lang.String)"], ["org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$RegionServerInfo", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker.getRegionServerInfo(org.apache.hadoop.hbase.ServerName)", "public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$RegionServerInfo getRegionServerInfo(org.apache.hadoop.hbase.ServerName)"], ["java.util.List<org.apache.hadoop.hbase.ServerName>", "org.apache.hadoop.hbase.zookeeper.RegionServerTracker.getOnlineServers()", "public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServers()"], ["org.apache.hadoop.hbase.zookeeper.ZKServerTool", "org.apache.hadoop.hbase.zookeeper.ZKServerTool()", "public org.apache.hadoop.hbase.zookeeper.ZKServerTool()"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKServerTool.main(java.lang.String[])", "public static void main(java.lang.String[])"], ["org.apache.hadoop.hbase.zookeeper.ZKSplitLog", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog()", "public org.apache.hadoop.hbase.zookeeper.ZKSplitLog()"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getEncodedNodeName(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public static java.lang.String getEncodedNodeName(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getFileName(java.lang.String)", "public static java.lang.String getFileName(java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getRescanNode(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public static java.lang.String getRescanNode(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.isRescanNode(java.lang.String)", "public static boolean isRescanNode(java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.isRescanNode(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public static boolean isRescanNode(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.isTaskPath(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public static boolean isTaskPath(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)"], ["org.apache.hadoop.fs.Path", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getSplitLogDir(org.apache.hadoop.fs.Path, java.lang.String)", "public static org.apache.hadoop.fs.Path getSplitLogDir(org.apache.hadoop.fs.Path, java.lang.String)"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getSplitLogDirTmpComponent(java.lang.String, java.lang.String)", "public static java.lang.String getSplitLogDirTmpComponent(java.lang.String, java.lang.String)"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem)", "public static void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.isCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem)", "public static boolean isCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem) throws java.io.IOException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.isRegionMarkedRecoveringInZK(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String)", "public static boolean isRegionMarkedRecoveringInZK(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String) throws org.apache.zookeeper.KeeperException"], ["long", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.parseLastFlushedSequenceIdFrom(byte[])", "public static long parseLastFlushedSequenceIdFrom(byte[])"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.deleteRecoveringRegionZNodes(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.util.List<java.lang.String>)", "public static void deleteRecoveringRegionZNodes(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.util.List<java.lang.String>)"], ["org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds", "org.apache.hadoop.hbase.zookeeper.ZKSplitLog.getRegionFlushedSequenceId(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.lang.String)", "public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getRegionFlushedSequenceId(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, java.lang.String) throws java.io.IOException"], ["org.apache.hadoop.hbase.zookeeper.ZKTableStateManager", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher)", "public org.apache.hadoop.hbase.zookeeper.ZKTableStateManager(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher) throws org.apache.zookeeper.KeeperException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.setTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State)", "public void setTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State) throws org.apache.hadoop.hbase.CoordinatedStateException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.setTableStateIfInStates(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)", "public boolean setTableStateIfInStates(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...) throws org.apache.hadoop.hbase.CoordinatedStateException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.setTableStateIfNotInStates(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)", "public boolean setTableStateIfNotInStates(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...) throws org.apache.hadoop.hbase.CoordinatedStateException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.isTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)", "public boolean isTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.isTableState(org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)", "public boolean isTableState(org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.setDeletedTable(org.apache.hadoop.hbase.TableName)", "public void setDeletedTable(org.apache.hadoop.hbase.TableName) throws org.apache.hadoop.hbase.CoordinatedStateException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.isTablePresent(org.apache.hadoop.hbase.TableName)", "public boolean isTablePresent(org.apache.hadoop.hbase.TableName)"], ["java.util.Set<org.apache.hadoop.hbase.TableName>", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.getTablesInStates(org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...)", "public java.util.Set<org.apache.hadoop.hbase.TableName> getTablesInStates(org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State...) throws java.io.InterruptedIOException, org.apache.hadoop.hbase.CoordinatedStateException"], ["void", "org.apache.hadoop.hbase.zookeeper.ZKTableStateManager.checkAndRemoveTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, boolean)", "public void checkAndRemoveTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos$Table$State, boolean) throws org.apache.hadoop.hbase.CoordinatedStateException"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer$HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain", "org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer$HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(java.lang.String[])", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer$HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(java.lang.String[]) throws java.io.IOException, java.lang.InterruptedException"], ["org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer", "org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer()", "public org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer()"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer.parse(org.apache.hadoop.conf.Configuration)", "public java.lang.String parse(org.apache.hadoop.conf.Configuration)"], ["void", "org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer.main(java.lang.String[])", "public static void main(java.lang.String[]) throws java.lang.Exception"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock(java.lang.String, int)", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock(java.lang.String, int)"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock.getPath()", "public java.lang.String getPath()"], ["int", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock.getVersion()", "public int getVersion()"], ["java.lang.String", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$AcquiredLock.toString()", "public java.lang.String toString()"], ["long", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$ZNodeComparator.getChildSequenceId(java.lang.String)", "public static long getChildSequenceId(java.lang.String)"], ["int", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$ZNodeComparator.compare(java.lang.String, java.lang.String)", "public int compare(java.lang.String, java.lang.String)"], ["int", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase$ZNodeComparator.compare(java.lang.Object, java.lang.Object)", "public int compare(java.lang.Object, java.lang.Object)"], ["void", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.acquire()", "public void acquire() throws java.io.IOException, java.lang.InterruptedException"], ["boolean", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.tryAcquire(long)", "public boolean tryAcquire(long) throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.release()", "public void release() throws java.io.IOException, java.lang.InterruptedException"], ["void", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.reapAllLocks()", "public void reapAllLocks() throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.reapExpiredLocks(long)", "public void reapExpiredLocks(long) throws java.io.IOException"], ["void", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.visitLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public void visitLocks(org.apache.hadoop.hbase.InterProcessLock$MetadataHandler) throws java.io.IOException"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, byte[], org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, byte[], org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock.readLock(byte[])", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock readLock(byte[])"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock.writeLock(byte[])", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock writeLock(byte[])"], ["org.apache.hadoop.hbase.InterProcessLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock.writeLock(byte[])", "public org.apache.hadoop.hbase.InterProcessLock writeLock(byte[])"], ["org.apache.hadoop.hbase.InterProcessLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock.readLock(byte[])", "public org.apache.hadoop.hbase.InterProcessLock readLock(byte[])"], ["org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock", "org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, byte[], org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)", "public org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock(org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher, java.lang.String, byte[], org.apache.hadoop.hbase.InterProcessLock$MetadataHandler)"]]}